<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="In the previous post, I talked about the importance of the Central Limit Theorem (CLT) to A/B testing. Here we will explore cases when we cannot rely on the CLT to hold." />
<meta name="keywords" content="data, ab testing, statistics">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Beyond the Central Limit Theorem"/>
<meta property="og:description" content="In the previous post, I talked about the importance of the Central Limit Theorem (CLT) to A/B testing. Here we will explore cases when we cannot rely on the CLT to hold."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/beyond-the-central-limit-theorem.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-02-06 00:00:00+01:00"/>
<meta property="article:modified_time" content="2020-02-06 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="ab-testing"/>
<meta property="article:tag" content="data"/>
<meta property="article:tag" content="ab testing"/>
<meta property="article:tag" content="statistics"/>
<meta property="og:image" content="/images/running_mean_cauchy.PNG"/>

  <title>Bytepawn &ndash; Beyond the Central Limit Theorem</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="beyond-the-central-limit-theorem">Beyond the Central Limit Theorem</h1>
    <p>Posted on Thu 06 February 2020 in <a href="/category/ab-testing.html">ab-testing</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>In the previous post, <a href="http://bytepawn.com/ab-testing-and-the-central-limit-theorem.html">A/B testing and the Central Limit Theorem</a>, I talked about the importance of the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a> (CLT) to A/B testing. Here we will explore cases when we cannot rely on the CLT to hold.</p>
<p>Some of the use-cases discussed here are relevant to the everyday work of a Data Scientist (eg. low sample size), but others are not, they’re more mathematical peculiarities. I still find it good practice to explore this, because understanding when a theorem doesn’t hold is a good way to understand the theorem itself, and why it works. It’s a bit like writing tests for software and trying to break it.</p>
<p>I will show 3 cases when we cannot rely on the CLT to hold:</p>
<ol>
<li>the distribution that does not have a mean, eg. the Cauchy distribution</li>
<li>violating the independence assumption in the CLT, eg. with a random walk</li>
<li>small sample size, eg. when events such as fraudulent transactions have a very low probability</li>
</ol>
<h2>The distribution does not have a mean</h2>
<p>The CLT says that when we are approximating the mean of a distribution by sampling, the sample means follow a normal distribution. So the CLT is about approximating the mean of a distribution. What if the distribution does not have a mean? In cases like this, we can still sample it, and compute a mean, but:</p>
<ul>
<li>since the original distribution doesn't have a mean, we're not approximating it</li>
<li>the sampled means will not converge to any value, they will keep jumping around</li>
</ul>
<p>How is it even possible for a distribution not to have a mean? The mean, or <a href="https://en.wikipedia.org/wiki/Expected_value">expected value</a> $ E[X] $ of a continuous random variable $ X $ with pfd $ f $ is given by:</p>
<p>$ E[X] = \int f(x) x dx $, where $ \int f(x) dx = 1 $</p>
<p>For a discrete random variable:</p>
<p>$ E[X] = \sum p_i i $, where $ \sum p_i = 1 $</p>
<p>The mean of a distribution does not exist if the integral or sum does not exist, ie. for a "pathological" $ f $ or $ p_i $.</p>
<p>One example is the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy-distribution</a>, defined by $ f(x) = 1 \over{ \Pi ( 1 + x^2 )} $. If you plug this into the above integral, it does not exist.</p>
<p>The Cauchy distribution looks like this:</p>
<p><img src="/images/cauchy.PNG" alt="Cauchy" style="width: 400px;"/></p>
<p>We can see this for ourselves with a simulation. Let's run the following simulation:</p>
<ol>
<li>Draw samples from a distribution</li>
<li>Every 100 sample, compute the running mean (from the very beginning)</li>
<li>Plot the running mean, together with the standard error</li>
</ol>
<p>Like in the previous post, we'll use <a href="https://www.scipy.org/">scipy</a>. First let's do this for distributions that have a mean: uniform ($ \mu = 0.5 $), exponential ($ \mu = 1 $) and a standard normal ($ \mu = 0 $):</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">population_running_mean_plot</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">population</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">running_stats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">step_size</span><span class="p">):</span>
        <span class="n">running_sample</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span>
        <span class="n">running_stats</span><span class="o">.</span><span class="n">append</span><span class="p">(([</span><span class="n">i</span><span class="p">,</span> <span class="n">mean</span><span class="p">(</span><span class="n">running_sample</span><span class="p">),</span> <span class="n">sem</span><span class="p">(</span><span class="n">running_sample</span><span class="p">)]))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">envelope_min</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">envelope_max</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">envelope_min</span><span class="p">,</span> <span class="n">envelope_max</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">population_running_mean_plot</span><span class="p">(</span><span class="n">uniform</span><span class="p">)</span>
<span class="n">population_running_mean_plot</span><span class="p">(</span><span class="n">expon</span><span class="p">)</span>
<span class="n">population_running_mean_plot</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span>
</pre></div>


<p>Let's see how the running mean converges to the true sample mean after 10,000 samples:</p>
<p><img src="/images/running_mean_convergence.PNG" alt="Running mean converges for the uniform, exponential and normal distribution" style="width: 400px;"/></p>
<p>Let's do the same for the Cauchy distribution, but let's let it run for 10,000,000 samples:</p>
<p><img src="/images/running_mean_cauchy.PNG" alt="Running mean for Cauchy distribution" style="width: 400px;"/></p>
<p>Notice how even after millions of samples, sometimes there is a big jump in the running mean. It does not converge, if we keep running it, it will keep jumping around, and it will jump to arbitrarily large values.</p>
<p>Why is this? The Cauchy distribution can be visualized like this: imagine drawing the bottom part of a circle (a half-circle) around the position (x=0, y=1). Now, for each draw, pick an angle $ \theta \in ( -\Pi/2, \Pi/2) $ in a uniform way, and then shoot a ray at that angle to the x-axis. The x-coordinate of the x-axis is the returned value for the Cauchy sampling. Although the angle is uniform, the ray can shoot arbitrarily far on the x-axis to generate extremely large or small values, and this happens quite often. Note that eg. the normal distribution can also generate arbitrarily small or large values, but at a lower rate, so the mean still exists there.</p>
<p>Another way to see this is to look at a histogram of 100,000 draws and compare it with a normal distribution (top is normal, bottom is Cauchy):</p>
<p><img src="/images/scatter_norm_cauchy.PNG" alt="Scatter plot for normal and Cauchy distributions" style="width: 400px;"/></p>
<p>Notice how the normal doesn't produce extreme values, whereas the Cauchy does.</p>
<h2>Violating the independence assumption in the CLT</h2>
<p>The wikipedia quote for the CLT starts like this: <em>“...when <strong>independent</strong> random variables are added...”</em>.</p>
<p>What this is saying is that the samples we draw should be independent of each other. What does this mean for an A/B test? For example, if user X and user Y are both using the product, they should not talk to each other, they should not influence each other when making the conversion “decision”, or when "deciding" how much time to spend with the product.</p>
<p>One thought experiment to show how dependence breaks the CLT is this: suppose a sociology PhD student is taking a salary survey, she sits in a room, test subjects go in and tell her their salary number (like $50,000), she records it, they leave the room, and the next person goes in. Now suppose that the person who is about to go in asks the person who just left what salary number they said. Then, because they want to look good, they decide to inflate their number and say a bigger number than the previous person, just to impress the sociology PhD student. Putting aside the fact that the survey is flawed, the poor student, if she keeps a running mean, will see that it keeps going up, and it doesn’t converge. The problem is that she gave her subjects a chance to communicate, and the individual measurements are no longer independent. She needs to have 2 doors, one for incoming and one for outgoing subjects, and make sure people don’t talk to each other.</p>
<p>In statistics, we can construct a similar case by using a <a href="https://en.wikipedia.org/wiki/Random_walk">random walk</a>: a frog starts at 0, and either goes to +1 or -1 with even probabilities, and so on. We can imagine the frog’s position to be the draws of a distribution, and clearly the subsequent draws are not independent: if the 42th draw was 13 (the frog was at position 13 after 42 time steps), the 43rd draw is going to be either 12 or 14, it’s conditioned on the previous draws. Similarly to the PhD student’s case, if we keep drawing these numbers and compute the mean, it will not converge. Note that this is not the same as the Cauchy case: here, at each position (so each random variable in the series) the mean exists and is finite; the mean position at draw #42 or #43 can be computed and it’s a finite number. But averaging these dependent random variables yields a random variable that breaks the CLT, because the sum does not exist.</p>
<p>Let's see this in action:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_walk_draw</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="c1">#return np.cumsum(norm().rvs(size=num_steps)) # we can also generate the steps with a std. normal</span>

<span class="k">def</span> <span class="nf">random_walk_running_mean_plot</span><span class="p">(</span><span class="n">sample_size</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span><span class="p">,</span> <span class="n">martingale_normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">random_walk_draw</span><span class="p">(</span><span class="n">num_steps</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">running_stats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">step_size</span><span class="p">):</span>
        <span class="n">running_sample</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">martingale_normalize</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">running_sample</span><span class="p">)</span><span class="o">/</span><span class="n">sqrt</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">running_sample</span><span class="p">)</span>
        <span class="n">running_stats</span><span class="o">.</span><span class="n">append</span><span class="p">(([</span><span class="n">i</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">sem</span><span class="p">(</span><span class="n">running_sample</span><span class="p">)]))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">envelope_min</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">envelope_max</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">running_stats</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">envelope_min</span><span class="p">,</span> <span class="n">envelope_max</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">random_walk_running_mean_plot</span><span class="p">(</span><span class="n">sample_size</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span><span class="o">*</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>


<p>Even after 10,000,000 steps, the running mean is still moving around (orange is the random walk itself, blue is the running mean):</p>
<p><img src="/images/random_walk.PNG" alt="Random walk" style="width: 400px;"/></p>
<p>Two interesting notes here:</p>
<ol>
<li>As I mentioned, the mean at any N-th timestep in the random walk can be sampled, it exists, and the CLT works. This is because up until the N-th timestep, the frog can only get N steps away from the origin (-N or N), so at any N, the probability distribution is bounded, and the mean exists, and it will be 0 (the true population mean). We can see this for ourselves:</li>
</ol>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">random_walk_sample_mean_plot</span><span class="p">(</span><span class="n">sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">sample_means</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean</span><span class="p">(</span><span class="n">random_walk_draw</span><span class="p">(</span><span class="n">num_steps</span><span class="o">=</span><span class="n">sample_size</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)]</span>
    <span class="n">mn</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">sample_means</span><span class="p">)</span>
    <span class="n">mx</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">sample_means</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">mx</span> <span class="o">-</span> <span class="n">mn</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">rng</span> <span class="o">/</span> <span class="mf">4.0</span>
    <span class="n">resolution</span> <span class="o">=</span> <span class="n">rng</span> <span class="o">/</span> <span class="mf">100.0</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mn</span> <span class="o">-</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mx</span> <span class="o">+</span> <span class="n">padding</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">sample_means</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">sample_means</span><span class="p">))</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_means</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">random_walk_sample_mean_plot</span><span class="p">(</span><span class="n">sample_size</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>


<p>After 10,000 steps:</p>
<p><img src="/images/random_walk_after_10000.PNG" alt="Random walk sample means after 10,000 steps" style="width: 400px;"/></p>
<ol>
<li>Cases like the random walk can be handled by an extension to the CLT called the <a href="https://en.wikipedia.org/wiki/Martingale_central_limit_theorem">Martingale Central Limit Theorem</a>. It essentially says that if you normalize the mean by a suitable function of the steps, then the mean will converge and exists:</li>
</ol>
<blockquote>
<p>In probability theory, the central limit theorem says that, under certain conditions, the sum of many independent identically-distributed random variables, when scaled appropriately, converges in distribution to a standard normal distribution. The martingale central limit theorem generalizes this result for random variables to martingales, which are stochastic processes where the change in the value of the process from time t to time t + 1 has expectation zero, even conditioned on previous outcomes.</p>
</blockquote>
<p>Because simple a random walk, after $ t $ timesteps will on average be $ \sqrt{t} $ steps away from the origin, the normalizing factor is $ \sqrt{t} $. With this, the normalized mean converges to 0 (since the whole setup is symmetric around the origin):</p>
<div class="highlight"><pre><span></span>random_walk_running_mean_plot(sample_size=10*1000*1000, martingale_normalize=True)
</pre></div>


<p><img src="/images/martingale_mean.PNG" alt="Martingale mean" style="width: 400px;"/></p>
<p>But note that this is no longer the original CLT.</p>
<h2>Small sample size</h2>
<p>The final case when we cannot rely on the CLT is when the sample size is small. The CLT says that as we increase the sample size $N$, we get arbitrarily close to a normal distribution (even though we never reach it, only in the $ N \rightarrow \infinity $ infitinity limit). In real life, we don’t have infinite time, so we always stop at some fixed $N$; it’s our job to make sure we have a big enough $N$ to make sure that our approximation is good enough.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/data.html">data</a>
      <a href="/tag/ab-testing.html">ab testing</a>
      <a href="/tag/statistics.html">statistics</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Beyond the Central Limit Theorem",
  "headline": "Beyond the Central Limit Theorem",
  "datePublished": "2020-02-06 00:00:00+01:00",
  "dateModified": "2020-02-06 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/beyond-the-central-limit-theorem.html",
  "description": "In the previous post, I talked about the importance of the Central Limit Theorem (CLT) to A/B testing. Here we will explore cases when we cannot rely on the CLT to hold."
}
</script></body>
</html>