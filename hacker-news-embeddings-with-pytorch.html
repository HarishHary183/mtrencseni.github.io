<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="A PyTorch model is trained on public Hacker News data, embedding posts and comments into a high-dimensional vector space, using the mean squared error (MSE) of dot products as the loss function. The resulting model is reasonably good at finding similar posts." />
<meta name="keywords" content="pytorch embedding">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Hacker News Embeddings with PyTorch"/>
<meta property="og:description" content="A PyTorch model is trained on public Hacker News data, embedding posts and comments into a high-dimensional vector space, using the mean squared error (MSE) of dot products as the loss function. The resulting model is reasonably good at finding similar posts."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/hacker-news-embeddings-with-pytorch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-03-12 00:00:00+01:00"/>
<meta property="article:modified_time" content="2019-03-12 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="pytorch embedding"/>
<meta property="og:image" content="/images/vectors.png"/>

  <title>Bytepawn &ndash; Hacker News Embeddings with PyTorch</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="hacker-news-embeddings-with-pytorch">Hacker News Embeddings with PyTorch</h1>
    <p>Posted on Tue 12 March 2019 in <a href="/category/machine-learning.html">Machine Learning</a></p>
  </header>
  <div>
    <p>This post is based on <a href="https://douwe.com/about">Douwe Osinga’s</a> excellent <a href="https://www.amazon.com/Deep-Learning-Cookbook-Practical-Recipes/dp/149199584X">Deep Learning Cookbook</a>, specifically <a href="https://github.com/DOsinga/deep_learning_cookbook/blob/master/04.2%20Build%20a%20recommender%20system%20based%20on%20outgoing%20Wikipedia%20links.ipynb">Chapter 4</a>, embeddings. Embedding is a simple thing: given an entity like a Hacker News post or a Hacker News user, we associate an n-dimensional vector with it. We then do a simple thing: if two entities are similar in some way, we assert that the dot product (cosine similarity) should be +1, ie. the vectors should be “aligned”. If two entities are not similar, we assert that the dot product should be -1, ie. they should point in different directions. We then feed the data to a model, and in the training process get the optimizer to find assignments of entities to vectors such that those assertions are satisfied as much as possible. The most famous example of embeddings is Google's <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a>.</p>
<p><img src="/images/dlc.jpg" alt="Deep Learning Cookbook" style="width: 400px;"/></p>
<p>In the book, embedding is performed on movies. For each movie, the wikipedia page is retrieved, and outgoing links to other wiki pages are collected. Two movies are similar if they both link to the same wiki page, else they are not similar. <a href="https://keras.io/">Keras</a> is used to train the model and the results are reasonably good.</p>
<p>I wanted to implement the same thing in <a href="https://pytorch.org">PyTorch</a>, but on a different data set, to keep it interesting. As a regular <a href="https://news.ycombinator.com">Hacker News</a> reader, I chose Hacker News. Likes of user are not public, but comments are, so I use that for similarity.</p>
<p>The plan is:</p>
<ol>
<li>Retrieve the top 1,000 HN posts from 2018 by number of comments</li>
<li>For each post, retrieve the unique set of users who commented</li>
<li>Use these (post, user) pairs for similarity embedding</li>
<li>Train with <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a> (MSE)</li>
<li>Use the resulting model to get post similarity</li>
</ol>
<p>The code for this recipe is up on Github (link to Github).</p>
<h2>Getting the top 1000 HN posts</h2>
<p>The simplest way to get this is from <a href="https://cloud.google.com/bigquery">Google BigQuery</a>, which has a <a href="https://console.cloud.google.com/marketplace/details/y-combinator/hacker-news">public Hacker News dataset</a>. We can write a SQL query and download the results as a CSV file from the Google Cloud console:</p>
<div class="highlight"><pre><span></span>SELECT
    id,
    descendants,
    title
FROM
    `bigquery-public-data.hacker_news.full`
WHERE
        timestamp &gt;= &quot;2018-01-01&quot;
    AND timestamp &lt;  “2019-01-01”
    AND type = &quot;story&quot;
    AND score &gt; 1
ORDER BY
    2 DESC
LIMIT
    1000
</pre></div>


<p>The result of this is <code>top_1000_posts.csv</code> (link to Github).</p>
<h2>Retrieve commenters for top posts</h2>
<p>Getting the comments is not practical from BigQuery because the table stores the tree hierarchy (<code>parent_id</code> of the parent comment, but not the <code>post_id</code>), so we’d have to query repeatedly to get all the comments of the post, which is inconvenient. Fortunately there’s an easier way. <a href="https://algolia.com">Algolia</a> has a Hacker News API where we can download one big JSON per post, containing all the comments. The API endpoint for this is:</p>
<p><code>https://hn.algolia.com/api/v1/items/&lt;post_id&gt;</code></p>
<p>So we just go through all the posts from the previous step and download each one from Algolia.
Getting the set of commenters out of the JSON would be the easiest with <code>json.load()</code>, but this sometimes fails on bad JSON. Instead we use an <a href="https://github.com/mtrencseni/rxe">rxe</a> regexp:</p>
<p><code>rxe.one('"author":"').one_or_more(rxe.set_except(['"'])).one('"')</code></p>
<p>The code for this is here on Github (LINK). The script caches files, so repeatedly running it doesn’t repeatedly re-download data from Algolia.</p>
<p>The script outputs the <code>(post, user)</code> pairs into <code>post_comments_1000.csv</code> (Github link).</p>
<h2>Building the model</h2>
<p>PyTorch has a built-in module for Embeddings, which makes building the model simple. It’s essentially a big array, which stores for each entity the assigned high-dimensional vector. In our case, both posts and users are embedded so if there are num_posts posts and num_users users, so num_vectors = num_posts + num_users. So the array has num_vectors row, each row corresponds to that entity’s embedding vector.</p>
<p>PyTorch will then optimize the entries in this array, so that the dot products of the combinations of the vectors are +1 and -1 as specified during training, or as close as possible.</p>
<p>We create a Model which contains the embedding. We implement the forward() function, which just returns the dot product for a minibatch of posts and users, as per the current embedding vectors:</p>
<p>class Model(torch.nn.Module):
    def <strong>init</strong>(self, num_vectors, embedding_dim):
        super(Model, self).<strong>init</strong>()
        self.embedding = torch.nn.Embedding(num_vectors, embedding_dim, max_norm=1.0)
    def forward(self, input):
        t1 = self.embedding(torch.LongTensor([v[0] for v in input]))
        t2 = self.embedding(torch.LongTensor([v[1] for v in input]))
        dot_products = torch.bmm(
            t1.contiguous().view(len(input), 1, self.embedding.embedding_dim),
            t2.contiguous().view(len(input), self.embedding.embedding_dim, 1)
        )
        return dot_products.contiguous().view(len(input))</p>
<p>Next, we need to write a function to build the minibatches we will use for training. For training, we will pass in existing combinations and “assert” that the dot product should be +1, and some missing combinations with -1:</p>
<p>def build_minibatch(num_positives, num_negatives):
    minibatch = []
    for _ in range(num_positives):
        which = int(len(idx_list) * random())
        minibatch.append(idx_list[which] + [1])
    for _ in range(num_negatives):
        while True:
            post = int(len(posts) * random())
            user = min_user_idx + int(len(users) * random())
            if post not in idx_user_posts[user]:
                break
        minibatch.append([post, user] + [-1])
    shuffle(minibatch)
    return minibatch</p>
<p>Now we can perform the training. We will embed into 50 dimensions, we will use 500 positive and 500 negative combinations per minibatch. We use the Adam optimizer and minimize the MSE between our asserted dot products and the actual dot products:</p>
<p>embedding_dim = 50
model = Model(num_vectors, embedding_dim)
optimizer = torch.optim.Adam(model.parameters())
loss_function = torch.nn.MSELoss(reduction='mean')
num_epochs = 50
num_positives = 500
num_negatives = 500
num_steps_per_epoch = int(len(post_comments) / num_positives)
for i in range(num_epochs):
    for j in range(num_steps_per_epoch):
        optimizer.zero_grad()
        minibatch = build_minibatch(num_positives, num_negatives)
        y = model.forward(minibatch)
        target = torch.FloatTensor([v[2] for v in minibatch])
        loss = loss_function(y, target)
        if i == 0 and j == 0:
            print('r: loss = %.3f' % float(loss))
        loss.backward(retain_graph=True)
        optimizer.step()
    print('%s: loss = %.3f' % (i, float(loss)))</p>
<h1>print out some samples to see how good the fit is</h1>
<p>minibatch = build_minibatch(5, 5)
y = model.forward(minibatch)
target = torch.FloatTensor([v[2] for v in minibatch])
print('Sample vectors:');
for i in range(5+5):
    print('%.3f vs %.3f' % (float(y[i]), float(target[i])))</p>
<p>Output:</p>
<p>r: loss = 1.016
0: loss = 1.009
...
49: loss = 0.633
Sample vectors:
0.319 vs -1.000
0.395 vs 1.000
0.226 vs -1.000
-0.232 vs -1.000
0.537 vs 1.000
0.179 vs -1.000
-0.020 vs 1.000
0.392 vs 1.000
0.141 vs 1.000
-0.096 vs -1.000</p>
<p>We can see that training is able to reduce the MSE by about 40% from the initial random vectors by finding better alignments. That doesn’t sound too good, but it’s good enough for recommendations to work. Let’s write a function to find the closest vectors to a query vector:</p>
<p>def similar_posts_by_title(title):
    post_id = title_to_id[title]
    pv = get_post_vector(post_id)
    dists = []
    for other_post in posts:
        if other_post == post_id: continue
        ov = get_post_vector(other_post)
        dist = torch.dot(pv, ov)
        dists.append([float(dist), 'https://news.ycombinator.com/item?id=' + other_post, id_to_title[other_post]])
    similars = sorted(dists)[-3:]
    similars.reverse()
    return similars</p>
<p>We can use this to find similar posts, and it works:</p>
<p><make table here>
Discussion
Clearly we could use the text of the posts/comments to gauge similarity, and would get much better results.
Training doesn’t converge, if the positive/negative ratio is too different from 1/1. Obviously, if we include too many positive pairs where we “assert” +1 dot product, the optimizer would just pull all the vectors together to get +1 all the time and reduce MSE. If we include too many negative pairs, it will pull all posts to one vector and all users to the opposing vector, this configuration will satisfy the training criteria and result in a low MSE. (In the book, 1/10 ratio is used, I think it’s accidental that it works in that case.)
When emitting the (post, user) pairs, we cut the users, and only keep users who have between 3 and 50 comments. The lower 3 is just to cut out users who don’t connect posts, so won’t be valuable to the embedding similarity training; so this cut makes the training set leaner and meaner. The 50 is to throw out users who comment on a lot posts, and hence pollute the similarity signal during training. Interestingly, without the upper limit of 50, the model doesn’t converge to a useful configuration! This took a lot of playing around to figure out.
The sample above uses similarity between vectors to find posts similar to a query post, and also displays the “similarity” dot product. With the same logic, we can also get recommendations for query users, or similar users to query users. However, in this case the dot product is always significantly lower than in the movie-movie case. The users are more scattered in the high-dimensional space, the movies are more tightly packed in a subspace.
Issues/bugs that slowed me down:
As mentioned, without the upper 50 cut on user’s comments, it didn’t converge.
Both posts and users are embedded, so we must remember at which row in the embedding matrix the user vectors start (min_user_idx in the code). Initially I forgot to account for this, both started indexing at 0. Everything ran, but the similarity results were garbage. A nicer solution here would be to use 2 Embedding objects (essentially 2 arrays), so we don’t have to remember the offset.
I forgot to call optimizer.zero_grad() in the zero grad. Everything ran, but the similarity results were garbage. Without the zero_grad() call, the gradients are just cumulated, and the optimizer jumps around aimlessly.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/pytorch-embedding.html">pytorch embedding</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Hacker News Embeddings with PyTorch",
  "headline": "Hacker News Embeddings with PyTorch",
  "datePublished": "2019-03-12 00:00:00+01:00",
  "dateModified": "2019-03-12 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/hacker-news-embeddings-with-pytorch.html",
  "description": "A PyTorch model is trained on public Hacker News data, embedding posts and comments into a high-dimensional vector space, using the mean squared error (MSE) of dot products as the loss function. The resulting model is reasonably good at finding similar posts."
}
</script></body>
</html>