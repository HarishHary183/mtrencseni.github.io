<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="A PyTorch model is trained on public Hacker News data, embedding posts and comments into a high-dimensional vector space, using the mean squared error (MSE) of dot products as the loss function. The resulting model is reasonably good at finding similar posts." />
<meta name="keywords" content="pytorch embedding">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Hacker News Embeddings with PyTorch"/>
<meta property="og:description" content="A PyTorch model is trained on public Hacker News data, embedding posts and comments into a high-dimensional vector space, using the mean squared error (MSE) of dot products as the loss function. The resulting model is reasonably good at finding similar posts."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/hacker-news-embeddings-with-pytorch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-03-12 00:00:00+01:00"/>
<meta property="article:modified_time" content="2019-03-12 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="pytorch embedding"/>
<meta property="og:image" content="/images/vectors.png"/>

  <title>Bytepawn &ndash; Hacker News Embeddings with PyTorch</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="hacker-news-embeddings-with-pytorch">Hacker News Embeddings with PyTorch</h1>
    <p>Posted on Tue 12 March 2019 in <a href="/category/machine-learning.html">Machine Learning</a></p>
  </header>
  <div>
    <p>This post is based on <a href="https://douwe.com/about">Douwe Osinga’s</a> excellent <a href="https://www.amazon.com/Deep-Learning-Cookbook-Practical-Recipes/dp/149199584X">Deep Learning Cookbook</a>, specifically <a href="https://github.com/DOsinga/deep_learning_cookbook/blob/master/04.2%20Build%20a%20recommender%20system%20based%20on%20outgoing%20Wikipedia%20links.ipynb">Chapter 4</a>, embeddings. Embedding is a simple thing: given an entity like a Hacker News post or a Hacker News user, we associate an n-dimensional vector with it. We then do a simple thing: if two entities are similar in some way, we assert that the dot product (cosine similarity) should be <code>+1</code>, ie. the vectors should be “aligned”. If two entities are not similar, we assert that the dot product should be <code>-1</code>, ie. they should point in different directions. We then feed the data to a model, and in the training process get the optimizer to find assignments of entities to vectors such that those assertions are satisfied as much as possible. The most famous example of embeddings is Google's <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a>.</p>
<p><img src="/images/dlc.jpg" alt="Deep Learning Cookbook" style="width: 400px;"/></p>
<p>In the book, embedding is performed on movies. For each movie, the wikipedia page is retrieved, and outgoing links to other wiki pages are collected. Two movies are similar if they both link to the same wiki page, else they are not similar. <a href="https://keras.io/">Keras</a> is used to train the model and the results are reasonably good.</p>
<p>I wanted to implement the same thing in <a href="https://pytorch.org">PyTorch</a>, but on a different data set, to keep it interesting. As a regular <a href="https://news.ycombinator.com">Hacker News</a> reader, I chose Hacker News. Likes of user are not public, but comments are, so I use that for similarity.</p>
<p>The plan is:</p>
<ol>
<li>Retrieve the top 1,000 HN posts from 2018 by number of comments</li>
<li>For each post, retrieve the unique set of users who commented</li>
<li>Use these (post, user) pairs for similarity embedding</li>
<li>Train with <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a> (MSE)</li>
<li>Use the resulting model to get post similarity</li>
</ol>
<p>The code for this recipe is up on Github (link to Github).</p>
<h2>Getting the top 1000 HN posts</h2>
<p>The simplest way to get this is from <a href="https://cloud.google.com/bigquery">Google BigQuery</a>, which has a <a href="https://console.cloud.google.com/marketplace/details/y-combinator/hacker-news">public Hacker News dataset</a>. We can write a SQL query and download the results as a CSV file from the Google Cloud console:</p>
<div class="highlight"><pre><span></span>SELECT
    id,
    descendants,
    title
FROM
    `bigquery-public-data.hacker_news.full`
WHERE
        timestamp &gt;= &quot;2018-01-01&quot;
    AND timestamp &lt;  “2019-01-01”
    AND type = &quot;story&quot;
    AND score &gt; 1
ORDER BY
    2 DESC
LIMIT
    1000
</pre></div>


<p>The result of this is <code>top_1000_posts.csv</code> (link to Github).</p>
<h2>Retrieve commenters for top posts</h2>
<p>Getting the comments is not practical from BigQuery because the table stores the tree hierarchy (<code>parent_id</code> of the parent comment, but not the <code>post_id</code>), so we’d have to query repeatedly to get all the comments of the post, which is inconvenient. Fortunately there’s an easier way. <a href="https://algolia.com">Algolia</a> has a Hacker News API where we can download one big JSON per post, containing all the comments. The API endpoint for this is:</p>
<p><code>https://hn.algolia.com/api/v1/items/&lt;post_id&gt;</code></p>
<p>So we just go through all the posts from the previous step and download each one from Algolia.
Getting the set of commenters out of the JSON would be the easiest with <code>json.load()</code>, but this sometimes fails on bad JSON. Instead we use an <a href="https://github.com/mtrencseni/rxe">rxe</a> regexp:</p>
<p><code>rxe.one('"author":"').one_or_more(rxe.set_except(['"'])).one('"')</code></p>
<p>The code for this is here on Github (LINK). The script caches files, so repeatedly running it doesn’t repeatedly re-download data from Algolia.</p>
<p>The script outputs the <code>(post, user)</code> pairs into <code>post_comments_1000.csv</code> (Github link).</p>
<h2>Building the model</h2>
<p>PyTorch has a built-in module for <a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html">Embeddings</a>, which makes building the model simple. It’s essentially a big array, which stores for each entity the assigned high-dimensional vector. In our case, both posts and users are embedded so if there are <code>num_posts</code> posts and <code>num_users</code> users, so <code>num_vectors = num_posts + num_users</code>. So the array has <code>num_vectors</code> row, each row corresponds to that entity’s embedding vector.</p>
<p>PyTorch will then optimize the entries in this array, so that the dot products of the combinations of the vectors are <code>+1</code> and <code>-1</code> as specified during training, or as close as possible.</p>
<p>The next step is to create a Model which contains the embedding. We implement the <code>forward()</code> function, which just returns the dot product for a minibatch of posts and users, as per the current embedding vectors:</p>
<div class="highlight"><pre><span></span><span class="kr">class</span> <span class="nx">Model</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Module</span><span class="p">)</span><span class="o">:</span>
    <span class="nx">def</span> <span class="nx">__init__</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">num_vectors</span><span class="p">,</span> <span class="nx">embedding_dim</span><span class="p">)</span><span class="o">:</span>
        <span class="kr">super</span><span class="p">(</span><span class="nx">Model</span><span class="p">,</span> <span class="nx">self</span><span class="p">).</span><span class="nx">__init__</span><span class="p">()</span>
        <span class="nx">self</span><span class="p">.</span><span class="nx">embedding</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">nn</span><span class="p">.</span><span class="nx">Embedding</span><span class="p">(</span><span class="nx">num_vectors</span><span class="p">,</span> <span class="nx">embedding_dim</span><span class="p">,</span> <span class="nx">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="nx">def</span> <span class="nx">forward</span><span class="p">(</span><span class="nx">self</span><span class="p">,</span> <span class="nx">input</span><span class="p">)</span><span class="o">:</span>
        <span class="nx">t1</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">embedding</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nx">LongTensor</span><span class="p">([</span><span class="nx">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="nx">v</span> <span class="k">in</span> <span class="nx">input</span><span class="p">]))</span>
        <span class="nx">t2</span> <span class="o">=</span> <span class="nx">self</span><span class="p">.</span><span class="nx">embedding</span><span class="p">(</span><span class="nx">torch</span><span class="p">.</span><span class="nx">LongTensor</span><span class="p">([</span><span class="nx">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="nx">v</span> <span class="k">in</span> <span class="nx">input</span><span class="p">]))</span>
        <span class="nx">dot_products</span> <span class="o">=</span> <span class="nx">torch</span><span class="p">.</span><span class="nx">bmm</span><span class="p">(</span>
            <span class="nx">t1</span><span class="p">.</span><span class="nx">contiguous</span><span class="p">().</span><span class="nx">view</span><span class="p">(</span><span class="nx">len</span><span class="p">(</span><span class="nx">input</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">self</span><span class="p">.</span><span class="nx">embedding</span><span class="p">.</span><span class="nx">embedding_dim</span><span class="p">),</span>
            <span class="nx">t2</span><span class="p">.</span><span class="nx">contiguous</span><span class="p">().</span><span class="nx">view</span><span class="p">(</span><span class="nx">len</span><span class="p">(</span><span class="nx">input</span><span class="p">),</span> <span class="nx">self</span><span class="p">.</span><span class="nx">embedding</span><span class="p">.</span><span class="nx">embedding_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nx">dot_products</span><span class="p">.</span><span class="nx">contiguous</span><span class="p">().</span><span class="nx">view</span><span class="p">(</span><span class="nx">len</span><span class="p">(</span><span class="nx">input</span><span class="p">))</span>
</pre></div>


<p>Next, we need to write a function to build the minibatches we will use for training. For training, we will pass in existing combinations and “assert” that the dot product should be <code>+1</code>, and some missing combinations with <code>-1</code>:</p>
<div class="highlight"><pre><span></span>def build_minibatch(num_positives, num_negatives):
    minibatch = []
    for _ in range(num_positives):
        which = int(len(idx_list) * random())
        minibatch.append(idx_list[which] + [1])
    for _ in range(num_negatives):
        while True:
            post = int(len(posts) * random())
            user = min_user_idx + int(len(users) * random())
            if post not in idx_user_posts[user]:
                break
        minibatch.append([post, user] + [-1])
    shuffle(minibatch)
    return minibatch
</pre></div>


<p>Now we can perform the training. We will embed into 50 dimensions, we will use 500 positive and 500 negative combinations per minibatch. We use the <a href="https://pytorch.org/docs/stable/optim.html">Adam optimizer</a> and minimize the mean squared error between our asserted dot products and the actual dot products:</p>
<div class="highlight"><pre><span></span><span class="nt">embedding_dim</span> <span class="o">=</span> <span class="nt">50</span>
<span class="nt">model</span> <span class="o">=</span> <span class="nt">Model</span><span class="o">(</span><span class="nt">num_vectors</span><span class="o">,</span> <span class="nt">embedding_dim</span><span class="o">)</span>
<span class="nt">optimizer</span> <span class="o">=</span> <span class="nt">torch</span><span class="p">.</span><span class="nc">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="o">(</span><span class="nt">model</span><span class="p">.</span><span class="nc">parameters</span><span class="o">())</span>
<span class="nt">loss_function</span> <span class="o">=</span> <span class="nt">torch</span><span class="p">.</span><span class="nc">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="o">(</span><span class="nt">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="o">)</span>
<span class="nt">num_epochs</span> <span class="o">=</span> <span class="nt">50</span>
<span class="nt">num_positives</span> <span class="o">=</span> <span class="nt">500</span>
<span class="nt">num_negatives</span> <span class="o">=</span> <span class="nt">500</span>
<span class="nt">num_steps_per_epoch</span> <span class="o">=</span> <span class="nt">int</span><span class="o">(</span><span class="nt">len</span><span class="o">(</span><span class="nt">post_comments</span><span class="o">)</span> <span class="o">/</span> <span class="nt">num_positives</span><span class="o">)</span>
<span class="nt">for</span> <span class="nt">i</span> <span class="nt">in</span> <span class="nt">range</span><span class="o">(</span><span class="nt">num_epochs</span><span class="o">):</span>
    <span class="nt">for</span> <span class="nt">j</span> <span class="nt">in</span> <span class="nt">range</span><span class="o">(</span><span class="nt">num_steps_per_epoch</span><span class="o">):</span>
        <span class="nt">optimizer</span><span class="p">.</span><span class="nc">zero_grad</span><span class="o">()</span>
        <span class="nt">minibatch</span> <span class="o">=</span> <span class="nt">build_minibatch</span><span class="o">(</span><span class="nt">num_positives</span><span class="o">,</span> <span class="nt">num_negatives</span><span class="o">)</span>
        <span class="nt">y</span> <span class="o">=</span> <span class="nt">model</span><span class="p">.</span><span class="nc">forward</span><span class="o">(</span><span class="nt">minibatch</span><span class="o">)</span>
        <span class="nt">target</span> <span class="o">=</span> <span class="nt">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="o">(</span><span class="cp">[</span><span class="nx">v</span><span class="err">[</span><span class="mi">2</span><span class="cp">]</span> <span class="nt">for</span> <span class="nt">v</span> <span class="nt">in</span> <span class="nt">minibatch</span><span class="o">])</span>
        <span class="nt">loss</span> <span class="o">=</span> <span class="nt">loss_function</span><span class="o">(</span><span class="nt">y</span><span class="o">,</span> <span class="nt">target</span><span class="o">)</span>
        <span class="nt">if</span> <span class="nt">i</span> <span class="o">==</span> <span class="nt">0</span> <span class="nt">and</span> <span class="nt">j</span> <span class="o">==</span> <span class="nt">0</span><span class="o">:</span>
            <span class="nt">print</span><span class="o">(</span><span class="s1">&#39;r: loss = %.3f&#39;</span> <span class="o">%</span> <span class="nt">float</span><span class="o">(</span><span class="nt">loss</span><span class="o">))</span>
        <span class="nt">loss</span><span class="p">.</span><span class="nc">backward</span><span class="o">(</span><span class="nt">retain_graph</span><span class="o">=</span><span class="nt">True</span><span class="o">)</span>
        <span class="nt">optimizer</span><span class="p">.</span><span class="nc">step</span><span class="o">()</span>
    <span class="nt">print</span><span class="o">(</span><span class="s1">&#39;%s: loss = %.3f&#39;</span> <span class="o">%</span> <span class="o">(</span><span class="nt">i</span><span class="o">,</span> <span class="nt">float</span><span class="o">(</span><span class="nt">loss</span><span class="o">)))</span>
<span class="err">#</span> <span class="nt">print</span> <span class="nt">out</span> <span class="nt">some</span> <span class="nt">samples</span> <span class="nt">to</span> <span class="nt">see</span> <span class="nt">how</span> <span class="nt">good</span> <span class="nt">the</span> <span class="nt">fit</span> <span class="nt">is</span>
<span class="nt">minibatch</span> <span class="o">=</span> <span class="nt">build_minibatch</span><span class="o">(</span><span class="nt">5</span><span class="o">,</span> <span class="nt">5</span><span class="o">)</span>
<span class="nt">y</span> <span class="o">=</span> <span class="nt">model</span><span class="p">.</span><span class="nc">forward</span><span class="o">(</span><span class="nt">minibatch</span><span class="o">)</span>
<span class="nt">target</span> <span class="o">=</span> <span class="nt">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="o">(</span><span class="cp">[</span><span class="nx">v</span><span class="err">[</span><span class="mi">2</span><span class="cp">]</span> <span class="nt">for</span> <span class="nt">v</span> <span class="nt">in</span> <span class="nt">minibatch</span><span class="o">])</span>
<span class="nt">print</span><span class="o">(</span><span class="s1">&#39;Sample vectors:&#39;</span><span class="o">);</span>
<span class="nt">for</span> <span class="nt">i</span> <span class="nt">in</span> <span class="nt">range</span><span class="o">(</span><span class="nt">5</span><span class="o">+</span><span class="nt">5</span><span class="o">):</span>
    <span class="nt">print</span><span class="o">(</span><span class="s1">&#39;%.3f vs %.3f&#39;</span> <span class="o">%</span> <span class="o">(</span><span class="nt">float</span><span class="o">(</span><span class="nt">y</span><span class="cp">[</span><span class="nx">i</span><span class="cp">]</span><span class="o">),</span> <span class="nt">float</span><span class="o">(</span><span class="nt">target</span><span class="cp">[</span><span class="nx">i</span><span class="cp">]</span><span class="o">)))</span>
</pre></div>


<p>Output:</p>
<div class="highlight"><pre><span></span><span class="n">r</span><span class="o">:</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">1.016</span>
<span class="mi">0</span><span class="o">:</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">1.009</span>
<span class="o">...</span>
<span class="mi">49</span><span class="o">:</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.633</span>
<span class="n">Sample</span> <span class="n">vectors</span><span class="o">:</span>
<span class="mf">0.319</span> <span class="n">vs</span> <span class="o">-</span><span class="mf">1.000</span>
<span class="mf">0.395</span> <span class="n">vs</span> <span class="mf">1.000</span>
<span class="mf">0.226</span> <span class="n">vs</span> <span class="o">-</span><span class="mf">1.000</span>
<span class="o">-</span><span class="mf">0.232</span> <span class="n">vs</span> <span class="o">-</span><span class="mf">1.000</span>
<span class="mf">0.537</span> <span class="n">vs</span> <span class="mf">1.000</span>
<span class="mf">0.179</span> <span class="n">vs</span> <span class="o">-</span><span class="mf">1.000</span>
<span class="o">-</span><span class="mf">0.020</span> <span class="n">vs</span> <span class="mf">1.000</span>
<span class="mf">0.392</span> <span class="n">vs</span> <span class="mf">1.000</span>
<span class="mf">0.141</span> <span class="n">vs</span> <span class="mf">1.000</span>
<span class="o">-</span><span class="mf">0.096</span> <span class="n">vs</span> <span class="o">-</span><span class="mf">1.000</span>
</pre></div>


<p>We can see that training is able to reduce the MSE by about 40% from the initial random vectors by finding better alignments. That doesn’t sound too good, but it’s good enough for recommendations to work. Let’s write a function to find the closest vectors to a query vector:</p>
<div class="highlight"><pre><span></span>def similar_posts_by_title(title):
    post_id = title_to_id[title]
    pv = get_post_vector(post_id)
    dists = []
    for other_post in posts:
        if other_post == post_id: continue
        ov = get_post_vector(other_post)
        dist = torch.dot(pv, ov)
        dists.append([float(dist), &#39;https://news.ycombinator.com/item?id=&#39; + other_post, id_to_title[other_post]])
    similars = sorted(dists)[-3:]
    similars.reverse()
    return similars
</pre></div>


<p>We can use this to find similar posts, and it works:</p>
<p><make table here></p>
<h2>Discussion</h2>
<ol>
<li>Clearly we could use the text of the posts/comments to gauge similarity, and would get much better results.</li>
<li>Training doesn’t converge, if the positive/negative ratio is too different from 1/1. Obviously, if we include too many positive pairs where we “assert” +1 dot product, the optimizer would just pull all the vectors together to get <code>+1</code> all the time and reduce MSE. If we include too many negative pairs, it will pull all posts to one vector and all users to the opposing vector, this configuration will satisfy the training criteria and result in a low MSE. (In the book, 1/10 ratio is used, I think it’s accidental that it works in that case.)</li>
<li>When emitting the <code>(post, user)</code> pairs, we cut the users, and only keep users who have between 3 and 50 comments. The lower 3 is just to cut out users who don’t connect posts, so won’t be valuable to the embedding similarity training; so this cut makes the training set leaner and meaner. The 50 is to throw out users who comment on a lot posts, and hence pollute the similarity signal during training. Interestingly, without the upper limit of 50, the model doesn’t converge to a useful configuration! This took a lot of playing around to figure out.</li>
<li>The sample above uses similarity between vectors to find posts similar to a query post, and also displays the “similarity” dot product. With the same logic, we can also get recommendations for query users, or similar users to query users. However, in this case the dot product is always significantly lower than in the movie-movie case. The users are more scattered in the high-dimensional space, the movies are more tightly packed in a subspace.</li>
<li>Issues/bugs that slowed me down:
    a. Both posts and users are embedded, so we must remember at which row in the embedding matrix the user vectors start (<code>min_user_idx</code> in the code). Initially I forgot to account for this, both started indexing at 0. Everything ran, but the similarity results were garbage. A nicer solution here would be to use 2 <code>Embedding</code> objects (essentially 2 arrays), so we don’t have to remember the offset.
    b. I forgot to call <code>optimizer.zero_grad()</code> in the zero grad. Everything ran, but the similarity results were garbage. Without the <code>zero_grad()</code> call, the gradients are just cumulated, and the optimizer jumps around aimlessly.</li>
</ol>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/pytorch-embedding.html">pytorch embedding</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Hacker News Embeddings with PyTorch",
  "headline": "Hacker News Embeddings with PyTorch",
  "datePublished": "2019-03-12 00:00:00+01:00",
  "dateModified": "2019-03-12 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/hacker-news-embeddings-with-pytorch.html",
  "description": "A PyTorch model is trained on public Hacker News data, embedding posts and comments into a high-dimensional vector space, using the mean squared error (MSE) of dot products as the loss function. The resulting model is reasonably good at finding similar posts."
}
</script></body>
</html>