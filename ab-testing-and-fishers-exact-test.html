<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="google-site-verification" content="14dvQcWyDWHBo8aM0kld8XzEa6KypAzCQDz1_KPus9E" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn - Marton Trencseni Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="Fisher’s exact test directly computes the same p value as the Chi-squared test, so it does not rely on the Central Limit Theorem to hold." />
<meta name="keywords" content="ab-testing">
<meta property="og:site_name" content="Bytepawn - Marton Trencseni"/>
<meta property="og:title" content="A/B testing and Fisher's exact test"/>
<meta property="og:description" content="Fisher’s exact test directly computes the same p value as the Chi-squared test, so it does not rely on the Central Limit Theorem to hold."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/ab-testing-and-fishers-exact-test.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-03-03 00:00:00+01:00"/>
<meta property="article:modified_time" content="2020-03-03 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="ab-testing"/>
<meta property="og:image" content="/images/fisher2.png"/>

  <title>Bytepawn - Marton Trencseni &ndash; A/B testing and Fisher's exact test</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <!--<h2><a href="">Bytepawn - Marton Trencseni</a></h2>-->
      <h2><a href="http://bytepawn.com">Bytepawn</a></h2>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="ab-testing-and-fishers-exact-test">A/B testing and Fisher's exact test</h1>
    <p>Marton Trencseni - Tue 03 March 2020 - <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p><a href="https://en.wikipedia.org/wiki/Fisher%27s_exact_test">Fisher’s exact test</a> directly computes the same $p$ value as the <a href="https://en.wikipedia.org/wiki/Chi-squared_test">$\chi^2$ test</a>, without relying on the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a> (CLT) to hold, so it is accurate at low $N$. See the previous post on <a href="http://bytepawn.com/ab-testing-and-the-chi-squared-test.html">A/B testing and the Chi-squared test</a> for an introduction to the $\chi^2$ test. The trade-off is that Fisher’s exact test is more computationally intensive, so even at moderate $N$ the direct computation is not feasible. However, <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo sampling</a> can be used to get estimated results quickly.</p>
<p><a href="https://github.com/mtrencseni/playground/blob/master/AB%20testing%20and%20Fisher's%20exact%20test.ipynb">The code shown below is up on Github.</a></p>
<h2>Diversion: the Binomial test</h2>
<p>The best way to understand Fisher’s test is by a simpler analogue, coin flips. Suppose somebody gives you a coin, and you’re trying to decide whether the coin is fair. If you can flip it a lot of times, you can use a <a href="https://en.wikipedia.org/wiki/Z-test">Z-test</a> to decide whether it’s fair or not, because at high $N$, the CLT holds, and the distribution of heads follows a normal distribution.</p>
<p>But, what if you’re only allowed to flip it $N=24$ times and you get $H=18$ heads? This is a low $N$, so the Z-test will not work. But, we can just directly compute the p value by computing $ P(H &gt;= 18 \vee H &lt;= 6) $ assuming the null hypothesis of a fair toin coss. Note that we’re doing a two-tailed test here. $ P(H &gt;= 18 \vee H &lt;= 6) = P(H = 1) + ... + P(H = 6) + P(H = 18) + ... + P(H = 24)$, where $ P(H = k) = {n \choose k} p^k q^{n-k} $, where $p = 0.5, q = 1 - p = 0.5$ in this case from the null hypothesis ($p$ is the probability of heads, $q$ of tails).</p>
<p>What we’re doing here is called the <a href="https://en.wikipedia.org/wiki/Binomial_test">Binomial test</a>. The <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">scipy stats</a> package has a library function:</p>
<div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">binom_test</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
</pre></div>


<p>Prints:</p>
<div class="highlight"><pre><span></span><span class="mf">0.023</span>
</pre></div>


<p>We can calculate this ourselves per the above formula, we just have to be careful with the rounding:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">binomial_test</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">2.0</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">lo</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">2.0</span> <span class="o">-</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">hi</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">2.0</span> <span class="o">+</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">lo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="nb">range</span><span class="p">(</span><span class="n">hi</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">p</span> <span class="o">+=</span> <span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">**</span><span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>


<p>This is a direct p value calculation, it works at any $N$. Let’s double-check what we know. According to the CLT, at high $N$ the average ratio of heads will follow a normal distribution, so we can use the Z-test, and it should yield the same result as the direct calculation above:</p>
<div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">H</span> <span class="o">=</span> <span class="mi">5100</span> <span class="c1"># delta of +1% lift</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># the null hypothesis</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">H</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">H</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Binom test p: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">binom_test</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Z-test p:     </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">ztest</span><span class="p">(</span><span class="n">x1</span><span class="o">=</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">p</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>


<p>Prints:</p>
<div class="highlight"><pre><span></span><span class="n">Binom</span> <span class="n">test</span> <span class="n">p</span><span class="p">:</span> <span class="mf">0.047</span>
<span class="n">Z</span><span class="o">-</span><span class="n">test</span> <span class="n">p</span><span class="p">:</span>     <span class="mf">0.045</span>
</pre></div>


<p>Not quite the same, but pretty close. We can see how the exact binomial and the normal estimated Z-test p values converge thanks to the CLT:</p>
<div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># the null hypothesis</span>
<span class="n">actual_lift</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">H</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">p</span> <span class="o">+</span> <span class="n">actual_lift</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="n">H</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">H</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p_binom</span> <span class="o">=</span> <span class="n">binom_test</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">p_z</span> <span class="o">=</span> <span class="n">ztest</span><span class="p">(</span><span class="n">x1</span><span class="o">=</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">p</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">p_diff</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">p_binom</span> <span class="o">-</span> <span class="n">p_z</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">p_diff</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sample size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;|p z-test - p exact binomial|&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>The difference goes to zero in the $N \rightarrow \infty $ limit:</p>
<p><img src="/images/fisher1.png" alt="Binomial test and Z-test p value difference" style="width: 600px;"/></p>
<h2>Fisher’s exact test</h2>
<p>What the binomial test is to the Z-test, <a href="https://en.wikipedia.org/wiki/Fisher%27s_exact_test">Fisher’s exact test</a> is to the $\chi^2$ test. It’s a direct calculation of the p value in case of a $F \times C$ contingency table. Fisher’s exact test is accurate at all $N$s, and the $\chi^2$ test’s p converges to it at high $N$s, similar to the above case.</p>
<p>The null hypothesis is that all funnels have the same conversion event probabilities. Given the $F \times C$ contingency table outcome of an A/B test ($F$ funnels tested, $C$ mutually exclusive conversion events), the calculation of the p value is:</p>
<ol>
<li>first, calculate the marginals:<ul>
<li>row marginals: how many users were randomly assigned into each funnel in the A/B test</li>
<li>column marginals: across the tested funnels, conversion event totals</li>
</ul>
</li>
<li>given the marginals, what is the probability of the observed outcomes</li>
<li>for all the ways we can change numbers in the contingency table while keeping the marginals fixed, take the ones that have equal or lower probability then the actual outcome, and add up those probabilities; this is the p value</li>
</ol>
<p>The trick is, how to calculate the quantity “given the marginals, what is the probability of a specific outcome  (numbers in the contingency table that add up to the marginals)”; we need this in both step 2. and 3. For this we have to use the <a href="https://en.wikipedia.org/wiki/Hypergeometric_distribution">hypergeometric distribution</a>, the distribution for “urn draws”. Let’s reuse the contingency table from the previous post:</p>
<p><img src="/images/contingency_table3.PNG" alt="Contingency table" style="width: 600px;"/></p>
<p>Imagine this: we have a total of $N=10,000$ marbles. Each marble is one of $C=3$ colors (<strong>No conversion, Monthly, Annual</strong>). There are a total of 7,922 marbles <strong>No conversion</strong> marbles, 1,085 <strong>Monthly</strong> conversion marbles, etc. All these marbles are in one big urn. We start drawing marbles; what’s the probability that the first 5,916 drawn will be colored <strong>(No conversion, Monthly, Annual) = (4748, 595, 573)</strong>, irrespective or the order they are drawn? We can break this into two probabilities that we multiply: what is the probability that of 5,916 drawn the colors are <strong>(No conversion, Rest) = (4748, 595+573)</strong> from an urn that contains <strong>(No conversion, Rest) = (7922, 1085+993)</strong> marbles, multiplied by, what is the probability that of the rest 595+573=1,168 drawn the colors are <strong>(Monthly, Annual) = (595, 573)</strong> from an urn that contains <strong>(Monthly, Annual) = (1085, 993)</strong> marbles. These individual probabilities are given by the hypergeometric probability $P(X=k | N, K, n)$, ie. what is the probability of drawing $k$ red marbles from an urn that contains a total of $N$ marbles, $K$ of which are red, of total $n$ drawn ($k \leq n$). It is $P(X=k | N, K, n) = \frac{ { K \choose k} { N-K \choose n-k } }{ {N \choose n} }$. Then, we go on and calculate the same probabilities in the second row, <strong>but keeping in mind that we have already removed (4748, 595, 573) marbles from the urn</strong>. The ordering of the rows doesn’t matter.</p>
<p>The formula $P(X=k | N, K, n)$ above is implemented by the <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.hypergeom.html">scipy hypergeometric probability function </a> <code>hypergeom.pmf</code>, with that we can implement the calculation of the overall probability of a contingency table like:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hypergeom_probability</span><span class="p">(</span><span class="n">observations</span><span class="p">):</span>
    <span class="n">row_marginals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">col_marginals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">p</span> <span class="o">*=</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span>
              <span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span>
              <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">col_marginals</span><span class="p">[</span><span class="n">j</span><span class="p">:]),</span>
              <span class="n">col_marginals</span><span class="p">[</span><span class="n">j</span><span class="p">],</span>
              <span class="n">row_marginals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">row_marginals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">col_marginals</span> <span class="o">-=</span> <span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>


<p>We can now run an A/B test and whatever the outcome, we can compute the probability of that specific outcome, which will be a very small number:</p>
<div class="highlight"><pre><span></span><span class="n">funnels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="mf">0.6</span><span class="p">],</span> <span class="c1"># the first vector is the actual outcomes,</span>
    <span class="p">[[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">],</span> <span class="c1"># the second is the traffic split</span>
    <span class="p">[[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span><span class="o">*</span><span class="mi">1000</span>

<span class="n">observations</span> <span class="o">=</span> <span class="n">simulate_abtest</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">hypergeom_probability</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
</pre></div>


<p>But this is not the p value! This is just step 2. To get the p value, per step 3, we have to add up the probabilities for all possible numbers in the contingency table that add up to the marginals, that have lower probability than the actual observations (=are more extreme).</p>
<p>But, even at moderate $N$, there are too many combinations! Instead, what we’ll do is a Monte Carlo (MC) sum. We will randomly sample combinations that satisfy the marginals, and compute the ratio of cases that have lower probability than the given A/B test outcome (= =more extreme outcomes):</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">multi_hypergeom_sample</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">remaining</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">colors</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">colors</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">colors</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">m</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">hypergeometric</span><span class="p">(</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">remaining</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">m</span><span class="p">)</span>
        <span class="n">m</span> <span class="o">-=</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">m</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">sample_once</span><span class="p">(</span><span class="n">observations</span><span class="p">):</span>
    <span class="n">row_marginals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">col_marginals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">row_marginals</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">multi_hypergeom_sample</span><span class="p">(</span><span class="n">row_marginals</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">col_marginals</span><span class="p">)</span>
        <span class="n">col_marginals</span> <span class="o">-=</span> <span class="n">sample</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">sample</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">col_marginals</span>
    <span class="k">return</span> <span class="n">sample</span>

<span class="k">def</span> <span class="nf">fisher_monte_carlo</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">num_simulations</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">p_obs</span> <span class="o">=</span> <span class="n">hypergeom_probability</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
    <span class="n">hits</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">sample_once</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
        <span class="n">p_sample</span> <span class="o">=</span> <span class="n">hypergeom_probability</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p_sample</span> <span class="o">&lt;=</span> <span class="n">p_obs</span><span class="p">:</span>
            <span class="n">hits</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">hits</span><span class="o">/</span><span class="n">num_simulations</span>
</pre></div>


<p>Our implementation above works for arbitrary $F \times C$ contingency tables. The scipy stats library has Fisher’s exact test built in, but it only works for 2x2 tables. Also, remember that we also have the $\chi^2$ test. Let’s compare the three for 2x2 cases:</p>
<div class="highlight"><pre><span></span><span class="n">funnels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.40</span><span class="p">],</span> <span class="mf">0.5</span><span class="p">],</span> <span class="c1"># the first vector is the actual outcomes,</span>
    <span class="p">[[</span><span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.40</span><span class="p">],</span> <span class="mf">0.5</span><span class="p">],</span> <span class="c1"># the second is the traffic split</span>
<span class="p">]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">observations</span> <span class="o">=</span> <span class="n">simulate_abtest</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
<span class="n">ch</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;chi-squared p = </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">ch</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fs</span> <span class="o">=</span> <span class="n">fisher_exact</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;fishers exact p = </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">fs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fmc</span> <span class="o">=</span> <span class="n">fisher_monte_carlo</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;monte carlo p = </span><span class="si">%.3f</span><span class="s2">&quot;&quot;&quot;</span> <span class="o">%</span> <span class="n">fmc</span><span class="p">)</span>
</pre></div>


<p>Prints something like:</p>
<div class="highlight"><pre><span></span><span class="p">[[</span><span class="mf">37.</span> <span class="mf">16.</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">31.</span> <span class="mf">16.</span><span class="p">]]</span>
<span class="n">chi</span><span class="o">-</span><span class="n">squared</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">0.680</span>
<span class="n">fishers</span> <span class="n">exact</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">0.830</span>
<span class="n">monte</span> <span class="n">carlo</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">0.831</span>
</pre></div>


<p>The p value coming from <code>fisher_exact()</code> is exact, the one coming from <code>chi2_contingency()</code> assumes the Central Limit Theorem, while the one coming from our own <code>fisher_monte_carlo()</code> involves a Monte Carlo approximation. Similar to the binomial toin coss example above, let’s see what happens to the differences in $p$ values with increasing $N$, given two funnels and an actual conversion lift of 1%:</p>
<div class="highlight"><pre><span></span><span class="n">base_conversion</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">traffic_split</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">actual_lift</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="o">*</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">200</span><span class="p">):</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">base_conversion</span> <span class="o">*</span> <span class="n">traffic_split</span> <span class="o">*</span> <span class="n">N</span><span class="p">),</span>
         <span class="nb">int</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">base_conversion</span><span class="p">)</span> <span class="o">*</span> <span class="n">traffic_split</span> <span class="o">*</span> <span class="n">N</span><span class="p">)],</span>
        <span class="p">[</span><span class="nb">int</span><span class="p">((</span><span class="n">base_conversion</span><span class="o">+</span><span class="n">actual_lift</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">traffic_split</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span><span class="p">),</span>
         <span class="nb">int</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">base_conversion</span><span class="o">+</span><span class="n">actual_lift</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">traffic_split</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span><span class="p">)],</span>
    <span class="p">]</span>
    <span class="n">p_chi2</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">p_fish</span> <span class="o">=</span> <span class="n">fisher_exact</span><span class="p">(</span><span class="n">observations</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">p_fmc</span> <span class="o">=</span> <span class="n">fisher_monte_carlo</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">num_simulations</span><span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">p_diff_chi2_fish</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">p_chi2</span> <span class="o">-</span> <span class="n">p_fish</span><span class="p">)</span>
    <span class="n">p_diff_chi2_fmc</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">p_chi2</span> <span class="o">-</span> <span class="n">p_fmc</span><span class="p">)</span>
    <span class="n">p_diff_fish_fmc</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">p_fish</span> <span class="o">-</span> <span class="n">p_fmc</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">p_diff_chi2_fish</span><span class="p">,</span> <span class="n">p_diff_chi2_fmc</span><span class="p">,</span> <span class="n">p_diff_fish_fmc</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sample size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;p diff&quot;&quot;&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span>
  <span class="s1">&#39;| chi^2 - fisher exact |&#39;</span><span class="p">,</span>
  <span class="s1">&#39;| chi^2 - fisher mc |&#39;</span><span class="p">,</span>
  <span class="s1">&#39;| fisher exact - fisher mc |&#39;</span><span class="p">],</span>
  <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>The result is:</p>
<p><img src="/images/fisher2.png" alt="Fisher's test, Fisher Monte Carlo and Chi-squared test p values" style="width: 600px;"/></p>
<p>This is a very interesting plot:</p>
<ul>
<li>The difference between Fisher’s exact and the MC is wiggling around 0 (green line), not exactly 0 because of the random nature of MC.</li>
<li>The difference between Fisher’s exact and the $\chi^2$ (blue line) converges to 0 smoothly, like the difference between the z-test and the binomial test in the introduction.</li>
<li>The difference between the $\chi^2$ and the MC (orange line) follows the blue line, but is a bit more random, again because of the random nature of MC.</li>
<li>We could get the MC to be smoother by letting it run longer, here it sampled $m=100,000$, we could let it run 10x longer for more smoothness.</li>
</ul>
<p>This gives us confidence that our MC implementation is correct. However, unlike Fisher’s exact test, and like the $\chi^2$ test, our Monte Carlo version also works on bigger contingency tables:</p>
<div class="highlight"><pre><span></span><span class="n">funnels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">],</span> <span class="mf">0.6</span><span class="p">],</span> <span class="c1"># the first vector is the actual outcomes,</span>
    <span class="p">[[</span><span class="mf">0.60</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">,</span> <span class="mf">0.20</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">],</span> <span class="c1"># the second is the traffic split</span>
    <span class="p">[[</span><span class="mf">0.70</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">observations</span> <span class="o">=</span> <span class="n">simulate_abtest</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
<span class="n">ch</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;chi-squared p = </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">ch</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">mcp</span> <span class="o">=</span> <span class="n">fisher_monte_carlo</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">num_simulations</span><span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;monte carlo p = </span><span class="si">%.3f</span><span class="s2">&quot;&quot;&quot;</span> <span class="o">%</span> <span class="n">mcp</span><span class="p">)</span>
</pre></div>


<p>Prints something like:</p>
<div class="highlight"><pre><span></span><span class="k">[[368. 113. 112.]</span>
 <span class="k">[126.  45.  55.]</span>
 <span class="k">[125.  26.  30.]]</span>
<span class="na">chi-squared p</span> <span class="o">=</span> <span class="s">0.076</span>
<span class="na">monte carlo p</span> <span class="o">=</span> <span class="s">0.078</span>
</pre></div>


<h2>Conclusion</h2>
<p>At this point we have 3 tests we can use for conversion tests: Fisher’s exact, Fisher MC and $\chi^2$. At low $N$, the Fisher ones are more accurate, the exact one (the scipy stats library implementation) only works on 2x2 contingency tables, while our MC one works for any $F \times C$ case. If we let the MC collect enough samples, the two yield the same results numerically. At high $N$, all 3 yield the same results numerically, the simplest thing to do is use the $\chi^2$ test.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/ab-testing.html">ab-testing</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "A/B testing and Fisher's exact test",
  "headline": "A/B testing and Fisher's exact test",
  "datePublished": "2020-03-03 00:00:00+01:00",
  "dateModified": "2020-03-03 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/ab-testing-and-fishers-exact-test.html",
  "description": "Fisher’s exact test directly computes the same p value as the Chi-squared test, so it does not rely on the Central Limit Theorem to hold."
}
</script></body>
</html>