<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="Fisher’s exact test directly computes the same p value as the Chi-squared test, so it does not rely on the Central Limit Theorem to hold." />
<meta name="keywords" content="ab-testing">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="A/B testing and Fisher's exact test"/>
<meta property="og:description" content="Fisher’s exact test directly computes the same p value as the Chi-squared test, so it does not rely on the Central Limit Theorem to hold."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/ab-testing-and-fishers-exact-test.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-03-03 00:00:00+01:00"/>
<meta property="article:modified_time" content="2020-03-03 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="ab-testing"/>
<meta property="og:image" content="/images/fisher2.png"/>

  <title>Bytepawn &ndash; A/B testing and Fisher's exact test</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="ab-testing-and-fishers-exact-test">A/B testing and Fisher's exact test</h1>
    <p>Posted on Tue 03 March 2020 in <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>Fisher’s exact test directly computes the same p value as the <a href="https://en.wikipedia.org/wiki/Chi-squared_test">$\chi^2$ test</a>, without relying on the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a> (CLT) to hold (normal approximations), so it is accurate at low $N$. See the previous post on <a href="http://bytepawn.com/ab-testing-and-the-chi-squared-test.html">A/B testing and the Chi-squared test</a>. The trade-off is that it’s significantly more intenstive computationally, so even at moderate $N$s <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo sampling</a> is the preferred route.</p>
<h2>The Binomial test</h2>
<p>The best way to understand Fisher’s test is by a simpler analogue, coin flips. Suppose somebody gives you a coin, and you’re trying to decide whether it’s a fair coin or not. If you can flip it a lot of times, you can use a <a href="https://en.wikipedia.org/wiki/Z-test">Z-test</a> to decide whether it’s fair or not, because at high $N$, the CLT holds, and the distribution of heads follows a normal distribution.</p>
<p>But, what if you’re only allowed to flip it $N=24$ times and you get $H=18$ heads? This is a low $N$, so the Z-test will not work. But, we can just directly compute the p value by computing $ P(H &gt;= 18 \vee H &lt;= 6) $ assuming the null hypothesis of a fair toin coss. Note that here we’re doing a two-tailed test here. $ P(H &gt;= 18 \vee H &lt;= 6) = P(H = 1) + ... + P(H = 6) + P(H = 18) + ... + P(H = 24)$, where $ P(H = k) = {n \choose k} p^k q^{n-k} $, where $p = 0.5, q = 1 - p = 0.5$ in this case from the null hypothesis ($p$ is the probability of heads, $q$ of tails).</p>
<p>What we’re doing here is called the <a href="https://en.wikipedia.org/wiki/Binomial_test">Binomial test</a>. The <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">scipy stats</a> package has a library function:</p>
<div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">binom_test</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
</pre></div>


<p>Prints:</p>
<div class="highlight"><pre><span></span><span class="mf">0.023</span>
</pre></div>


<p>We can calculate this ourselves per the above formula, we just have to be careful with the rounding:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">binomial_test</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">2.0</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">lo</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">2.0</span> <span class="o">-</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">hi</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">2.0</span> <span class="o">+</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">p_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">lo</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="nb">range</span><span class="p">(</span><span class="n">hi</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">p_value</span> <span class="o">+=</span> <span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="o">**</span><span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p_value</span>
</pre></div>


<p>This is a direct p value calculation, it works at any $N$. Let’s double-check what we know. According to the CLT, at high $N$ the average ratio of heads will follow a normal distribution, so we can use the Z-test, and it should yield the same result as the direct calculation above:</p>
<div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">H</span> <span class="o">=</span> <span class="mi">5100</span> <span class="c1"># delta of +1% lift</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># the null hypothesis</span>
<span class="n">raw_data</span> <span class="o">=</span> <span class="n">H</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">H</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Binom test p: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">binom_test</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Z-test p:     </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">ztest</span><span class="p">(</span><span class="n">x1</span><span class="o">=</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">p</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>


<p>Prints:</p>
<div class="highlight"><pre><span></span><span class="n">Binom</span> <span class="n">test</span> <span class="n">p</span><span class="p">:</span> <span class="mf">0.047</span>
<span class="n">Z</span><span class="o">-</span><span class="n">test</span> <span class="n">p</span><span class="p">:</span>     <span class="mf">0.045</span>
</pre></div>


<p>Not quite the same, but pretty close. We can see how the exact binomial and the normal estimated Z-test p values converge thanks to the CLT:</p>
<div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># the null hypothesis</span>
<span class="n">actual_lift</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">H</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">p</span> <span class="o">+</span> <span class="n">actual_lift</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">raw_data</span> <span class="o">=</span> <span class="n">H</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">H</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p_binom</span> <span class="o">=</span> <span class="n">binom_test</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">p_z</span> <span class="o">=</span> <span class="n">ztest</span><span class="p">(</span><span class="n">x1</span><span class="o">=</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">p</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">p_diff</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">p_binom</span> <span class="o">-</span> <span class="n">p_z</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">p_diff</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sample size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;|p z-test - p exact binomial|&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>The difference goes to zero in the $N \rightarrow \infty $ limit:</p>
<p><img src="/images/fisher1.png" alt="Binomial test and Z-test p value difference" style="width: 600px;"/></p>
<h2>Fisher’s exact test</h2>
<p>What the binomial test is to the Z-test, <a href="https://en.wikipedia.org/wiki/Fisher%27s_exact_test">Fisher’s exact test</a> is to the $chi^2$ test. It’s a direct calculation of the p value in case of a $F \times C$ contingency table. Fisher’s exact test is accurate at all $N$s, and the $chi^2$ test’s p converges to it at high $N$s, similar to the above case.</p>
<p>The null hypothesis is that all funnels have the same conversion event probabilities. Given the $F \times C$ contingency table outcome of an A/B test ($F$ funnels tested, $C$ mutually exclusive conversion events), the calculation of the p value is:</p>
<ol>
<li>first, calculate the marginals:</li>
<li>row marginals: how many users were randomly assigned into each funnel in the A/B test</li>
<li>column marginals: across the tested funnels, conversion event totals</li>
<li>given the marginals, what is the probability of the observed outcomes</li>
<li>for all the ways we can change numbers in the contingency tables while keeping the marginals fixed, take the ones that have equal or lower probability then the actual outcome, and add up those probabilities; this is the p value</li>
</ol>
<p>The trick, how to calculate the quantity “given the marginals, what is the probability of a specific outcome  (numbers in the contingency table that add up to the marginals)”; we need this in both step 2. and 3. For this we have to use the <a href="https://en.wikipedia.org/wiki/Hypergeometric_distribution">hypergeometric distribution</a>, the distribution for “urn draws”. Let’s reuse the contingency table from the previous post:</p>
<p><img src="/images/contingency_table3.PNG" alt="Contingency table" style="width: 600px;"/></p>
<p>Imagine this: we have a total of $N=10,000$ marbles. Each marble is one of $C=3$ colors (<strong>No conversion, Monthly, Annual</strong>). There are a total of 7,922 marbles <strong>No conversion</strong> marbles, 195 <strong>Monthly</strong> conversion marbles, etc. All these marbles are in one big urn. We start drawing marbles; what’s the probability that the first 5,916 drawn will be colored <strong>(No conversion, Monthly, Annual)=(4748, 595, 573)</strong>, irrespective or the order they are drawn? We can break this into two probabilities that we multiply: what is the probability that of 5,916 drawn the colors are <strong>(No conversion, Rest)=(4748, 595+573)</strong> from an urn that contains <strong>(No conversion, Rest)=(7922, 1085+993)</strong> marbles, multiplied by, what is the probability that of the rest 595+573=1,168 drawn the colors are <strong>(Monthly, Annual)=(595, 573)</strong> from an urn that contains <strong>(Monthly, Annual)=(1085, 993)</strong> marbles. These individual probabilities are given by the hypergeometric probability $P(X=k | N, K, n)$, ie. what is the probability of drawing $k$ red marbles from an urn that contains a total of $N$ marbles, $K$ of which are red, of total $n$ drawn ($k \leq n$). It is $P(X=k | N, K, n) = \frac{ { K \ choose k} { N-K \ choose n-k } }{ {N \choose n} }$.</p>
<p>Then, in the second row we calculate those probabilities, but keeping in mind that we have already removed (4748, 595, 573) marbles from the system. The ordering of the rows doesn’t matter! This can be implemented with the help of the <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.hypergeom.html">scipy hypergeometric probability function</a> ($P(X=k | N, K, n)$ above):</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hypergeom_probability</span><span class="p">(</span><span class="n">observations</span><span class="p">):</span>
    <span class="n">row_marginals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">col_marginals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">p</span> <span class="o">*=</span> <span class="n">hypergeom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">col_marginals</span><span class="p">[</span><span class="n">j</span><span class="p">:]),</span> <span class="n">col_marginals</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">row_marginals</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">row_marginals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span>
        <span class="n">col_marginals</span> <span class="o">-=</span> <span class="n">observations</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>


<p>We can now run an A/B test and whatever the outcome, we can compute the probability of that specific outcome, which will be a very small number:</p>
<div class="highlight"><pre><span></span><span class="n">funnels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="mf">0.6</span><span class="p">],</span> <span class="c1"># the first vector is the actual outcomes, the second is the traffic split</span>
    <span class="p">[[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[[</span><span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span><span class="o">*</span><span class="mi">1000</span>

<span class="n">observations</span> <span class="o">=</span> <span class="n">simulate_abtest</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">hypergeom_probability</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
</pre></div>


<p>But this is not the p value! This is just step 2. To get the p value, per step 3, we have to add up the probabilities this for all possible numbers in the contingency table that add up to the marginals, that have lower probability than the actual observations (=are more extreme).</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/ab-testing.html">ab-testing</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "A/B testing and Fisher's exact test",
  "headline": "A/B testing and Fisher's exact test",
  "datePublished": "2020-03-03 00:00:00+01:00",
  "dateModified": "2020-03-03 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/ab-testing-and-fishers-exact-test.html",
  "description": "Fisher’s exact test directly computes the same p value as the Chi-squared test, so it does not rely on the Central Limit Theorem to hold."
}
</script></body>
</html>