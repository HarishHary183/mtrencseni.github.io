<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset." />
<meta name="keywords" content="data, etl, workflow, airflow, fetchr, model, ml">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Fetchr Data Science Infra"/>
<meta property="og:description" content="A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/fetchr-data-science-infra.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-08-14 00:00:00+02:00"/>
<meta property="article:modified_time" content="2018-08-14 00:00:00+02:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="data"/>
<meta property="article:tag" content="etl"/>
<meta property="article:tag" content="workflow"/>
<meta property="article:tag" content="airflow"/>
<meta property="article:tag" content="fetchr"/>
<meta property="article:tag" content="model"/>
<meta property="article:tag" content="ml"/>
<meta property="og:image" content="">  <title>Bytepawn &ndash; Fetchr Data Science Infra</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="fetchr-data-science-infra">Fetchr Data Science Infra</h1>
    <p>Posted on Tue 14 August 2018 in <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>This is a quick follow-up to my <a href="http://bytepawn.com/fetchr-airflow.html#fetchr-airflow">previous post describing the Fetchr Data Science infra and philosophy</a>. The platform has expanded quite a bit in the last 6 months, and I'm currently approaching the end of the first year at Fetchr, so it's a good time to post an update.</p>
<p>The basic principles behind our infrastructure have not changed, but we have scaled it out horizontally in key functionalities, and we've added a small ML model prediction cluster. As of today, the cluster is about 20 nodes and looks like this:</p>
<p><img src="/images/fetchr-data-science-infra-update.png" alt="Fetchr Data Science Infra" style="width: 800px;"/></p>
<p>Architecture:</p>
<ol>
<li>S3 buckets (all data lives here)</li>
<li>two Presto clusters<ul>
<li>5 node Presto cluster for ETL and dashboards</li>
<li>5 node Presto cluster for analytics queries</li>
</ul>
</li>
<li>Airflow<ul>
<li>scheduler + webserver</li>
<li>worker</li>
<li>staging</li>
</ul>
</li>
<li>Superset<ul>
<li>1 node for dashboarding</li>
<li>1 node for analytics queries</li>
</ul>
</li>
<li>Jupyter host (Machine Learning notebooks)</li>
<li>2 node ML prediction cluster (blue+green)</li>
</ol>
<h2>S3</h2>
<p>As before, all data lives on S3, whether it's data imported from our production databases or data produced by the ETL. Data imported is stored in flat CSV files, whereas DWH tables produces by Airflow running Presto jobs are stored in ORC format (like at Facebook).</p>
<p>We continue to use the <code>ds</code> partitioned Data Warehouse architecture. This means that every night the ETL imports a fresh copy of all production tables into a new ds partition, like <code>ds=2018-08-01</code>, and all subsequent tables are also "re-created" in a new partition. Because all tables are backed on S3, this is also mirrored in our S3 path hierarchy. For example, the backing files for our main <code>company_metrics</code> table are divided like this:</p>
<p><img src="/images/company_metrics_s3.png" alt="A table on S3." style="width: 650px;"/></p>
<p>It's a fair question whether this is wasteful and slow? After all, we just imported all these tables last night, do we need to re-import the whole dataset again? Surely not everything has changed. As the company and our data volume grew, the nightly import actually started becoming quite slow, so we were forced to optimize this: for our big tables, now we import the (historic tail) once a week, and on a daily basis we only import data on orders that we received in the last ~5 months. This ensures our ETL finishes on time every night.</p>
<h2>Presto</h2>
<p>We currently run two EMR+Presto clusters, each 5 nodes. As before we don't run any ETL or queries on Hive/MapReduce, we exclusively use Presto for compute, since our queries never touch more than 10-100M rows.</p>
<p>We introduced a secondary cluster for ad-hoc analytics queries because, in cases when our ETL is slow and running during the day, or our regular hourly ETLs are running during the day, it kept blocking us from getting our work done.</p>
<p>Since all our data lives in S3, having two clusters see (read and write) the same data is not very hard. All we had to do is make sure our schemas are in sync on the two clusters. Since 99% of our schema operations are managed through Airflow jobs (<code>CREATE TABLE ...</code>), we just had to modify our Airflow framework to also execute these on the secondary cluster. Additionally, when we manually make changes to an existing table (<code>ALTER TABLE ...</code>), we have to execute this on both clusters, which is an inconveniance, but a minor one, and quite managable at this scale.</p>
<h2>Airflow</h2>
<p>We continue to use Airflow to be the backbone of our data crunching with great success. We have two nodes for production: one running the scheduler and webserver, and one running the worker processes. Since these nodes don't do any compute themselves (they just launch Presto <code>INSERT INTO ... SELECT</code> jobs), we did not need to scale out here so far, nor do we expect this to happen in the next year.</p>
<p>One the other hand, we have deepened our investment into Airflow as our standard ETL system. We have identified the 4-5 common Airflow use-cases we have (import from Postgres to S3, run an ETL job on Presto, export data to the legacy Redshift DWH, create dashbord screenshots and send in email, run Growth Accounting) and we have created helper functions to encapsulate them. As a result, the vast majority of our Airflow DAGs don't create Airflow operators directly, they call these library functions, which construct and return the DAGs. As a result our Airflow jobs look very clean, with all of the messy complexity hidden:</p>
<p><img src="/images/airflow_code_example.png" alt="A table on S3." style="width: 650px;"/></p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/data.html">data</a>
      <a href="/tag/etl.html">etl</a>
      <a href="/tag/workflow.html">workflow</a>
      <a href="/tag/airflow.html">airflow</a>
      <a href="/tag/fetchr.html">fetchr</a>
      <a href="/tag/model.html">model</a>
      <a href="/tag/ml.html">ml</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Fetchr Data Science Infra",
  "headline": "Fetchr Data Science Infra",
  "datePublished": "2018-08-14 00:00:00+02:00",
  "dateModified": "2018-08-14 00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/fetchr-data-science-infra.html",
  "description": "A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset."
}
</script></body>
</html>