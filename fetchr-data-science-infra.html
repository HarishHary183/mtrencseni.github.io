<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset." />
<meta name="keywords" content="data, etl, workflow, airflow, fetchr, model, ml">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Fetchr Data Science Infra"/>
<meta property="og:description" content="A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/fetchr-data-science-infra.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-08-14 00:00:00+02:00"/>
<meta property="article:modified_time" content="2018-08-14 00:00:00+02:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="data"/>
<meta property="article:tag" content="etl"/>
<meta property="article:tag" content="workflow"/>
<meta property="article:tag" content="airflow"/>
<meta property="article:tag" content="fetchr"/>
<meta property="article:tag" content="model"/>
<meta property="article:tag" content="ml"/>
<meta property="og:image" content="">  <title>Bytepawn &ndash; Fetchr Data Science Infra</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="fetchr-data-science-infra">Fetchr Data Science Infra</h1>
    <p>Posted on Tue 14 August 2018 in <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>This is a quick follow-up to my <a href="http://bytepawn.com/fetchr-airflow.html#fetchr-airflow">previous post describing the Fetchr Data Science infra and philosophy</a>. The platform has expanded quite a bit in the last 6 months, and I'm currently approaching the end of the first year at Fetchr, so it's a good time to post an update.</p>
<p>The basic principles behind our infrastructure have not changed, but we have scaled it out horizontally in key areas, and we've added a small ML prediction cluster. As of today, the Data Science infra is about 20 nodes and looks like this:</p>
<p><img src="/images/fetchr-data-science-infra-update.png" alt="Fetchr Data Science Infra" style="width: 800px;"/></p>
<p>Architecture:</p>
<ol>
<li>S3 buckets (all data lives here)</li>
<li>two Presto clusters<ul>
<li>5 node Presto cluster for ETL and dashboards</li>
<li>5 node Presto cluster for analytics queries</li>
</ul>
</li>
<li>Airflow<ul>
<li>scheduler + webserver</li>
<li>worker</li>
<li>staging</li>
</ul>
</li>
<li>Superset<ul>
<li>1 node for dashboarding</li>
<li>1 node for analytics queries</li>
</ul>
</li>
<li>Jupyter host (Machine Learning notebooks)</li>
<li>2 node ML prediction cluster (blue+green)</li>
</ol>
<h2>S3</h2>
<p>As before, all data lives on S3, whether it's data imported from our production databases or data produced by the ETL. Data imported is stored in flat CSV files, whereas DWH tables produces by Airflow running Presto jobs are stored in ORC format (like at Facebook). EMR/EC2 nodes never store data.</p>
<p>We continue to use the <code>ds</code> partitioned Data Warehouse architecture. This means that every night the ETL imports a fresh copy of all production tables into a new ds partition, like <code>ds=2018-08-01</code>, and all subsequent tables are also re-created in a new partition. Because all tables are backed on S3, this is also mirrored in our S3 path hierarchy. For example, the backing files for our main <code>company_metrics</code> table are divided like this:</p>
<p><img src="/images/company_metrics_s3.png" alt="A table on S3." style="width: 650px;"/></p>
<p>It's a fair question whether this is wasteful and slow? After all, we just imported all these tables last night, do we need to re-import the whole dataset again? Surely not everything has changed. As the company and our data volume grew, the nightly import actually started taking too long, so we were forced to optimize this: for our big tables, now we import the historic tail once a week, and on a daily basis we only import data on orders that we received in the last ~3 months. This ensures our ETL finishes on time every night.</p>
<h2>Presto</h2>
<p>We currently run two EMR+Presto clusters, each 5 nodes. As before we don't run any ETL or queries on Hive/MapReduce, we exclusively use Presto for compute, since our queries never touch more than 10-100M rows.</p>
<p>We introduced a secondary cluster for ad-hoc analytics queries because, in cases when our ETL is slow and running during the day, or our regular hourly ETLs are running during the day, it kept blocking us from getting our work done.</p>
<p>Since all our data lives in S3, having two clusters see (read and write) the same data is not very hard. All we had to do is make sure our schemas are in sync on the two clusters. Since 99% of our schema operations are managed through Airflow jobs (<code>CREATE TABLE IF NOT EXISTS ...</code>), we just had to modify our Airflow framework to also execute these on the secondary cluster. Additionally, when we manually make changes to an existing table (<code>ALTER TABLE ...</code>), we have to execute this on both clusters, which is an inconveniance, but a minor one, and quite managable at this scale.</p>
<h2>Airflow</h2>
<p>We continue to use Airflow to be the backbone of our data pipelines with great success. We have two nodes for production: one running the scheduler and webserver, and one running the worker processes. Since these nodes don't do any compute themselves (they just launch Presto <code>INSERT INTO ... SELECT</code> jobs), we did not need to scale out here so far, nor do we expect this to happen in the next year.</p>
<p>One the other hand, we have deepened our investment into Airflow as our standard ETL system wrt code. We have identified the 4-5 common Airflow use-cases we have (import from Postgres to S3, run an ETL job on Presto, export data to the legacy Redshift DWH, create dashbord screenshots and send in email, run Growth Accounting) and we have created helper functions to encapsulate them. As a result, the vast majority of our Airflow DAGs don't create Airflow operators directly, instead they call these library functions, which construct and return the DAGs. As a result our Airflow jobs look very clean, with all of the messy complexity hidden away:</p>
<p><img src="/images/airflow_code_example.png" alt="A table on S3." style="width: 650px;"/></p>
<p>This had a big pay-off when we introduced our secondary Presto cluster a few months ago, and we needed to automtically create all our schemas there. We just added extra operators to our library functions to run schema operations on the secondary, and the next night when the ETL ran, all our table schemas were created on the secondary Presto cluster, pointing to the backing S3 files, ready to go. We were running analytics queries on the secondary cluster the day after we spun it up!</p>
<p>Currently we have 76 DAGs in Airflow, importing and exporting from 5-10 data sources (3 production databases, S3, 2 Presto clusters, legacy Redshift, DynamoDB, various custom extracts sent to clients and the ML cluster).</p>
<h2>Superset</h2>
<p>Superset is both a dashboarding system and has an SQL IDE (called SQL Lab), which our Data Scientists use as their primary tool for accessing data. We continue to use Superset for dashboarding and have spun up a secondary Superset instance just for queries. Why? </p>
<p>We have hit a limitation with Superset, where Superset will launch a new gunicorn process for each page request, and if the page happens to be a dashboard, for each chart on the dashboard. The chart processes will launch a Presto query each, which could take 10-60 seconds to return. Some of our dashboards have a lot of charts on them (20+), plus we have many concurrent users accessing dashboards (or running queries). In cases like this, Superset runs out of worker processes, and it becomes totally unresponsive. Each worker process eats up 1-2G of RAM, so the number of processes it can run are limited.</p>
<p>As an initial workaround, we split the dashboarding and querying use-case into two Superset instances, so analysts are not blocked by the dashboards. Additionally, we broke large dashboards into smaller pieces (which is unfortunate). This way, running 32 worker processes each, both instances are good for every-day work at current loads.</p>
<p>Both Airflow and Superset are still rough around the edges, but since Superset is user-facing, it can create more problems. It still happens that we want to look at a dashboard but the webserver times out because Superset ran out worker processes (maybe because the Presto queries are slow, because they're running on the same cluster as the ETL, and the ETL is slow, because something changed in production). Right now we get by with work-arounds (for example, in the previous case, we re-direct the Superset dashboarding traffic to the analytics cluster temporarily by changing a connection string, until ETL finishes). So far Superset is good enough for internal Data Science / Understand dashboards, and we do have a fair number of colleagues using it on a daily basis. But admittedly we will need to invest more time into understanding how to tune if we want to deploy it to a company-wide audience and sleep well.</p>
<p>Currently we have 26 dashboards in Superset, many of them viewed by CxOs and country General Managers every day.</p>
<h2>Machine Learning</h2>
<p>In the last 3 months we have rolled out a prediction model in production. We perform the Data Science work to arrive at the models on our laptops and/or on a dedicated Jupyter host, in ipython notebooks. Once we're happy with the result, we deploy the model to our ML prediction cluster. We don't yet have CI/CD set up for it, deployment is manual and requires domain knowledge.</p>
<p>The model is already running in production and is being used for on-the-ground delivery operations, so downtime is not acceptable. We quickly switched to two node blue/green model: both nodes are running identical code/data, we deploy to green first, if it goes well, then to blue. Both are behind an Elastic Load Balancer (<code>predict</code> happens over a HTTP API call), so things keep going if one of them is down, even if down for a long time.</p>
<p>Fresh data is loaded every night by an Airflow process: first it creates a daily dump for the ML models, uploads it to S3, and then triggers a special <code>load</code> API call, first on the green, then the blue host. The Airflow job for blue depends on green, so if green fails, it won't touch blue, so production will not be impacted.</p>
<p><img src="/images/zone_prediction_dag.png" alt="A table on S3." style="width: 650px;"/></p>
<p>At this scale/complexity, this simple model works quite well and we have excellent uptime; we have more problems coming from software bugs than availability.</p>
<h2>Impact and Conclusion</h2>
<p>Over the last year, a very small ~2.5 person Data Science team was able to have an outsized multi-million dollar impact on Fetchr by using data to understand and optimize operations and business processes. Today, the most important operational decisions are based on data, including sizing our fleets, warehouses, understanding their performance, and sometimes introducing ML to automate away human labor. This is possible by using AWS for low-level infra, Presto to run complex queries without having to do custom tuning (or MapReduce), Airflow to manufacture the tables that allow us to get to metrics fast, and Superset to visualize it all. And being able to copy a lot of templates we saw at Facebook. The architecture descibed here works quite well</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/data.html">data</a>
      <a href="/tag/etl.html">etl</a>
      <a href="/tag/workflow.html">workflow</a>
      <a href="/tag/airflow.html">airflow</a>
      <a href="/tag/fetchr.html">fetchr</a>
      <a href="/tag/model.html">model</a>
      <a href="/tag/ml.html">ml</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Fetchr Data Science Infra",
  "headline": "Fetchr Data Science Infra",
  "datePublished": "2018-08-14 00:00:00+02:00",
  "dateModified": "2018-08-14 00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/fetchr-data-science-infra.html",
  "description": "A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset."
}
</script></body>
</html>