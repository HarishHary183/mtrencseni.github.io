<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="When working with hypothesis testing, the desciptions of the statistical method often has normality assumptions. For example, the Wikipedia page for the z-test starts like this: "A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution". What does this mean? How do I know it’s a valid assumption for my data?" />
<meta name="keywords" content="data, ab testing, statistics">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="A/B testing and the Central Limit Theorem"/>
<meta property="og:description" content="When working with hypothesis testing, the desciptions of the statistical method often has normality assumptions. For example, the Wikipedia page for the z-test starts like this: "A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution". What does this mean? How do I know it’s a valid assumption for my data?"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/ab-testing-and-the-central-limit-theorem.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-02-05 00:00:00+01:00"/>
<meta property="article:modified_time" content="2020-02-05 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="ab-testing"/>
<meta property="article:tag" content="data"/>
<meta property="article:tag" content="ab testing"/>
<meta property="article:tag" content="statistics"/>
<meta property="og:image" content="/images/normal_from_uniform.PNG"/>

  <title>Bytepawn &ndash; A/B testing and the Central Limit Theorem</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="ab-testing-and-the-central-limit-theorem">A/B testing and the Central Limit Theorem</h1>
    <p>Posted on Wed 05 February 2020 in <a href="/category/ab-testing.html">ab-testing</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>Data Scientists run lots of A/B tests, whether they’re working on SaaS products, social networking, logistics or self-driving cars. A/B testing is a form of hypothesis testing, a decision-making method powered by statistics. By following the rules of hypothesis testing we make sure we have gathered enough and strong enough evidence to support our decisions.</p>
<p>When working with hypothesis testing, the desciptions of the statistical methods often have normality assumptions. For example, <a href="https://en.wikipedia.org/wiki/Z-test">the Wikipedia page for the z-test starts like this</a>:</p>
<blockquote>
<p>A Z-test is any statistical test for which the distribution of the test statistic under <strong>the null hypothesis can be approximated by a normal distribution</strong>.</p>
</blockquote>
<p>This a common source of confusion. What does it mean that <em>“the test statistic under the null hypothesis can be approximated by a normal distribution”</em>? And whatever that is, how do I know it’s  a valid assumption for my data?</p>
<p><a href="https://github.com/mtrencseni/playground/blob/master/AB%20testing%20and%20the%20Central%20Limit%20Theorem.ipynb">Note: the code shown below is up on Github</a>.</p>
<h2>A/B testing</h2>
<p>Let’s take a concrete example. Let’s suppose that we’re doing an A/B test, and we’re measuring two metrics for variants A and B: a $C$ conversion rate (at what rate do people convert) and a $T$ timespent (how many minutes do they spend in the product). In both case, our null hypothesis would be <em>“A and B are the same”</em>, which is translated into the language of mathematics like:</p>
<ul>
<li>$ H_0 $ null hypothesis for conversion rate: $ C_A = C_B $</li>
<li>$ H_0 $ null hypothesis for timespent: $ T_A  = T_B $</li>
</ul>
<p>Here comes the important part: in the above expression, $ C_A $ and $ C_B $ are the conversion rate, and $ T_A $ and $ T_B $ are the average timespent minutes. Both of these quantities are averages: for the conversion rate, we can imagine a conversion counting as a 1 and a non-conversion as a 0, and the conversion rate is the average of this random variable (like coinflips). The timespent is also computed by adding up the individual timespent minutes and dividing by the number of samples.</p>
<p>And this is the key: the z-test (and many other tests) work only if these averages can be approximated by a normal distribution. <strong>So it’s not the distribution of conversions or the distributions of timespents which must be normal.</strong> In fact, these do not follow a normal distribution at all! The conversions are 0s and 1s and follow a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>, like toin cosses. Timespents usually follow an exponential-looking drop-off in SaaS products. <strong>But, given a big enough sample size, the distribution of averages computed from samples can in fact be approximated by a normal.</strong> This is the guarantee of the Central Limit Theorem (CLT). A final technical note: the test statistic is actually the difference of the means: for example, $ T_A = T_B $ can be reformulated as $ T_A - T_B = 0 $, and it is this difference (normalized) that is the test statistic. Fortunately, <a href="https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables">independent normal distributions have a very nice additive property</a>: if $ X $ and $ Y $ are i.i.d. normal random variables, then $ Z = X + Y $ and $ W = X - Y $ are also normals. So if $ T_A $ and  $ T_B $ are normal, so is the test statistic $ T_A - T_B $.</p>
<h2>The Central Limit Theorem</h2>
<p>Without further ado, the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a>:</p>
<blockquote>
<p>In probability theory, the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a "bell curve") even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.</p>
<p>For example, suppose that a sample is obtained containing many observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic mean of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the distribution of the average will be closely approximated by a normal distribution. A simple example of this is that if one flips a coin many times the probability of getting a given number of heads in a series of flips will approach a normal curve, with mean equal to half the total number of flips in each series; in the limit of an infinite number of flips, it will equal a normal curve.</p>
</blockquote>
<p>In layman’s terms, the CLT says that: given a population P, with some metric M whose true average is $ \mu_M $, and you take a random sample of independent measurements from P and take the average $ a_M $, then $ a_M $ follows a normal distribution. Note that the error of the measurement, $ e_M = \mu_M - a_M $ also follows a normal distribution since $ \mu_M $ is a constant, this is the quantity most closely related to the $ H_0 $ null hypothesis' test statistic. I will not discuss the exact mathematical formulation here.</p>
<h2>Simulation</h2>
<p>We can "prove" this to ourselves by running simulations:</p>
<ol>
<li>First, we will define a population with some distribution. It doesn’t have to be normal, it can be uniform, exponential, whatever.</li>
<li>Then we take <code>sample_size</code> samples and compute the mean.</li>
<li>We do the above step <code>num_sample</code> times, so we have <code>num_sample</code> means.</li>
<li>Then we plot these means and according to the CLT, we should see a nice bell curve centered on the true mean of the original population distribution (ie. the mean of the uniform, the mean of the exponential, etc).</li>
</ol>
<p>Let's use <a href="https://www.scipy.org/">scipy</a>, it gives us convenient ways to sample standard distributions. First, a function which samples a given distribution and shows a histogram of the samples against the <a href="https://en.wikipedia.org/wiki/Probability_density_function">probability density function</a> of the distribution. We can use this to check that we're doing the right thing:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">population_sample_plot</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">population</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="nb">min</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span> <span class="o">/</span> <span class="mf">4.0</span>
    <span class="n">resolution</span> <span class="o">=</span> <span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="nb">min</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span> <span class="o">/</span> <span class="mf">100.0</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">padding</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">+</span> <span class="n">padding</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">population</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>Let's visualize a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)">uniform</a>, an <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential</a> and a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> using the above function:</p>
<div class="highlight"><pre><span></span><span class="n">population_sample_plot</span><span class="p">(</span><span class="n">uniform</span><span class="p">)</span>
<span class="n">population_sample_plot</span><span class="p">(</span><span class="n">expon</span><span class="p">)</span>
<span class="n">population_sample_plot</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span>
</pre></div>


<p><img src="/images/uniform_exp_normal_histo.PNG" alt="Uniform, exponential and normal distributions" style="width: 400px;"/></p>
<p>So these are the original population distributions, and we're trying to estimate the mean by drawing samples. Let's write code to do this:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">population_sample_mean_plot</span><span class="p">(</span><span class="n">population</span><span class="p">,</span> <span class="n">sample_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">sample_means</span> <span class="o">=</span> <span class="p">[</span><span class="n">mean</span><span class="p">(</span><span class="n">population</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sample_size</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)]</span>
    <span class="n">mn</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">sample_means</span><span class="p">)</span>
    <span class="n">mx</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">sample_means</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">mx</span> <span class="o">-</span> <span class="n">mn</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">rng</span> <span class="o">/</span> <span class="mf">4.0</span>
    <span class="n">resolution</span> <span class="o">=</span> <span class="n">rng</span> <span class="o">/</span> <span class="mf">100.0</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mn</span> <span class="o">-</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mx</span> <span class="o">+</span> <span class="n">padding</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">sample_means</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">sample_means</span><span class="p">))</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample_means</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>Let's see the distribution of the means for the same three distributions:</p>
<div class="highlight"><pre><span></span><span class="n">population_sample_mean_plot</span><span class="p">(</span><span class="n">uniform</span><span class="p">)</span>
<span class="n">population_sample_mean_plot</span><span class="p">(</span><span class="n">expon</span><span class="p">)</span>
<span class="n">population_sample_mean_plot</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span>
</pre></div>


<p><img src="/images/uniform_exp_normal_mean_sample.PNG" alt="Uniform, exponential and normal distribution sample means" style="width: 400px;"/></p>
<p>The Central Limit Theorem works!The distribution of the means, for uniform, exponential and normal distributions is a normal distribution, about the true population mean.</p>
<p>Note that the CLT also works for discrete distributions such as the <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>, ie. conversions:</p>
<div class="highlight"><pre><span></span><span class="n">population_sample_mean_plot</span><span class="p">(</span><span class="n">bernoulli</span><span class="p">)</span>
</pre></div>


<p><img src="/images/bernoulli_mean_sample.PNG" alt="Bernoulli sample means" style="width: 400px;"/></p>
<h2>Conclusion</h2>
<p>The Central Limit Theorem is the reason why, when you're doing A/B testing on averages (such as conversion or average timespents), the normality assumption for hypothesis testing is justified.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/data.html">data</a>
      <a href="/tag/ab-testing.html">ab testing</a>
      <a href="/tag/statistics.html">statistics</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "A/B testing and the Central Limit Theorem",
  "headline": "A/B testing and the Central Limit Theorem",
  "datePublished": "2020-02-05 00:00:00+01:00",
  "dateModified": "2020-02-05 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/ab-testing-and-the-central-limit-theorem.html",
  "description": "When working with hypothesis testing, the desciptions of the statistical method often has normality assumptions. For example, the Wikipedia page for the z-test starts like this: "A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution". What does this mean? How do I know it’s a valid assumption for my data?"
}
</script></body>
</html>