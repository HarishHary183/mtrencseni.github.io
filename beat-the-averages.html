<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="When working with averages, we have to be careful. There are pitfalls lurking to pollute our statistics and results reported." />
<meta name="keywords" content="data, statistics">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Beat the averages"/>
<meta property="og:description" content="When working with averages, we have to be careful. There are pitfalls lurking to pollute our statistics and results reported."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/beat-the-averages.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-07-07 00:00:00+02:00"/>
<meta property="article:modified_time" content="2018-07-07 00:00:00+02:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="data"/>
<meta property="article:tag" content="statistics"/>
<meta property="og:image" content="">  <title>Bytepawn &ndash; Beat the averages</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="beat-the-averages">Beat the averages</h1>
    <p>Posted on Sat 07 July 2018 in <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p><em>The numbers in this article are made up, but the lessons come from real life.</em></p>
<p>When doing Data Science, we almost always report averages. This is natural, because it stands for a simple model that everybody can understand. For example, in the delivery business, a topline metric is Deliveries per Driver (DPD), the average deliveries made per driver per day. This is a simple model the CEO can also remember: if our fleet is performing at DPD = 40, and we have 1,000 drivers, we make 40,000 deliveries per day. Being able to multiply two topline numbers and get a third one is a good thing.</p>
<p>When working with averages, we have to be careful though: there are pitfalls lurking to pollute our statistics and results reported. It is important to note that there is nothing wrong with averages themselves, we just have to be careful with them. I don’t believe that for most reporting purposes averages should or can be replaced (eg. by reporting the median), it is simply the job of the Data Science team to make sure the metrics make sense.</p>
<h2>Outliers</h2>
<p>When we say that our DPD is 40 and we have 1,000 drivers, the natural inclination (even for data people) is to <em>imagine</em> 1,000 equivalent drivers, each performing exactly 40 deliveries every day. But we know that the world isn’t this simple. Things like driver performance tend to follow some more interesting distribution. The simplest thing we can imagine is that it follows a normal distribution. The plot below shows a normal distribution, the mean (green) and median (red) coincide. Gauss is happy.</p>
<p><img src="/images/averages-1.png" alt="Probability distribution" style="width: 650px;"/></p>
<p>But almost always there are outliers. In the case of drivers, there are various special circumstances which can cause a driver to have very low or very high DPD. For example, maybe the driver got sick, interrupted his route and went home early. Below is a the same distribution as above, with some stragglers introduced. We can see that this shifts the mean (green) down.</p>
<p><img src="/images/averages-2.png" alt="Probability distribution" style="width: 650px;"/></p>
<p>The shift in the mean is important, because it signals that something is going on: a bunch of our drivers got sick and went home early. Maybe tomorrow they are not coming to work. So monitoring both the average and median is important to detect and understand deviations.</p>
<p>Apart from the median, which is also called the 50th <a href="https://en.wikipedia.org/wiki/Percentile">percentile</a>, checking out the bottom and top percentiles is also very helpful. Below is the same two plots, with p10 and p90 also showing in red.</p>
<div>
<img src="/images/averages-3.png" alt="Probability distribution" style="width: 325px;"/>

<img src="/images/averages-4.png" alt="Probability distribution" style="width: 325px;"/>
</div>

<p>Something really useful happened! After we introduced the stragglers, the p10 dropped from about 27 to about 8!</p>
<p>In general, showing percentiles is a useful technique, but as the example above shows, they can dramatically speed up detection of interesting events. In real life work, looking at distribution doesn’t happen on a daily basis, but a timeseries showing the historic DPD can also show p10, median and p50, and can show such events. The chart below shows such a made-up example, showing p10, p50 and p90 in red, the average in green, for the last 30 days for the fleet. On the 25th day a flu started spreading between our drivers, introducing the stragglers as shown in the distribution above. The mean and the median separate somewhat, but the p10 gives it away.
It’s worth showing all four lines, at least on internal, debug dashboards.</p>
<p><img src="/images/averages-5.png" alt="Probability distribution" style="width: 650px;"/></p>
<p>It’s worth noting the outliers can come from another source too: data bugs. Another good trick is to periodically examine low and high performers in a table, attached to the bottom of the internal report/dashboard.</p>
<p>Finally, outlier/anomaly detection can also be automated, for example Facebook does this internally for various metrics.</p>
<p>It’s important to automate at least the visualization of these in a debug dasboard (even if anomaly detection is not running), because in the long-run, Data Scientists will forget to check manually (export and plot in ipython takes time).</p>
<h2>Several populations</h2>
<p>Another reason averages can be polluted is because of multiple populations. (Outliers can also be thoughtof as a population.) In the delivery business, it is not uncommon to have many separate fleets of drivers, for different purposes. For example, we may have a B2C and a C2C fleet. Another distinction is cars vs bikes. Uber could have a fleet for passengers and a totally separaste fleet for UberEats. Below is a (made-up) distribution that’s actually two fleets, a C2C fleet performing at DPD=20 and a B2C one at DPD=40.</p>
<p><img src="/images/averages-6.png" alt="Probability distribution" style="width: 650px;"/></p>
<p>In cases like this, reporting on the blended mean (or any other statistic) may or may not be a mistake. For example, if country X has a B2C and a C2C fleet, while country Y only has a B2C fleet, then the reporting just on country-wise DPD will be weird, because for country X the C2C fleet will pull the DPD down. But this doesn’t mean that the Ops team in country X is performing better, in fact it’s possible their B2C fleet is outperforming country Y’s.</p>
<h2>Skewed distributions</h2>
<p>Sometimes distributions are not symmetric, they can be lopsided. In this case the median, mode (the most frequent outcome, the maximum of the distribution) and mean can be at different locations, which is often unintuitive for people. This isn’t a problem wrt the mean, but it’s good to know. The lognormal distribution is one such example:</p>
<p><img src="/images/averages-7.png" alt="Probability distribution" style="width: 650px;"/></p>
<h2>Percentage instead of mean</h2>
<p>Sometimes, when building a metric, the mean is not a good choice. Let’s take page or app load times an an example. Suppose we measure the average pageload time in miliseconds, and we see that it is 4,200ms, too high. After rolling out changes, it goes down to 3,700ms. However, 3,700 is still too high. Does that mean the rollout didn’t make a dent?</p>
<p>In situations like this, it makes sense to bake the goal into the metric. Suppose our goal is 2,000ms, which we deem pleasant from a UX perspective. Then a better way to define the metric is % of pageloads that are within 2,000ms. If it was 57% before, and 62% after the rollout, it’s more natural to understand what happens: an additional 5% of people now have a pleasant experience when loading our page. If there are 1,000,000 users per month, we impacted 50,000 users per month with the rollout. Not bad.</p>
<p>Another big advantage of using percentages like this is that it’s more resilient to outliers. While the mean could be polluted by outliers (users on slow connections, bots, data bugs), in the % it will be “just” a constant additive factor.</p>
<h2>Ratios instead of means</h2>
<p>Our delivery business also has C2C, ie. people can call a driver and send a package to another person, in real time. For example, if my partner is at the airport, but they forgot their passport at home, I can use the C2C app to fetch a car and send her the passport in an envelope. As such, the C2C app has standard metrics such as Daily Active Users (DAU) and Monthly Active Users (MAU). These are topline metrics, but we also need a metric which expresses how often people use the product. One way to do it using means would be to count, for each user, how many days they were DAU of the last 28 days. Suppose we call this Average DAU, and it’s 5.2. This is not that hard to understand, but could still be confusing. For example, people always forget the definition of a metric, in this case they would forget if the metric is 28 and 30 day (or 7?) based. Also, increments like this don’t feel natural, for example a +1% increment corresponds to +0.28 active days or 6.72 hours.</p>
<p>A better metric is simply to divide DAU per MAU, this is a common metric also used inside Facebook. This feels more natural: if we are honest with ourselves, a user is essentially a MAU, because somebody who hasn’t used the product for 28 days is probably not coming back. Thinking like this DAU/MAU is a very natural metric: it is the % of users who use the product daily. (Depending on the product, the denominator could be WAU or MAU or quarterly acives). </p>
<h2>Daily variations</h2>
<p>Suppose our fleet’s average DPD is 40. Looking at driver X, his DPD yesterday was 29. Is he a low performer? Our first intuition might be to ask what the standard deviation of the fleet is, and then argue that this value is not “significantly” off. Suppose it is 10. But from a business perspective, this in itself doesn’t help: if the COO wants to improve DPD and is looking for low performing drivers to cut, cutting at mean minus one sigma may be valid from a business perspective.</p>
<p>However, it’s possible that our drivers have significant daily variation in their performance. It’s possible that this driver has a DPD of 29 before, but the previous day it was 47, and their historic average is actually 42. Always compare averages to averages. In this case, compare the fleet’s average DPD over a long enough timeframe (probably at least 28 days) to the driver’s average DPD in the same 28 days. That is a more fair comparison to make, because it smooths daily variation. Of course, remember what was said here, for example don’t count days when the driver was sick, compare him to his own fleet, and also look at percentiles. </p>
<h2>In summary</h2>
<ol>
<li>Using averages is okay most of the time.</li>
<li>Reporting on medians is probably not feasible in a business/product setting.</li>
<li>Instead, make sure the average is meaningful.</li>
<li>Watch out for outliers.</li>
<li>Check median/p10/p90 and distributions regularly.</li>
<li>Prune/separate outliers.</li>
<li>Split up populations, etc. to make sure the reported average (or median) is a meaningful number.</li>
<li>Outliers can be real outliers, or issues in the data (eg. Self Pickup as a driver)</li>
<li>Often there are two or more populations making up the distribution, and the average is not a meaningful number (example: B2C vs C2C fleet DPD for drivers).</li>
<li>Sometimes the population is homogeneous, but the distribution is skewed to one side or bimodal, in this case the average is intuitively misleading.</li>
<li>Sometimes, using %s instead of averages makes a better metric (example: Scheduling Accuracy vs Average Scheduling Error).</li>
<li>Sometimes, using a ratio instead of averages makes a better metric (example: DAU/MAU vs average number of DAUs in the last 28 days).</li>
<li>Be careful when comparing daily snapshots and averages, there may be significant daily variation in performance.</li>
</ol>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/data.html">data</a>
      <a href="/tag/statistics.html">statistics</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Beat the averages",
  "headline": "Beat the averages",
  "datePublished": "2018-07-07 00:00:00+02:00",
  "dateModified": "2018-07-07 00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/beat-the-averages.html",
  "description": "When working with averages, we have to be careful. There are pitfalls lurking to pollute our statistics and results reported."
}
</script></body>
</html>