<!DOCTYPE html>
<html lang="en">
<head>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="I will show how to solve the standard Ax=b matrix equation with PyTorch. This is a good toy problem to show some guts of the framework without involving neural networks." />
<meta name="keywords" content="pytorch">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="PyTorch Basics: Solving the Ax=b matrix equation with autograd"/>
<meta property="og:description" content="I will show how to solve the standard Ax=b matrix equation with PyTorch. This is a good toy problem to show some guts of the framework without involving neural networks."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/pytorch-basics-solving-the-axb-matrix-equation-with-autograd.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-02-08 00:00:00+01:00"/>
<meta property="article:modified_time" content="2019-02-08 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="pytorch"/>
<meta property="og:image" content="/images/computational-graph.PNG"/>

  <title>Bytepawn &ndash; PyTorch Basics: Solving the Ax=b matrix equation with autograd</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="pytorch-basics-solving-the-axb-matrix-equation-with-autograd">PyTorch Basics: Solving the Ax=b matrix equation with autograd</h1>
    <p>Posted on Fri 08 February 2019 in <a href="/category/machine-learning.html">Machine Learning</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>PyTorch is my favorite deep learning framework. It’s a hacker’s framework, It’s easier to work with than Tensorflow, which was developed for Google’s internal use-cases and ways of working, which just doesn’t apply to use-cases that are several orders of magnitude smaller (less data, less features, less prediction volume, less people working on it). There are tons of <a href="https://pytorch.org/tutorials/">great tutorials</a> and <a href="https://www.youtube.com/watch?v=_H3aw6wkCv0">talks</a> that help people quickly get a deep neural network model up and running with PyTorch. <a href="https://pytorch.org/docs/stable/torchvision/datasets.html">PyTorch comes with standard datasets (like MNIST)</a> and <a href="https://pytorch.org/docs/stable/torchvision/models.html">famous models (like Alexnet) out of the box</a>.</p>
<p>What actually goes on under the hood in these examples? At its core libraries like PyTorch are essentially computing derivatives of functions, and backpropagating the gradients in a computational graph; this is called autograd. This can also be applied to solve problems that don’t explicitly involve a deep neural network.</p>
<p><img src="/images/computational-graph.PNG" alt="PyTorch computational graph" style="width: 300px;"/></p>
<p>I will show how to solve the standard <strong>Ax=b</strong> matrix equation with PyTorch. This is a good toy problem to show some guts of the framework without involving neural networks.</p>
<p><img src="/images/axb.PNG" alt="Ax=b" style="width: 400px;"/></p>
<p>As a reminder, in the <strong>Ax=b</strong> matrix equation, <strong>A</strong> is a fixed matrix, <strong>b</strong> is a fixed vector, and we’re looking for vector <strong>x</strong> such that <strong>Ax</strong> is just the vector <strong>b</strong>. If <strong>A</strong> is a 1x1 matrix, then this is just a scalar equation, and <strong>x=b/A</strong>. Let’s write this as <strong>x=A<sup>-1</sup> b</strong>, and then this applies to the n x n matrix case as well: the exact solution is to compute the inverse of <strong>A</strong>, and multiply it by <strong>b</strong>. (Note: the  technical conditions for a solution is <strong>det A ≠ 0</strong>, I'll ignore this since I'll be using random matrices). Let’s say the solution is <strong>x=x<sub>s</sub></strong>.</p>
<p><em>Note: using gradient descent to estimate the solution would be incredibly stupid in real life, as the solution can be computed quickly with matrix inversion. We're doing this to understand PyTorch on a toy problem.</em></p>
<p>How can we use <a href="https://pytorch.org/docs/stable/autograd.html">PyTorch and autograd</a> to solve it? We can use it to approximate the solution: start with some random <strong>x<sub>0</sub></strong>, compute the vector <strong>A x<sub>0</sub> - b</strong>, take the norm <strong>L =  ‖ A x<sub>0</sub> - b   ‖</strong>, and use a gradient descent to find a next, better <strong>x<sub>1</sub></strong> vector so that it’s closer to the real solution <strong>x<sub>s</sub></strong>. They key idea is that for <strong>x=x<sub>s</sub></strong>, the norm is <strong>L =    ‖ A x<sub>s</sub> - b   ‖ = 0</strong>, so we essentially want to minimize that. This <strong>L</strong> is called the loss function in such optimization problems.</p>
<p>Let’s start with the standard L2 norm:</p>
<p><img src="/images/l2-definition.PNG" alt="L2 norm definition" style="width: 150px;"/></p>
<p>This will result in a standarad parabolic loss function, where we will converge on the minimum.</p>
<p><img src="/images/parabola2d.PNG" alt="L2 norm parabola" style="width: 300px;"/></p>
<h2>Code</h2>
<p>This is what the PyTorch code for setting up <strong>A, x</strong> and <strong>b</strong> looks like. We initialize <strong>A</strong> and <strong>b</strong> to random:</p>
<script src="https://gist.github.com/mtrencseni/d0f65883f2be329cac1ec390869d02e0.js"></script>

<p>We set <code>requires_grad</code> is set to <code>False</code> for <strong>A</strong> and <strong>b</strong>. These are constants in this scenario, their gradient is zero. <strong>x</strong> is the variable which we will compute gradients for, so we set <code>requires_grad = True</code>.</p>
<script src="https://gist.github.com/mtrencseni/a232e345680f17f0c00ce816036308f1.js"></script>

<p>We then tell PyTorch to do a backward pass and compute the gradients:</p>
<script src="https://gist.github.com/mtrencseni/4583fdf66ea02a4d4756103c734074c5.js"></script>

<p>At this point, PyTorch will have compute the gradient for <strong>x</strong>, stored in <code>x.grad.data</code>. What this means is “adjust <strong>x</strong> in this direction, by this much, to decrease the loss function, given what <strong>x</strong> is right now”. Now we just need to introduce a step size to control our speed of descent, and actually adjust <strong>x</strong>:</p>
<script src="https://gist.github.com/mtrencseni/c1929a58684b2962c4008b24564b48a0.js"></script>

<p>Almost done. We just need to set <code>step_size</code>, put this in a for loop, and figure out when to stop it:</p>
<script src="https://gist.github.com/mtrencseni/3e1b3d47f55fdb5a54518312d298940a.js"></script>

<p>Let’s stop it when the loss is smaller than <code>stop_loss</code>, and put an upper bound on the number of iterations:</p>
<script src="https://gist.github.com/mtrencseni/d6be2e6b35f100be1a92e6726f5210c2.js"></script>

<p>It’s a good exercise to play around with this. Set a specific <strong>A</strong> and <strong>b</strong>, print things out, try other dimensions, etc.</p>
<h2>Discussion</h2>
<p><strong>How is this related to neural networks?</strong><br/>
Imagine <strong>A</strong> as a bitmap image, and <strong>b</strong> as some sort of multi-classification (probability) output, and <strong>x</strong> are the weights. We’re trying to get the neural network to learn the classification, in this trivial example, where the training set size is 1. Note that in real machine learning scenarios, the loss function (potential surface) is in a higher dimensional space and has several minima; the goal is not to find the global optimum (untractable), just a good enough local one (gets the job done).</p>
<p><img src="/images/potential-surface.PNG" alt="Potential surface" style="width: 300px;"/></p>
<p><strong>How do we know what step_size should be? How do we know when to stop?</strong><br/>
In this case, because the loss function we’re optimizing is quadratic, we can get away with a fixed step_size, as the gradient will get smaller as we approach the optimum (the solution). In more complicated deep neural network scanarios (where the step size is called the learning rate), there are strategies on how to gradually decay the step size. In general, if the step size is too small, we waste a lot of time far away from the solution. If it’s too big, we jump around the optimum and may never converge. See also <a href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1">Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning</a>.</p>
<p><img src="/images/learning-rate.PNG" alt="Learning rate" style="width: 300px;"/></p>
<p><strong>What should the stopping condition be?</strong><br/>
In this toy example, the <code>step_size</code> is set smaller than <code>stop_loss</code>, so it can converge on the optimum below the accepted loss. If you would set the <code>stop_loss</code> to be much smaller than the step size, you will see that it never stops, it will jump around the optimum (left side of above picture).</p>
<p><strong>What exactly is the quantity <code>x.grad.data</code>?</strong><br/>
It stored the gradient of the loss function we called backward() on, with respect to the variable, in this case <strong>x</strong>. So in the dim=1 case it’s just <strong>dL/dx</strong>. Note that since <strong>x</strong> is the only independent variable here, the partial derivative is the total derivative. If we had multiple independent variables, we'd have to add the partial derivates to get the <a href="https://en.wikipedia.org/wiki/Total_derivative">total derivative</a>.</p>
<p><strong>What if we use the L1 norm as the loss function?</strong><br/>
To use the L1 norm, set <code>p=1</code> in the code. The L1 norm in <code>dim=1</code> is the <strong>abs()</strong> function, so it’s derivative is piecewise constant. In this case the slope is <strong>+-   ‖A  ‖</strong>.</p>
<p><img src="/images/l1-norm.PNG" alt="L1 norm" style="width: 300px;"/></p>
<p>This is “less nice” than the L2 norm for this simple case, because the gradient doesn’t vanish as the solution approaches the optimum. The solution is more likely to overshoot the optimum and oscillate. In this case this toy example is not representative of machine learning: real-world training sets often contain outliers, and L1 is more robust against outliers than L2.</p>
<p><strong>What happens if we run this on the GPU vs CPU?</strong><br/>
To turn on the GPU, put this line at the top:</p>
<script src="https://gist.github.com/mtrencseni/ae8c2b405025c21a63c6562d8dc7dcff.js"></script>

<p>On my computer, this is significantly slower on the GPU than the CPU, because copying such a small problem to the GPU creates a time overhead which is not worth it. This toy example is too small to demonstrate how GPUs speed deep learning.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/pytorch.html">pytorch</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "PyTorch Basics: Solving the Ax=b matrix equation with autograd",
  "headline": "PyTorch Basics: Solving the Ax=b matrix equation with autograd",
  "datePublished": "2019-02-08 00:00:00+01:00",
  "dateModified": "2019-02-08 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/pytorch-basics-solving-the-axb-matrix-equation-with-autograd.html",
  "description": "I will show how to solve the standard Ax=b matrix equation with PyTorch. This is a good toy problem to show some guts of the framework without involving neural networks."
}
</script></body>
</html>