<!DOCTYPE html>
<html lang="en">
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="I gave this talk at the O’Reilly Strata Conference London in 2016 June, mostly based on what I learned at Prezi from 2012-2016." />
<meta name="keywords" content="ab-testing">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="A/B tests: Moving Fast vs Being Sure"/>
<meta property="og:description" content="I gave this talk at the O’Reilly Strata Conference London in 2016 June, mostly based on what I learned at Prezi from 2012-2016."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/ab-tests-moving-fast-vs-being-sure.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-07-01 00:00:00+02:00"/>
<meta property="article:modified_time" content="2019-07-01 00:00:00+02:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="ab-testing"/>
<meta property="article:tag" content="ab-testing"/>
<meta property="og:image" content="/images/ab-testing-base.png"/>

  <title>Bytepawn &ndash; A/B tests: Moving Fast vs Being Sure</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="ab-tests-moving-fast-vs-being-sure">A/B tests: Moving Fast vs Being Sure</h1>
    <p>Posted on Mon 01 July 2019 in <a href="/category/ab-testing.html">ab-testing</a></p>
  </header>
  <div>
    <p>The basic flow of an A/B test (in a frequentist setting) is roughly:
- formulate a hypothesis ("sending additional notifications will cause people to be available for deliveries")
- select a target metric ("Delivery Performance = Deliveries/Dispatches") and specify the base value ("75%")
- estimate the lift on the target metric ("1%")
- use an <a href="https://www.evanmiller.org/ab-testing/sample-size.html">off-the-shelf A/B testing tool to figure out how many N samples you will need</a>, given the base metric value and expected lift</p>
<p><img src="/images/ab-testing-evanmiller-sample-size.png" alt="Sample size" style="width: 650px;"/></p>
<p>At this point, in the A/B testing tool, there are 2 magic parameters that the Data Scientist has to set for the Chi-squared test:</p>
<ul>
<li><strong>α:</strong>   the probability that, if the experiment goes off, the effect is actually not there (<strong>False Positive Rate, FPR</strong>, aka sensitivity)</li>
<li><strong>1-β:</strong> also called the <strong>power</strong>, this is the probability that, if the effect is there, the experiment will go off (<strong>True Positive Rate, TPR</strong>)</li>
</ul>
<p>Most tools will default α=0.05 and 1-β=0.8, which is a sensible default. With these, the A/B testing tool will tell the Data Scientist how many N samples she needs to collect per variant (control and test) to be able to detect the given lift (1%) from the base value (75%), with the specified statistics (FPR, TPR).</p>
<p>By pushing down the FPR, we can increase our confidence in our results. More power will enable us to catch more working experiments. But there is no free lunch, we will need to collect more N samples for this. So what is a good trade-off here? Almost all tools default to the values above, but there is nothing special about them.</p>
<p>The reason I started thinking about this is that in a startup setting, when there are low volumes [for the subset we're testing, eg. a specific city for a delivery company], it takes a lot of days to collect the N samples, which slows down the product development velocity. This is essentially a management concern: if A/B testing is seen by management to be a burden that slows things down, then it won't happen, which is a net loss. Often I feel that it's probably fine to run at α=0.1 or even α=0.2, being wrong 20% of the time is not too terrible in a startup setting when no lives are at stake.</p>
<p>So let's try to quantify this with a toy model. We continue with the assumptions above:</p>
<ul>
<li>base metric = 75%</li>
<li>expected lift = +1%
Let's look at three different scenarios, α=0.05, α=0.10 and α=0.20.
We use <a href="https://www.evanmiller.org/ab-testing/sample-size.html">Evan Miller's A/B testing sample size tool</a> to read off the N samples required.
We continuosly run experiments for 365 days. At higher α, we need less N, so we can run more experiments, that's the point (but we will get more false positives, FPs). Let's assume that 1 in 4 experiments actually yield a hit, which results in the desired +1% lift. Let's quantify the $ value of a hit at $100,000 (we realize this on true positives, TPs), and the cost of a rollout at $25,000 (we incur this cost on both TP and FP hits). This is what the three scenarios look like, annualized for easy readability, <a href="https://docs.google.com/spreadsheets/d/1thsMPiUAd4WYbxZI4cVB1zyZqcYW9lCZHqPrR9X3gFQ">the spreadsheet is here</a>:</li>
</ul>
<p><img src="/images/ab-testing-base.png" alt="A/B testing base case" style="width: 650px;"/></p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/ab-testing.html">ab-testing</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "A/B tests: Moving Fast vs Being Sure",
  "headline": "A/B tests: Moving Fast vs Being Sure",
  "datePublished": "2019-07-01 00:00:00+02:00",
  "dateModified": "2019-07-01 00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/ab-tests-moving-fast-vs-being-sure.html",
  "description": "I gave this talk at the O’Reilly Strata Conference London in 2016 June, mostly based on what I learned at Prezi from 2012-2016."
}
</script></body>
</html>