<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="The t-test is a better version of z-tests for timespent data, because it explicitly models the uncertainty of the variance due to sampling." />
<meta name="keywords" content="ab-testing">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="A/B testing and the t-test"/>
<meta property="og:description" content="The t-test is a better version of z-tests for timespent data, because it explicitly models the uncertainty of the variance due to sampling."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/ab-testing-and-the-ttest.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-02-23 00:00:00+01:00"/>
<meta property="article:modified_time" content="2020-02-23 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="ab-testing"/>
<meta property="og:image" content="/images/t-test-5-10.png"/>

  <title>Bytepawn &ndash; A/B testing and the t-test</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="ab-testing-and-the-ttest">A/B testing and the t-test</h1>
    <p>Posted on Sun 23 February 2020 in <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>In the last post, I showed how to do <a href="http://bytepawn.com/ab-testing-and-the-ztest.html">A/B testing with the z-test</a>. I used two examples:</p>
<ul>
<li>conversions, ie. proportions ($X_A$ out of $N_A$ converted)</li>
<li>timespents (timespents for A were $x_i, x_2 ... x_N$)</li>
</ul>
<p>In this post, let’s concentrate on timespent data. The t-test is a better version of z-tests for timespent data, because it explicitly models the uncertainty of the variance due to sampling. The <a href="https://en.wikipedia.org/wiki/Student%27s_t-test">Wikipedia page for Student’s t-test</a>:</p>
<blockquote>
<p>The t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. A t-test is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known. When the scaling term is unknown and is replaced by an estimate based on the data, the test statistics (under certain conditions) follow a Student's t distribution. The t-test can be used, for example, to determine if the means of two sets of data are significantly different from each other.</p>
</blockquote>
<p><a href="https://github.com/mtrencseni/playground/blob/master/AB%20testing%20and%20the%20ttest.ipynb">The code shown below is up on Github.</a></p>
<h2>The t-test vs the z-test</h2>
<p>What does this mean? Before I talked about the z-test, <a href="http://bytepawn.com/ab-testing-and-the-central-limit-theorem.html">I wrote about the Central Limit Theorem (CLT)</a>. The CLT essentially says that as we collect more independent samples from a population, we can estimate the true mean of the population by averaging our samples. The distribution of our estimate will be a normal distribution around the true mean, with variance $ \sigma_2 = \sigma_p^2 / N $, where $\sigma_p$ is the true standard deviation of the population. The population mean and standard deviation should exist, but the population doesn’t have to be normally distributed, eg. it can be exponential.</p>
<p>When we perform a z-test, we model the distribution as a normal variable, with mean $ \mu = \frac{1}{N} \sum{ x_i } $ and variance $ \sigma^2 = s^2/N $, where $ s^2 = \frac{1}{N} \sum{(\mu - x_i)^2} $. The problem is, we cheated a little: we used $s^2$ and not $\sigma_p^2$! We do this because we don’t know $\sigma_p^2$, all we have is the estimate $s^2$.</p>
<p>The t-test models this uncertainty in the estimation of $ \sigma^2 $. When we perform a t-test, it feels very similar to the z-test, except in some places we write $N-1$ instead of $N$. And in the end, we don’t look up a $z$ value on a normal distribution, instead we look up a $t$ value on a <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">t-distribution</a>:</p>
<blockquote>
<p>In probability and statistics, Student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and the population standard deviation is unknown. If we take a sample of n observations from a normal distribution, then the t-distribution with $ \nu =n-1 $ degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term $ \sqrt {n} $. In this way, the t-distribution can be used to construct a confidence interval for the true mean.</p>
</blockquote>
<h2>The normal distribution vs the t-distribution</h2>
<p>As in the previous posts, we use the <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">scipy.stats</a> module, which has pdfs for both normal and t-distributions. Compared to a standard normal distribution, the t-distribution has an additional parameter called $\nu$ or degrees of freedom (dof). When using the t-distribution on sample size $N$, $ \nu = N-1 $. Let’s plot a standard normal (blue) and t’s with $\nu=5$ (green) and $\nu=10$ (orange):</p>
<p><img src="/images/t-test-5-10.png" alt="Normal distribution vs t-distribution" style="width: 400px;"/></p>
<p>Note how the t-distributions have a bell shape like the normal, but have lower maximum and fatter tails.</p>
<p>Next, let’s plot a standard normal (blue) and a t with $\nu=100$ (green):</p>
<p><img src="/images/t-test-100.png" alt="Normal distribution vs t-distribution" style="width: 400px;"/></p>
<p>At a moderate sample size of $N=100$ there is effectively no difference between the distributions.</p>
<h2>The t-test becomes the z-test at $ N = 50 $</h2>
<p>As $ N \rightarrow \infty $:</p>
<ul>
<li>the t-distribution becomes a normal distribution</li>
<li>the final outcome of hypothesis testing, the p-value becomes identical for a t-test and a z-test.</li>
</ul>
<p>The difference effectively disappears at around $N=50$ sample size. So if you’re performing a timespent A/B test, and you have 100s or more samples in each bucket, the t-test and the z-test will yield numerically identical results. This is becauce at such sample sizes, the estimate of $s^2$ for $\sigma_p^2$ becomes really good for estimating the mean, and it’s divided by $N$ anyway, so the importance of the estimate goes down with increasing $N$.</p>
<p>When googling for “z test vs t test”, a lot of advice goes like “use the t-test if you don’t know the variance” and “use the t-test for $N&lt;50$. This is not incorrect, but it’s a bit confusing. For A/B testing, a clear and concise statement is: <strong>in A/B testing you never know the population mean, you’re just estimating it, so always use the t-test. For $N&gt;50$, the t-test numerically yields the same results as the z-test, there's no difference.</strong></p>
<h2>Simulating p-values</h2>
<p>Let’s perform a Monte-Carlo simulation of A/B tests using the <a href="https://www.statsmodels.org/stable/stats.html">statsmodel</a> package, which has both z and t tests (1 sided and 2 sided). Let’s assume we have true populations for A and B, we take some samples from both to estimate the mean, and we perform both a t-test and a z-test to get 1-sided p-values. We then compute the average and maximum absolute p-value difference:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simulate_p_values</span><span class="p">(</span><span class="n">population_A</span><span class="p">,</span> <span class="n">population_B</span><span class="p">,</span> <span class="n">sample_size_A</span><span class="p">,</span> <span class="n">sample_size_B</span><span class="p">,</span> <span class="n">num_simulations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">p_diffs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
        <span class="n">sample_A</span> <span class="o">=</span> <span class="n">population_A</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sample_size_A</span><span class="p">)</span>
        <span class="n">sample_B</span> <span class="o">=</span> <span class="n">population_B</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">sample_size_B</span><span class="p">)</span>
        <span class="n">t_stat</span> <span class="o">=</span> <span class="n">ttest_ind</span><span class="p">(</span><span class="n">sample_A</span><span class="p">,</span> <span class="n">sample_B</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;larger&#39;</span><span class="p">)</span>
        <span class="n">z_stat</span> <span class="o">=</span> <span class="n">ztest</span><span class="p">(</span><span class="n">sample_A</span><span class="p">,</span> <span class="n">sample_B</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;larger&#39;</span><span class="p">)</span>
        <span class="n">p_diff</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">z_stat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span>  <span class="n">t_stat</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">p_diffs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_diff</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean</span><span class="p">(</span><span class="n">p_diffs</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">p_diffs</span><span class="p">)</span>
</pre></div>


<p>Let’s see what happens if we assume that both A and B are identical exponentials (so the null hypothesis is true), and we very sample size from 10 to 500:</p>
<div class="highlight"><pre><span></span><span class="n">p_diffs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample_size</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">):</span>
    <span class="n">p_diff</span> <span class="o">=</span> <span class="n">simulate_p_values</span><span class="p">(</span>
        <span class="n">population_A</span><span class="o">=</span><span class="n">expon</span><span class="p">,</span>
        <span class="n">population_B</span><span class="o">=</span><span class="n">expon</span><span class="p">,</span>
        <span class="n">sample_size_A</span><span class="o">=</span><span class="n">sample_size</span><span class="p">,</span>
        <span class="n">sample_size_B</span><span class="o">=</span><span class="n">sample_size</span>
    <span class="p">)</span>
    <span class="n">p_diffs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p_diff</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sample size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;|p z-test - p t-test|&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_diffs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>The output is (blue is mean, orange is maximum absolute p-value difference across 100 A/B tests performed at each sample size):</p>
<p><img src="/images/pz-same.png" alt="p-values for t and z tests" style="width: 400px;"/></p>
<p>Note that:</p>
<ul>
<li>the difference tend to 0 with increasing sample size</li>
<li>the p-value differences shown are on the order of 0.001, in real life we usually work with p-values between 0.01 and 0.05</li>
</ul>
<p>Let’s see what happens when the A/B test is actually working, B has better timespent on average (so the null hypothesis is false). To make the effect more visible, let’s pretend that timespent doubled. For this, we just have to change the lines:</p>
<div class="highlight"><pre><span></span>    <span class="n">population_A</span><span class="o">=</span><span class="n">expon</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">population_B</span><span class="o">=</span><span class="n">expon</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
</pre></div>


<p>in the above code. This yields:</p>
<p><img src="/images/pz-different.png" alt="p-values for t and z tests" style="width: 400px;"/></p>
<p>Comparing with the above (null hypothesis is true), in such a case the p-value difference drops even quicker. This makes sense: if there is an effect (null hypothesis is false), the tests return a lower p-value, so the difference will also be lower.</p>
<h2>Conversions</h2>
<p>Are t-tests also better then z-tests for conversions? When dealing with conversions, we always know the underlying distribution is a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial</a>, with variance $np(1-p)$. But we don’t know $p$, that’s what we’re trying to measure. In any case, for conversions, the better solution versus the z-test is not the t-test, it’s the $\chi^2$-test, which I’ll cover in the next post. The reason the $\chi^2$-test is better than both the z and and t-test is that is explicitly models the binomial, and the fact that p is bounded between 0 and 1 (and the difference between A and B is bounded between -1 and 1).</p>
<h2>Conclusion</h2>
<p>In a timespent A/B test scenario, we should always use the t-test. Both the t and z-tests are a library call, so there’s no difference in effort. For real-life high sample size use-cases, numerically there’s no difference in the p-values computed. However, the z-test is much simpler mental model, as it models the test statistic as the well-known normal distribution and there’s no degrees of freedom involved like in the t-distribution. So, my rule of thumb: <strong>use the t-test, pretend it’s a z-test</strong>.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/ab-testing.html">ab-testing</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "A/B testing and the t-test",
  "headline": "A/B testing and the t-test",
  "datePublished": "2020-02-23 00:00:00+01:00",
  "dateModified": "2020-02-23 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/ab-testing-and-the-ttest.html",
  "description": "The t-test is a better version of z-tests for timespent data, because it explicitly models the uncertainty of the variance due to sampling."
}
</script></body>
</html>