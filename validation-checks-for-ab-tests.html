<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="google-site-verification" content="14dvQcWyDWHBo8aM0kld8XzEa6KypAzCQDz1_KPus9E" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn - Marton Trencseni Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="A/B tests go wrong all the time, even in sophisticated product teams. As this article shows, for a range of problems we can run automated tests to catch problems early, before they have too bad of an effect on the customer or the business. These tests can look for big differences, differences in distribution, and test various statistical properties of the funnels to catch likely problems. Large tech companies are running such validation checks automatically and continuously for their online experiments." />
<meta name="keywords" content="ab-testing">
<meta property="og:site_name" content="Bytepawn - Marton Trencseni"/>
<meta property="og:title" content="Validation checks for A/B tests"/>
<meta property="og:description" content="A/B tests go wrong all the time, even in sophisticated product teams. As this article shows, for a range of problems we can run automated tests to catch problems early, before they have too bad of an effect on the customer or the business. These tests can look for big differences, differences in distribution, and test various statistical properties of the funnels to catch likely problems. Large tech companies are running such validation checks automatically and continuously for their online experiments."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/validation-checks-for-ab-tests.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-04-16 00:00:00+02:00"/>
<meta property="article:modified_time" content="2020-04-16 00:00:00+02:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="ab-testing"/>
<meta property="og:image" content="/images/k-s.png"/>

  <title>Bytepawn - Marton Trencseni &ndash; Validation checks for A/B tests</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <!--<h2><a href="">Bytepawn - Marton Trencseni</a></h2>-->
      <h2><a href="http://bytepawn.com">Bytepawn</a></h2>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="validation-checks-for-ab-tests">Validation checks for A/B tests</h1>
    <p>Marton Trencseni - Thu 16 April 2020 - <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p><em>"Anything that can go wrong will go wrong"</em>, according to <a href="https://en.wikipedia.org/wiki/Murphy%27s_law">Murphy’s law</a>. A/B testing is no different. Anybody who has run a lot of A/B tests, over several years, has seen multiple failure modes. Some errors I've seen (assuming “B” is the new experimental funnel in the A/B test):</p>
<ul>
<li><strong>misconfigured test</strong>: we want to do a 20%-80% split, but we accidentally configure 80%-20% split instead</li>
<li><strong>accidentally still running</strong>: we conclude the test, and want to remove A or B from production, but accidentally leave the test on with the original traffic split</li>
<li><strong>randomization bias</strong>: users are not assigned into A and B randomly</li>
<li><strong>logging problem</strong>: no/less/faulty logs coming from B</li>
<li><strong>buggy product</strong>: a software bug in B causes users to drop out</li>
</ul>
<p>Let’s look at how we can automatically catch some of these problems. Note that in all of the tests below, we are doing a <a href="https://en.wikipedia.org/wiki/One-_and_two-tailed_tests">two-tailed test</a>, we want to catch deviations in either direction.</p>
<p><a href="https://github.com/mtrencseni/playground/blob/master/Validation%20checks%20for%20AB%20tests.ipynb">The code shown below is up on Github.</a></p>
<h2>Problem: misconfigured test, missing exposure logs</h2>
<p>This is the easiest to catch, assuming we have an independent validation system that is correctly configured. On the one hand, if this is the case, we can simply write code that checks whether the configured traffic splits match in production vs validation. However, this is often not possible, because eg. the production configuration is hardcoded into Python or Java source code files.</p>
<p>In this case, we can perform validation on the exposure logs. Exposure log just means a log entry which is generated when a user is assigned into the funnels A or B. We can do a check using the one-way <a href="https://en.wikipedia.org/wiki/Chi-squared_test">$\chi^2$ test</a>: if we expect an 80%-20% split between A and B (were 80-20 is typed a second time in the validation check), we can check how likely it is that the exposure log counts for A and B are coming from that distribution. If there is a misconfiguration, we will get a very low p-value, and can alert on it:</p>
<div class="highlight"><pre><span></span><span class="n">N</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">traffic_split</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
<span class="n">user_counts</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1987</span><span class="p">,</span> <span class="mi">8013</span><span class="p">]</span> <span class="c1"># bad</span>
<span class="c1">#user_counts = [8013, 1987] # good</span>
<span class="c1"># simulates a case where we accidentally switched A and B</span>
<span class="c1"># so the test would return a very low ~0 p value, indicating</span>
<span class="c1"># that it&#39;s very unlikely that the observed counts are coming from</span>
<span class="c1"># the indicated traffic_split</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">chisquare</span><span class="p">(</span><span class="n">user_counts</span><span class="p">,</span> <span class="n">f_exp</span><span class="o">=</span><span class="p">[</span><span class="n">N</span><span class="o">*</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">traffic_split</span><span class="p">])[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="n">p_crit</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Low p value (</span><span class="si">%f</span><span class="s1">). Probably indicated badly configured test, or bad logs!&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Everything seems good.&#39;</span><span class="p">)</span>
</pre></div>


<p>For automation, use a very small critical p-value like 0.001 or 0.0001, since we’re not interested in statistical fluctuations, we want to catch misconfiguration, which will even at moderate sample sizes yield a very small p value, close to 0.</p>
<p>Note that here we’re not doing a significance test on the outcome of the A/B test. We’re just making sure the split is what we think it is. In the example above, we assumed that out of 10,000 impressions, 1,987 were in the A funnel, and it’s configured to get 80% of the traffic. The $\chi^2$ test that then tells us this is an extremely unlikely outcome. Note that the above $\chi^2$ test can be run for experiments with more than 2 funnels.</p>
<p><a href="http://bytepawn.com/ab-testing-and-the-chi-squared-test.html">See my earlier post on the $\chi^2$ test.</a></p>
<h2>Problem: accidentally still running</h2>
<p>For this problem, the easiest sanity check is not statistical, it’s a consistency check in code/reporting:</p>
<ul>
<li>have a report of running A/B tests, which is based on exposure logs only, irrespective of whether the experiment is turned on or off in the experiment configuration tool</li>
<li>have an alert, so that if an A/B test is turned off in the experiment configuration tool, but the experiment is still producing exposure logs, an alert is generated</li>
</ul>
<p>In a good A/B testing culture, there is separate library/framework for running experiments in code. For a good example see <a href="https://github.com/facebook/planout">Planout</a>, an open-source framework released by Facebook. Here is the demo example from Planout:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">planout.experiment</span> <span class="kn">import</span> <span class="n">SimpleExperiment</span>
<span class="kn">from</span> <span class="nn">planout.ops.random</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">class</span> <span class="nc">FirstExperiment</span><span class="p">(</span><span class="n">SimpleExperiment</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">userid</span><span class="p">):</span>
    <span class="n">params</span><span class="o">.</span><span class="n">button_color</span> <span class="o">=</span> <span class="n">UniformChoice</span><span class="p">(</span><span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;#ff0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00ff00&#39;</span><span class="p">],</span> <span class="n">unit</span><span class="o">=</span><span class="n">userid</span><span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">button_text</span> <span class="o">=</span> <span class="n">WeightedChoice</span><span class="p">(</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Join now!&#39;</span><span class="p">,</span> <span class="s1">&#39;Sign up.&#39;</span><span class="p">],</span>
        <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="n">unit</span><span class="o">=</span><span class="n">userid</span><span class="p">)</span>

<span class="n">my_exp</span> <span class="o">=</span> <span class="n">FirstExperiment</span><span class="p">(</span><span class="n">userid</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># parameters may be accessed via the . operator</span>
<span class="k">print</span> <span class="n">my_exp</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;button_text&#39;</span><span class="p">),</span> <span class="n">my_exp</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;button_color&#39;</span><span class="p">)</span>

<span class="c1"># experiment objects include all input data</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
  <span class="k">print</span> <span class="n">FirstExperiment</span><span class="p">(</span><span class="n">userid</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</pre></div>


<p>Exposure logs should be generated in this line: <code>my_exp = FirstExperiment(userid=12)</code>, when the user “enters the funnel” and the framework randomly decided on A or B.</p>
<h2>Problem: randomization bias</h2>
<p>Sometimes the software engineer who implements some of the A/B testing code isn’t math-savvy, and makes a simple mistake. For example, he may think that <code>user_id % 2 == 0</code> is a good enough way to do a 50%-50% split.</p>
<p>Note: Another related fallacy is to use and re-use fixed buckets (10 buckets, <code>user_id % 10</code>), and use different buckets for different A/B tests. This is a problem for a number of reasons: it unnecessarily lowers the sample size, <a href="http://bytepawn.com/running-multiple-ab-tests-in-parallel.html">see this previous post on parallel testing</a>. Also, this introduces bias, because we keep reusing the same groups of users, so if a previous A/B test influenced users in the first bucket in a certain way, we may still be measuring that effect in later, unrelated A/B tests.</p>
<p>This can be tested by running a $\chi^2$-squared test on the last digits (or last 2 digits) of the <code>user_id</code>s. The below code simulates this check:</p>
<div class="highlight"><pre><span></span><span class="n">MAX_USER_ID</span><span class="o">=</span><span class="mi">1000</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">N</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">traffic_split</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
<span class="n">digits_to_test</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">users_in_experiment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">MAX_USER_ID</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">users_in_funnels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">traffic_split</span><span class="p">:</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">s</span><span class="o">*</span><span class="n">N</span><span class="p">)</span>
    <span class="n">users_in_funnels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">users_in_experiment</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">end</span>

<span class="n">p_min</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">expected_split</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">digits_to_test</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">10</span><span class="o">**</span><span class="n">digits_to_test</span>
<span class="k">for</span> <span class="n">id_list</span> <span class="ow">in</span> <span class="n">users_in_funnels</span><span class="p">:</span>
    <span class="n">digit_counts</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Counter</span><span class="p">([</span><span class="n">x</span> <span class="o">%</span> <span class="mi">10</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">id_list</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="n">expected_counts</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">id_list</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">expected_split</span><span class="p">]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">chisquare</span><span class="p">(</span><span class="n">digit_counts</span><span class="p">,</span> <span class="n">f_exp</span><span class="o">=</span><span class="n">expected_counts</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">p_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p_min</span><span class="p">)</span>
<span class="k">if</span> <span class="n">p_min</span> <span class="o">&lt;</span> <span class="n">p_crit</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Low p value (</span><span class="si">%f</span><span class="s1">). Probably indicated badly configured test, or bad logs!&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Everything seems good.&#39;</span><span class="p">)</span>
</pre></div>


<h2>Problem: logging problem or buggy product</h2>
<p>The kinds of tests we can run here are similar to the tests above. For example, if the product is buggy and the new dialog doesn’t load at all, we would see that with a test like in the first example, because logs would be missing for B.</p>
<p>Similar tests can be performed on properties (segmentS) of users. For example, we can look at the country of the user (US, Europe, Rest), browser (Chrome, Firefox, Edge/IE, Rest), platform (desktop, mobile) and make sure that the splits are the same. For example, if the browser split (Chrome, Firefox, Edge/IE, Rest) of users in A is (0.4, 0.2, 0.3, 0.1) and for B it is (0.5, 0.24, 0.24, 0.02) we can run a  $\chi^2$-squared test (with counts) on the vector pairs to see how likely it is that they are coming from the same distribution; maybe our new B version doesn’t render correctly on all browsers, or there are performance differences.</p>
<p>Another thing we can do is to run the test for the final target metric (eg. conversion or timespent), and see if the difference is unusually large. As above, we can set an aggressive p-value like (0.001 or 0.0001) and alert on that. This test would go off if for example in one funnel due to a software problem it’s impossible to convert, or lots of users with a certain browser are unable to convert, so the the conversion number is unrealistically suppressed.</p>
<p>The problem with the above is that, at large sample sizes, which happens at companies with billions of users, even small lifts will be very significant, so achieve a low p value. In such cases a better validation check is to compare the distribution, assuming that in case of a software error we would see a noticably different distribution. For this we can use the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov test</a>, which tests how likely it is that two samples are coming from the same distribution. The test statistic used by the K-S test is the largest difference in the cumulative probability distribution function.</p>
<p><img src="/images/k-s.png" alt="Kolmogorov-Smirnov" style="width: 600px;"/></p>
<p>Code, which notifies us that a uniform distribution is not normal:</p>
<div class="highlight"><pre><span></span>
</pre></div>


<h2>Conclusion</h2>
<p>A/B tests go wrong all the time, even in sophisticated product teams. As this article shows, for a range of problems we can run automated tests to catch problems early, before they have too bad of an effect on the customer or the business. These tests can look for big differences, differences in distribution, and test various statistical properties of the funnels to catch likely problems. Large tech companies are running such validation checks automatically and continuously for their online experiments.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/ab-testing.html">ab-testing</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Validation checks for A/B tests",
  "headline": "Validation checks for A/B tests",
  "datePublished": "2020-04-16 00:00:00+02:00",
  "dateModified": "2020-04-16 00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/validation-checks-for-ab-tests.html",
  "description": "A/B tests go wrong all the time, even in sophisticated product teams. As this article shows, for a range of problems we can run automated tests to catch problems early, before they have too bad of an effect on the customer or the business. These tests can look for big differences, differences in distribution, and test various statistical properties of the funnels to catch likely problems. Large tech companies are running such validation checks automatically and continuously for their online experiments."
}
</script></body>
</html>