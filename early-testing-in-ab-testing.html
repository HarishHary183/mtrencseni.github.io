<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="Increased false positive rate due to early stopping is beautiful nuance of statistical testing. It is equivalent to running at an overall higher alpha. Data scientists need to be aware of this phenomenon so they can control it and keep their organizations honest about their experimental results." />
<meta name="keywords" content="ab-testing">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Early stopping in A/B testing"/>
<meta property="og:description" content="Increased false positive rate due to early stopping is beautiful nuance of statistical testing. It is equivalent to running at an overall higher alpha. Data scientists need to be aware of this phenomenon so they can control it and keep their organizations honest about their experimental results."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/early-testing-in-ab-testing.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-03-05 00:00:00+01:00"/>
<meta property="article:modified_time" content="2020-03-05 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="ab-testing"/>
<meta property="og:image" content="/images/early_stopping.png"/>

  <title>Bytepawn &ndash; Early stopping in A/B testing</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="early-testing-in-ab-testing">Early stopping in A/B testing</h1>
    <p>Posted on Thu 05 March 2020 in <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>In the past posts we’ve been computing p values for various <a href="https://en.wikipedia.org/wiki/Frequentist_inference">frequentist</a> statistical tests that are useful for A/B testing (<a href="http://bytepawn.com/ab-testing-and-the-ztest.html#ab-testing-and-the-ztest">Z-test</a>, <a href="http://bytepawn.com/ab-testing-and-the-ttest.html#ab-testing-and-the-ttest">t-test</a>, <a href="http://bytepawn.com/ab-testing-and-the-chi-squared-test.html#ab-testing-and-the-chi-squared-test">Chi-squared</a>, <a href="http://bytepawn.com/ab-testing-and-fishers-exact-test.html#ab-testing-and-fishers-exact-test">Fisher's exact</a>). When we modeled the A/B test, we assumed the protocol is:</p>
<ol>
<li>decide what metric we will use to evaluate the test (eg. conversion, timespent, DAU)</li>
<li>dedice how many $N$ samples we will collect</li>
<li>decide what type of test (eg. t-test or $\chi^2$) we will use</li>
<li>decide $\alpha$ acceptable false positive rate (FPR)</li>
<li>collect $N$ samples</li>
<li>compute $p$ value, if $p &lt; \alpha$ reject the null hypothesis, else accept it</li>
</ol>
<p><a href="https://github.com/mtrencseni/playground/blob/master/AB%20testing%20and%20the%20pitfall%20of%20early%20stopping.ipynb">The code shown below is up on Github.</a></p>
<h2>Early stopping</h2>
<p>What happens if the tester is curious or impatient and follows a different protocol and peeks at the data repeatedly to see if it’s “already significant”. So instead of steps 5-6 above, they:</p>
<ul>
<li>collect $N_1$ samples, run hypothesis test on $N’ := N_1$ samples, compute $p$ value, if $p &lt; \alpha$ reject the null hypothesis and stop, else go on</li>
<li>collect $N_2$ more samples, run hypothesis test on $N’ := N + N_2$ samples, compute $p$ value, if $p &lt; \alpha$ reject the null hypothesis and stop, else go on</li>
<li>collect $N_3$ more samples...</li>
<li>stop if $N’ &gt;= N$</li>
</ul>
<p>We can simulate this <strong>early stopping protocol</strong> with Monte Carlo code:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">abtest_episode</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">prior_observations</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="n">simulate_abtest</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prior_observations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">observations</span> <span class="o">+=</span> <span class="n">prior_observations</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="bp">False</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">observations</span><span class="p">,</span> <span class="n">p</span>

<span class="k">def</span> <span class="nf">early_stopping_simulation</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">num_simulations</span><span class="p">,</span> <span class="n">episodes</span><span class="p">,</span> <span class="n">alphas</span><span class="p">):</span>
    <span class="n">hits</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_simulations</span><span class="p">):</span>
        <span class="n">observations</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">episodes</span><span class="p">)):</span>
            <span class="n">observations</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">abtest_episode</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">episodes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">observations</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;=</span> <span class="n">alphas</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">hits</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">break</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">hits</span> <span class="o">/</span> <span class="n">num_simulations</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>


<p>Let’s assume our A/B test is actually not working (no lift), so both A and B are the same:</p>
<div class="highlight"><pre><span></span><span class="n">funnels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.50</span><span class="p">],</span> <span class="mf">0.5</span><span class="p">],</span> <span class="c1"># the first vector is the actual outcomes,</span>
    <span class="p">[[</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.50</span><span class="p">],</span> <span class="mf">0.5</span><span class="p">],</span> <span class="c1"># the second is the traffic split</span>
<span class="p">]</span>
</pre></div>


<p>First, let’s check that we get what we expect in the simple case, without early stopping:</p>
<div class="highlight"><pre><span></span><span class="n">Ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3000</span><span class="p">]</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">]</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">early_stopping_simulation</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Ns</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;False positive ratio: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
</pre></div>


<p>Prints something like:</p>
<div class="highlight"><pre><span></span><span class="bp">False</span> <span class="n">positive</span> <span class="n">ratio</span><span class="p">:</span> <span class="mf">0.057</span>
</pre></div>


<p>This is what we expect. If the null hypothesis is true (A and B are the same), we expect to get $\alpha$ false positives, that’s exactly what $\alpha$ controls. Let’s see what happens if we collect the same amount of total samples, but follow the early stopping protocol with 2 extra peeks:</p>
<div class="highlight"><pre><span></span><span class="n">Ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1000</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">early_stopping_simulation</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Ns</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;False positive ratio: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
</pre></div>


<p>Prints something like:</p>
<div class="highlight"><pre><span></span><span class="bp">False</span> <span class="n">positive</span> <span class="n">ratio</span><span class="p">:</span> <span class="mf">0.105</span>
</pre></div>


<p>This is the problem with early stopping! <strong>If we repeatedly perform the significance test at the same $\alpha$ level, the overall $\alpha$ level will be higher.</strong> If we do this, we will on average have a higher false positive rate than we think. In the above simulation, with 2 extra peeks, at equal $N$ intervals, the FPR roughly doubles!</p>
<h2>Intuition</h2>
<p>Why does the FPR go up in the case of early stopping? The best way to see this is to evaluate the $p$ value a lot of times, and plot the results. The simulation code is straightforward:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">repeated_significances</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">episodes</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">episodes</span><span class="p">)):</span>
        <span class="n">observations</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">abtest_episode</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">episodes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">observations</span><span class="p">)</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>


<p>Let’s evaluate at every 100 samples, 100 times (total $N=10,000$), and run it 3 times:</p>
<div class="highlight"><pre><span></span><span class="n">Ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>

<span class="n">results1</span> <span class="o">=</span> <span class="n">repeated_significances</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">Ns</span><span class="p">)</span>
<span class="n">results2</span> <span class="o">=</span> <span class="n">repeated_significances</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">Ns</span><span class="p">)</span>
<span class="n">results3</span> <span class="o">=</span> <span class="n">repeated_significances</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">Ns</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sample size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results1</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results2</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results2</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results3</span><span class="p">],</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.05</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">results3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Test 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Test 2&#39;</span><span class="p">,</span> <span class="s1">&#39;Test 3&#39;</span><span class="p">,</span> <span class="s1">&#39;p = 0.05&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>The result is something like:</p>
<p><img src="/images/early_stopping.png" alt="Early stopping" style="width: 800px;"/></p>
<p>There is no “correct” $p$ value to compute at any point, since we are collecting samples from random process. The guarantee of frequentist hypothesis testing (as discussed in the past posts), is that, if we evaluate the data at the end (at $N=10,000$ on the chart, at the end), if the null hypothesis is true, then on average  in $1-\alpha$ fraction of cases the p value will be bigger than $\alpha$, and we will make the correct decision to accept the null hypothesis (the correct decision). But there is no guarantee about the trajectory of the p value in between. The trajectory is by definition random, so if we repeatedly test against the $p=0.05$ line with an early stopping protocol, then we will reject the null hypothesis (the incorrect decision) more often. In the case above, for the green line, we would have done so at the beinning, and for the orange line, we could have done so several times; even though at the end, as it happens, these tests would all (correctly) accept the null hypothesis.</p>
<p>A mathy way of saying this is to realize that $P(p_{N} &lt; \alpha | H_0)$ &lt; $P(p_{N1} &lt; \alpha | H_0) + P(p_{N1+N2} &lt; \alpha | H_0 \wedge p_{N1} &gt; \alpha)) + ...$</p>
<h2>Alpha spending in sequential trials</h2>
<p>In itself, an early stopping protocol is not a problem. In the above example, we saw that taking 2 extra peeks at equal $N$ intervals with early stopping at $\alpha=0.05$ each yields an overall $\alpha$ of ~0.10. As long as we know that the overall $\alpha$ of our protocol is what it is, we’re fine. The problem is if we’re not aware of this, and we believe we’re actually operating at a lower $\alpha$, and potentially report a lower $\alpha$ along with the results.</p>
<p>What if we are mindful of the increase in $\alpha$ that early stopping induces, but we want to keep the overall (=real) $\alpha$ at a certain level, let’s say $\alpha=0.05$. Based on the previous simulation, intuitively, this is possible, we just have to test at lower $\alpha$ at each early stopping opportunity. This is called <a href="https://en.wikipedia.org/wiki/Sequential_analysis">alpha spending</a>, because it’s like we have an overall budget of $\alpha$, and we’re spending it in steps. <strong>Note that alpha spending is not additive!</strong> Let’s look at two protocols to achieve overall $\alpha=0.05$.</p>
<p>First, the <a href="https://en.wikipedia.org/wiki/Pocock_boundary">Pocock boundary, from Wikipedia</a>:</p>
<blockquote>
<p>The Pocock boundary gives a p-value threshold for each interim analysis which guides the data monitoring committee on whether to stop the trial. The boundary used depends on the number of interim analyses.</p>
</blockquote>
<p><img src="/images/pocock.png" alt="Pocock table" style="width: 450px;"/></p>
<blockquote>
<p>The Pocock boundary is simple to use in that the p-value threshold is the same at each interim analysis. The disadvantages are that the number of interim analyses must be fixed at the start and it is not possible under this scheme to add analyses after the trial has started. Another disadvantage is that investigators and readers frequently do not understand how the p-values are reported: for example, if there are five interim analyses planned, but the trial is stopped after the third interim analysis because the p-value was 0.01, then the overall p-value for the trial is still reported as &lt;0.05 and not as 0.01.</p>
</blockquote>
<p>So, in the simulated case earlier, if we use $\alpha=0.0221$ at each step, we will achieve an overall $\alpha=0.05$:</p>
<div class="highlight"><pre><span></span><span class="n">Ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1000</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0221</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">early_stopping_simulation</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Ns</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;False positive ratio: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
</pre></div>


<p>Prints something like:</p>
<div class="highlight"><pre><span></span><span class="bp">False</span> <span class="n">positive</span> <span class="n">ratio</span><span class="p">:</span> <span class="mf">0.052</span>
</pre></div>


<p>It works!</p>
<p>The <a href="https://en.wikipedia.org/wiki/Haybittle%E2%80%93Peto_boundary">Haybittle–Peto boundary</a> is much simpler, but it’s not an exact rule. It essentially says, perform the in-between tests at a very low $\alpha=0.001$, and the final test at the desired $\alpha=0.05$. Because the the early steps were performed at such low $\alpha$, it doesn’t change the overall $\alpha$ by much.</p>
<p><img src="/images/haybittle-peto.png" alt="Haybittle-Peto table" style="width: 400px;"/></p>
<p>What this is essentially saying is: if you peek early and the null hypothesis is without a reasonable doubt wrong, ie. the treatment is without a reasonable doubt better than the control group already at lower $N$s, then stop, else keep going.</p>
<div class="highlight"><pre><span></span><span class="n">Ns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1000</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">early_stopping_simulation</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">Ns</span><span class="p">,</span> <span class="n">alphas</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;False positive ratio: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">p</span><span class="p">)</span>
</pre></div>


<p>Prints something like:</p>
<div class="highlight"><pre><span></span><span class="bp">False</span> <span class="n">positive</span> <span class="n">ratio</span><span class="p">:</span> <span class="mf">0.049</span>
</pre></div>


<p>Obviously, there are countless protocols like this we can construct.</p>
<p>A final option is to not do this, never to stop the experiment until the original sample size of $N$ is reached. This is what I recommend to do. Note that peeking itself is not a problem, as long as we don’t stop the experiment. Some A/B testing tools go the extra mile and don’t show (blur out) the result before the agreed-upon sample size is reached.</p>
<p>What is the actual formula for the final $\alpha$, if we follow a protocol of testing at $\alpha_i$ at $N_i$ with early stopping? I’m not going to derive it here, we can just run the simulation above and see what it is. In the real-world, we don’t follow exact protocols like this anyway (peek at exact $N$ steps). In my opinion, it’s enough to be aware of this issue, and control our behaviour. I recommend to not early stop, or follow the conservative Haybittle–Peto boundary.</p>
<h2>What’s the big deal?</h2>
<p>Why do we even use A/B testing? Why don't we just eyeball the difference? After all, in a typical SaaS A/B testing setting, lives are not at stake, unlike in <a href="https://en.wikipedia.org/wiki/Clinical_trial">clinical trials</a>.</p>
<p>We use formal A/B testing so we don’t fool ourselves. Usually the person or team performing the test has some attachment to the test, because it’s their idea, their code, they invested time into running the experiment, their bonus depends on their impact. We’re biased. By agreeing with ourselves or the team that we will follow a certain protocol to evaluate the experiment (sample size $N$, metric, $\alpha$), we’re eliminating or at least controlling our own bias.</p>
<p>But, we have to be mindful that A/B testing shouldn’t hold us back. In the post <a href="http://bytepawn.com/ab-tests-moving-fast-vs-being-sure.html#ab-tests-moving-fast-vs-being-sure">A/B tests: Moving Fast vs Being Sure</a> I’ve argued that in startup settings it may make sense to run at higher $\alpha$ (and collect less samples per experiment), accept more false positives, because it will allow us to perform more experiments per year, which ultimately may be worth it.</p>
<p>Another thing to keep in mind that early stopping affects $\alpha$, the false positive rate. But false positives only matter for experiments that are not working, ie. when there is no lift from A to B. In the extreme case of all our experiments working, early stopping is actually good, because we will spend less time deciding, and the "error" is in the right direction. But this is a perverse case, if all our experiments always work out, then there’s no need to A/B test anyway.</p>
<p>We can do a back-of-the-envelope calculation to see what happens if we follow an early stopping protocol and (accidentally, or knowingly) run at $\alpha=0.10$ insteaf of $\alpha=0.05$. Let’s suppose that 1 in 4 ($\gamma=0.25$) experiment actually works, and we’re running at 0.8 power, ie. when an experiment works, we catch it 80% of the time. If we run 100 experiments a year:</p>
<ul>
<li>25 will work, and we will catch 20 of these, 5 we will mis-classify as not working</li>
<li>75 will not work, if we run at:<ul>
<li>$\alpha=0.05$ we will mis-classify 3.75 as working</li>
<li>$\alpha=0.10$ we will mis-classify 7.5 as working</li>
</ul>
</li>
</ul>
<p>The mis-classification rate due to a $\Delta \alpha$ is $(1 - \gamma) \Delta \alpha$.</p>
<h2>Conclusion</h2>
<p>Increased false positive rate due to early stopping is beautiful nuance of statistical testing. It is equivalent to running at an overall higher $\alpha$. Data scientists need to be aware of this phenomenon so they can control it and keep their organizations honest about their experimental results.</p>
<h2>Links</h2>
<ul>
<li><a href="https://www.evanmiller.org/how-not-to-run-an-ab-test.html">How Not To Run an A/B Test</a></li>
<li><a href="https://www.evanmiller.org/sequential-ab-testing.html">Simple Sequential A/B Testing</a></li>
<li><a href="https://codeascraft.com/2018/10/03/how-etsy-handles-peeking-in-a-b-testing/">How Etsy Handles Peeking in A/B Testing</a></li>
</ul>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/ab-testing.html">ab-testing</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Early stopping in A/B testing",
  "headline": "Early stopping in A/B testing",
  "datePublished": "2020-03-05 00:00:00+01:00",
  "dateModified": "2020-03-05 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/early-testing-in-ab-testing.html",
  "description": "Increased false positive rate due to early stopping is beautiful nuance of statistical testing. It is equivalent to running at an overall higher alpha. Data scientists need to be aware of this phenomenon so they can control it and keep their organizations honest about their experimental results."
}
</script></body>
</html>