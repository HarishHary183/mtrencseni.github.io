<!DOCTYPE html>
<html lang="en">
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="I use simulated self-play by ranking episodes by summed reward. Game outcomes are divided in two by cutting at the median, winners are assigned +1 rewards and losers -1, and the standard policy gradient loss function is computed from this. Unlike naive policy gradient descent, this version solves all OpenAI classic control problems." />
<meta name="keywords" content="python, pytorch, reinforcement, learning, openai, gym">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch"/>
<meta property="og:description" content="I use simulated self-play by ranking episodes by summed reward. Game outcomes are divided in two by cutting at the median, winners are assigned +1 rewards and losers -1, and the standard policy gradient loss function is computed from this. Unlike naive policy gradient descent, this version solves all OpenAI classic control problems."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-11-14 00:00:00+01:00"/>
<meta property="article:modified_time" content="2019-11-14 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="pytorch"/>
<meta property="article:tag" content="reinforcement"/>
<meta property="article:tag" content="learning"/>
<meta property="article:tag" content="openai"/>
<meta property="article:tag" content="gym"/>
<meta property="og:image" content="/images/mountaincar.png"/>

  <title>Bytepawn &ndash; Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch">Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch</h1>
    <p>Posted on Thu 14 November 2019 in <a href="/category/machine-learning.html">Machine Learning</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>In <a href="http://bytepawn.com/solving-the-cartpole-reinforcement-learning-problem-with-pytorch.html">a previous blog post</a>, I applied plain vanilla Reinforcement Learning policy gradient to solve the CartPole OpenAI gym classic control problem. In <a href="http://bytepawn.com/solving-openai-gym-classic-control-problems-with-pytorch.html#solving-openai-gym-classic-control-problems-with-pytorch">the subsequent blog post</a>, I generalized that code (in a software engineering sense) and applied it to all classic control problems; the only "trick" was to quantize the applied action for the continuous problems to convert them to discrete problems. It was able to solve CartPole and Acrobot, but failed on Pendulum and MountainCar (the failure was unrelated to the discretization). I described the problem that I saw examining the numerics at the end of the post: </p>
<p>.. the loss function is structured like <code>loss := -1 * sum(reward x log(probability of action taken))</code>, where the <code>log(probability of action taken)</code> is negative, so the overall expression is positive, assuming the reward is positive. In this case, making the loss 0 would be a global minimum. This can happen if the optimizer sets the probabilities of an arbitrary sub-optimal policy to one, hence making the <code>log(probability)</code> zero, making the entire loss function go to zero, even though the solution is actually "random".</p>
<h2>Simulated self-play</h2>
<p>It occured to me that this sort of problem wouldn’t occur when using RL for Chess or Go, since in that case, the agent plays itself, and the winner would get a +1 reward, the loser a -1 reward. This means that gradient descent can’t converge on a bad solution by just adjusting the weights so that the probability of an arbitrary action becomes 1, because this gets supressed by the -1 reward parts in the loss function.</p>
<p>It then occured to me that it’s easy to simulate this setup with any game: simply pair the games, and the one with the higher reward (as set by the original rules of the game) is the winner, the other the loser. Overwrite the original reward, and simply use +1 for all actions taken by the winner, and -1 for all actions taken by the loser. In actual implementation, there is no need to actually pair: it’s easier to just play N games, sort the games by overall reward, and treat the lower half as losers and give them -1 rewards, and the upper half as winners and give them +1 rewards. N should be chosen to control variance, I used N=100 here.</p>
<p>The only "hack" necessary to make this work is on the MountainCar-v0 problem: the way the default reward returned by OpenAI Gym is structured, initially it returns a constant -200 (-1 for each timestep the agent doesn't reach the top of the hill, with 200 timesteps per episode). So with this default reward structure, this approach would fail, because the initial totally random agent has no way of knowing which way to proceed, as all runs return a constant -200 <code>(min=median=avg=max=-200)</code>. I defined a custom reward function, which is simply the maximum x-coordinate that the agent gets up the hill, and set the solved reward threshold at 0.5, which is the position of the flag at the top of the hill.</p>
<p><img src="/images/mountaincar.png" alt="OpenAI Gym mountaincar" style="width: 600px;"/></p>
<h2>Coding it up</h2>
<p>sdfsdf</p>
<h2>Results and discussion</h2>
<p>Custom reward function for MC
LR=0.001 for MC</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/python.html">python</a>
      <a href="/tag/pytorch.html">pytorch</a>
      <a href="/tag/reinforcement.html">reinforcement</a>
      <a href="/tag/learning.html">learning</a>
      <a href="/tag/openai.html">openai</a>
      <a href="/tag/gym.html">gym</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch",
  "headline": "Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch",
  "datePublished": "2019-11-14 00:00:00+01:00",
  "dateModified": "2019-11-14 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch.html",
  "description": "I use simulated self-play by ranking episodes by summed reward. Game outcomes are divided in two by cutting at the median, winners are assigned +1 rewards and losers -1, and the standard policy gradient loss function is computed from this. Unlike naive policy gradient descent, this version solves all OpenAI classic control problems."
}
</script></body>
</html>