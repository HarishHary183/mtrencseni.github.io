<!DOCTYPE html>
<html lang="en">
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="I use simulated self-play by ranking episodes by summed reward. Game outcomes are divided in two by cutting at the median, winners are assigned +1 rewards, losers are assigned -1 rewards, like in games like Go and Chess. Unlike naive policy gradient descent used in previous posts, this version solves all OpenAI classic control problems, albeit slowly." />
<meta name="keywords" content="python, pytorch, reinforcement, learning, openai, gym">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch"/>
<meta property="og:description" content="I use simulated self-play by ranking episodes by summed reward. Game outcomes are divided in two by cutting at the median, winners are assigned +1 rewards, losers are assigned -1 rewards, like in games like Go and Chess. Unlike naive policy gradient descent used in previous posts, this version solves all OpenAI classic control problems, albeit slowly."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-11-14 00:00:00+01:00"/>
<meta property="article:modified_time" content="2019-11-14 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="pytorch"/>
<meta property="article:tag" content="reinforcement"/>
<meta property="article:tag" content="learning"/>
<meta property="article:tag" content="openai"/>
<meta property="article:tag" content="gym"/>
<meta property="og:image" content="/images/mountaincar.png"/>

  <title>Bytepawn &ndash; Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch">Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch</h1>
    <p>Posted on Thu 14 November 2019 in <a href="/category/machine-learning.html">Machine Learning</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>In <a href="http://bytepawn.com/solving-the-cartpole-reinforcement-learning-problem-with-pytorch.html">a previous blog post</a>, I applied plain vanilla Reinforcement Learning policy gradient to solve the CartPole OpenAI gym classic control problem. In <a href="http://bytepawn.com/solving-openai-gym-classic-control-problems-with-pytorch.html#solving-openai-gym-classic-control-problems-with-pytorch">the subsequent blog post</a>, I generalized that code (in a software engineering sense) and applied it to all classic control problems; the only "trick" was to quantize the applied action for the continuous problems to convert them to discrete problems. It was able to solve CartPole and Acrobot, but failed on Pendulum and MountainCar (the failure was unrelated to the discretization). I described the problem that I saw examining the numerics at the end of the post: </p>
<p><em>.. the loss function is structured like <code>loss := -1 * sum(reward x log(probability of action taken))</code>, where the <code>log(probability of action taken)</code> is negative, so the overall expression is positive, assuming the reward is positive. In this case, making the loss 0 would be a global minimum. This can happen if the optimizer sets the probabilities of an arbitrary sub-optimal policy to one, hence making the <code>log(probability)</code> zero, making the entire loss function go to zero, even though the solution is actually "random".</em></p>
<h2>Simulated self-play</h2>
<p>It occured to me that this sort of problem wouldn’t occur when using RL for Chess or Go, since in that case, the agent plays itself, and the winner would get a +1 reward, the loser a -1 reward. This means that gradient descent can’t converge on a bad solution by just adjusting the weights so that the probability of an arbitrary action becomes 1, because this gets supressed by the -1 reward parts in the loss function.</p>
<p>It then occured to me that it’s easy to simulate this setup with any game: let the agent play games, and once finished, pair the games, and the one with the higher episode reward (as set by the original rules of the game) is the winner, the other the loser. Overwrite the original reward, and simply use +1 for all actions taken by the winner, and -1 for all actions taken by the loser. In actual implementation, there is no need to actually pair: it’s easier to just play N games, sort the games by overall reward, and treat the lower half as losers and give them -1 rewards, and the upper half as winners and give them +1 rewards. N should be chosen to control variance, I used N=100 here.</p>
<p>The only "hack" necessary to make this work is on the MountainCar-v0 problem: the way the default reward returned by OpenAI Gym is structured, initially it returns a constant -200 (-1 for each timestep the agent doesn't reach the top of the hill, with 200 timesteps per episode). So with this default reward structure, this approach would fail, because the initial totally random agent has no way of knowing which way to proceed, as all runs return a constant -200 <code>(min=median=avg=max=-200)</code>. To make this approach work, I defined a custom reward function, which is simply the maximum x-coordinate that the agent reaches, and set the solved reward threshold at 0.5, which is the position of the flag at the top of the hill.</p>
<p><img src="/images/mountaincar.png" alt="OpenAI Gym mountaincar" style="width: 400px;"/></p>
<h2>Coding it up</h2>
<p>The code is very similar to the previous solutions. <a href="https://github.com/mtrencseni/pytorch-playground/blob/master/11-gym-self-play/OpenAI%20Gym%20classic%20control.ipynb">The ipython notebook is up on Github.</a> The main change is in the main training loop, which collects episode results, splits them into winners and losers, and computes the custom loss function based on that:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_solve_self_play</span><span class="p">(</span><span class="n">env_desc</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eps_non_greedy</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Doing </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_desc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span> <span class="o">=</span> <span class="n">in_out_length</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">output_quantization_levels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">output_length</span> <span class="o">*=</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">output_quantization_levels</span>
    <span class="k">if</span> <span class="n">model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">model_class</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">!=</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">learning_rate</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">games</span> <span class="o">=</span> <span class="n">play_games</span><span class="p">(</span><span class="n">env_desc</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">,</span> <span class="n">eps_non_greedy</span><span class="p">)</span>
        <span class="n">games</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># sorts by first key, the reward</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">games</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">num_episodes</span><span class="o">/</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">wins</span> <span class="o">=</span> <span class="n">games</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">num_episodes</span><span class="o">/</span><span class="mi">2</span><span class="p">):]</span>
        <span class="n">sum_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">game</span> <span class="ow">in</span> <span class="n">losses</span><span class="p">:</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">game</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">sum_loss</span> <span class="o">+=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span> <span class="c1"># losers get -1 reward for all actions taken</span>
        <span class="k">for</span> <span class="n">game</span> <span class="ow">in</span> <span class="n">wins</span><span class="p">:</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">game</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">sum_loss</span> <span class="o">+=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="p">[</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">probs</span><span class="p">))</span> <span class="c1"># winners get +1 reward for all actions taken</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">sum_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">evaluation_rewards</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">play_one_game</span><span class="p">(</span><span class="n">env_desc</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">eps_non_greedy</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">policy_func</span><span class="o">=</span><span class="n">select_action_from_policy_best</span><span class="p">)</span>
            <span class="n">evaluation_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="n">mean_reward</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">evaluation_rewards</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%d</span><span class="s1">: min=</span><span class="si">%.2f</span><span class="s1"> median=</span><span class="si">%.2f</span><span class="s1"> max=</span><span class="si">%.2f</span><span class="s1"> eval=</span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">games</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">games</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">num_episodes</span><span class="o">/</span><span class="mi">2</span><span class="p">)][</span><span class="mi">0</span><span class="p">],</span> <span class="n">games</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">mean_reward</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">mean_reward</span> <span class="o">&gt;=</span> <span class="n">reward_threshold</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_desc</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Solved!&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">model</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Failed!&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>


<h2>Results and discussion</h2>
<p>This approach solves all the classic control problems from OpenAI Gym. I find it intellectually rewarding to adjust a simple, generic solution so it works across problems. Caveats of this approach are:</p>
<ul>
<li>As mentioned above, I had to adjust the reward structure for MountainCar so the training loop is able to tell apart better from worse runs (winners/losers).</li>
<li>This approach still assumes a discrete action space, which means continuous problems like Pendulum had to be discretized. This would not work or would be very inefficient for higher dimensional action spaces.</li>
<li>Although this approach works, it's quite slow: in each epoch, 100 games are played; for the simple games like CartPole about 100 epochs are enough, but for MountainCar about 2,000 epochs (so a total of 2,000,000 games played) are needed; other RL methods can find solutions to these simple problems using orders of magnitude less playing time.</li>
<li>I played around with off-policy learning; this would mean that in epsilon percent of cases, the agent does not pick the action based on the current policy distribution, but totally randomly (ie. uniform); this introduces more random exploration (and noise) into the training process; in the end, this was not needed here.</li>
<li>As before, I used the <a href="https://pytorch.org/docs/stable/_modules/torch/optim/adam.html">Adam optimizer</a> for all problems; I had to adjust learning rate to 0.001 for MountainCar, the default 0.01 worked for the other environments.</li>
<li>This approach is still naive because it evenly splits each epoch's N games on the median into two equal sets of winners and losers. As the agent approaches a good solution, this strategy becomes less effective, because some of the runs classified as a loss would still be "pretty good", however this way those decisions are penalized. Similar to adjusting the learning rate, it would be more sophisticated to adjust the ratio of winners and losers as the solution approaches the reward theshold.</li>
</ul>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/python.html">python</a>
      <a href="/tag/pytorch.html">pytorch</a>
      <a href="/tag/reinforcement.html">reinforcement</a>
      <a href="/tag/learning.html">learning</a>
      <a href="/tag/openai.html">openai</a>
      <a href="/tag/gym.html">gym</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch",
  "headline": "Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch",
  "datePublished": "2019-11-14 00:00:00+01:00",
  "dateModified": "2019-11-14 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch.html",
  "description": "I use simulated self-play by ranking episodes by summed reward. Game outcomes are divided in two by cutting at the median, winners are assigned +1 rewards, losers are assigned -1 rewards, like in games like Go and Chess. Unlike naive policy gradient descent used in previous posts, this version solves all OpenAI classic control problems, albeit slowly."
}
</script></body>
</html>