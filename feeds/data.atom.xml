<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bytepawn - Data</title><link href="/" rel="alternate"></link><link href="/feeds/data.atom.xml" rel="self"></link><id>/</id><updated>2020-02-23T00:00:00+01:00</updated><entry><title>A/B testing and the t-test</title><link href="/ab-testing-and-the-ttest.html" rel="alternate"></link><published>2020-02-23T00:00:00+01:00</published><updated>2020-02-23T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2020-02-23:/ab-testing-and-the-ttest.html</id><summary type="html">&lt;p&gt;The t-test is a better version of z-tests for timespent data, because it explicitly models the uncertainty of the variance due to sampling.&lt;br/&gt;&lt;br/&gt; &lt;img src="/images/t-test-5-10.png" alt="normal distribution vs t-distribution" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the last post, I showed how to do &lt;a href="http://bytepawn.com/ab-testing-and-the-ztest.html"&gt;A/B testing with the z-test&lt;/a&gt;. I used two examples:
- conversions, ie. proportions ($X_A$ out of $N_A$ converted)
- timespents (timespents for A were $x_i, x_2 ... x_N$)&lt;/p&gt;
&lt;p&gt;In this post, let’s concentrate on timespent data. The t-test is a better version of z-tests for timespent data, because it explicitly models the uncertainty of the variance due to sampling. The &lt;a href="https://en.wikipedia.org/wiki/Student%27s_t-test"&gt;Wikipedia page for Student’s t-test&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. A t-test is most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known. When the scaling term is unknown and is replaced by an estimate based on the data, the test statistics (under certain conditions) follow a Student's t distribution. The t-test can be used, for example, to determine if the means of two sets of data are significantly different from each other.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/mtrencseni/playground/blob/master/AB%20testing%20and%20the%20ttest.ipynb"&gt;The code shown below is up on Github.&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;The t-test vs the z-test&lt;/h2&gt;
&lt;p&gt;What does this mean? Before I talked about the z-test, &lt;a href="http://bytepawn.com/ab-testing-and-the-central-limit-theorem.html"&gt;I wrote about the Central Limit Theorem (CLT)&lt;/a&gt;. The CLT essentially says that as we collect more independent samples from a population, we can estimate the true mean of the population by averaging our samples. The distribution of our estimate will be a normal distribution around the true mean, with variance $ \sigma_2 = \sigma_p^2 / N $, where $\sigma_p$ is the true standard deviation of the population. The population mean and standard deviation should exist, but the population doesn’t have to be normally distributed, eg. it can be exponential.&lt;/p&gt;
&lt;p&gt;When we perform a z-test, we model the distribution as a normal variable, with mean $ \mu = \frac{1}{N} \sum{ x_i } $ and variance $ \sigma^2 = s^2/N $, where $ s^2 = \frac{1}{N} \sum{(\mu - x_i)^2} $. The problem is, we cheated a little: we used $s^2$ and not $\sigma_p^2$! We do this because we don’t know $\sigma_p^2$, all we have is the estimate $s^2$.&lt;/p&gt;
&lt;p&gt;The t-test models this uncertainty in the estimation of the $ \sigma^2 $. When we perform a t-test, it feels very similar to the z-test, except in some places we write $N-1$ instead of $N$. And in the end, we don’t look up a $z$ value on a normal distribution, instead we look up a $t$ value on a t-distribution (which actually looks similar to the normal, it’s also bell shaped).&lt;/p&gt;
&lt;p&gt;The most important thing to know is that, as $ N \rightarrow \infty $, the t-distribution becomes a normal distribution, and the final outcome of hypothesis testing, the p-value becomes identical for a t-test and a z-test. The difference effectively disappears between around $N=30-50$ sample size. So if you’re performing a timespent A/B test, and you have 100s or more samples in each bucket, the t-test and the z-test will yield numerically identical results.
This is becauce at such sample sizes, the estimate of $s^2$ for $\sigma_p^2$ becomes really good for estimating the mean (it’s divided by $N$, so the importance goes down with increasing $N$).&lt;/p&gt;</content><category term="ab-testing"></category></entry><entry><title>A/B testing and the Z-test</title><link href="/ab-testing-and-the-ztest.html" rel="alternate"></link><published>2020-02-15T00:00:00+01:00</published><updated>2020-02-15T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2020-02-15:/ab-testing-and-the-ztest.html</id><summary type="html">&lt;p&gt;I discuss the Z-test for A/B testing and show how to compute parameters such as sample size from first principles. I use Monte Carlo simulations to validate significance level and statistical power, and visualize parameter scaling behaviour.&lt;br/&gt;&lt;br/&gt; &lt;img src="/images/conversion_diff.png" alt="Conversion difference vs N" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the previous two posts, we talked about the &lt;a href="http://bytepawn.com/ab-testing-and-the-central-limit-theorem.html"&gt;A/B testing and the Central Limit Theorem&lt;/a&gt; and discussed when the CLT doesn’t hold in &lt;a href="http://bytepawn.com/beyond-the-central-limit-theorem.html"&gt;Beyond the Central Limit Theorem&lt;/a&gt; (CLT). The next step in exploring A/B testing is to look at the Z-test, which is the most common and straightforward staistical test.&lt;/p&gt;
&lt;p&gt;With our understanding of the CLT, the Z-test is simple to explain. We’re running a Z-test if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;our null hypothesis is the relationship between population means or other test statistics, and&lt;/li&gt;
&lt;li&gt;we can assume that the CLT holds and the test statistics follow a normal distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The same from &lt;a href="https://en.wikipedia.org/wiki/Z-test"&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/mtrencseni/playground/blob/master/Conversion%20AB%20test%20with%20the%20Z-test.ipynb"&gt;The code shown below is up on Github.&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Statistical hypothesis testing&lt;/h2&gt;
&lt;p&gt;In a conversion A/B test setting, &lt;a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing"&gt;statistical hypothesis testing&lt;/a&gt; is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we have a base version A and contender version B, and we’re trying to decide whether B is better than A&lt;/li&gt;
&lt;li&gt;if B is converting worse than A, then we’re done&lt;/li&gt;
&lt;li&gt;if B is converting better than A, we’d like to know how &lt;strong&gt;significant&lt;/strong&gt; our results are; in hypothesis testing, we compute the probability that we’d get this result if B is actually not better than A; ie. we compute the probability of getting the result that B is better than A due to random chance, even if B is not better than A; this probability is called the &lt;strong&gt;p-value&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get a better feeling for the point above, imagine if somebody gives you a coin. They claim it’s a fair coin, meaning you get Heads and Tails half the time. You want to test this claim, ie. the null hypothesis of a fair coin. If you flip it 10 times, and you get 6 Hs and 4 Ts, how confident are you that it’s not a fair coin? You can’t be too sure, because you haven’t collected enough samples, because the 6:4 result is a very likely outcome even if the coin is fair. The 6:4 result is not significant enough to disprove the null hypothesis of a fair coin. But if you flip it 1000 times, and you get 599 Hs and 401 Ts, that’s quite suspicious. Getting 599:401 from a fair coin is unlikely (it can be calculated explicitly, see below).&lt;/p&gt;
&lt;h2&gt;Types of Z-tests&lt;/h2&gt;
&lt;p&gt;Some points to make our thinking about the Z-test clear.&lt;/p&gt;
&lt;p&gt;It’s called Z-test because when running the numbers, it’s common to transform the data to a standard normal distribution $N(0, 1)$. In the old days, before computers, the p-value, ie. the portion of the normal distribution outside the normalized test statistic (eg. the difference of the means) would be read off a printout table (eg. the back of statistics textbooks), and this statistic is conventionally denoted with the letter z. A more verbose, but descriptive name would be &lt;strong&gt;test-for-normally-distributed-test-statistics&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Z-test is not one specific test, it’s a kind of test. Any time we work with an approximately normally distributed test statistic, we’re performing a Z-test. The practical bible of statistical testing, &lt;a href="https://www.amazon.com/Statistical-Tests-Third-Gopal-Kanji/dp/141292376X"&gt;100 Statistical Tests&lt;/a&gt; by Gopal Kanji, lists the following types of Z-tests:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Test 1: Z-test for a population mean (variance known)&lt;/li&gt;
&lt;li&gt;Test 2: Z-test for two population means (variances known and equal)&lt;/li&gt;
&lt;li&gt;Test 3: Z-test for two population means (variances known and unequal)&lt;/li&gt;
&lt;li&gt;Test 4: Z-test for a proportion (binomial distribution)&lt;/li&gt;
&lt;li&gt;Test 5: Z-test for the equality of two proportions (binomial distribution)&lt;/li&gt;
&lt;li&gt;Test 6: Z-test for comparing two counts (Poisson distribution)&lt;/li&gt;
&lt;li&gt;Test 13: Z-test of a correlation coefficient&lt;/li&gt;
&lt;li&gt;Test 14: Z-test for two correlation coefficients&lt;/li&gt;
&lt;li&gt;Test 23: Z-test for correlated proportions&lt;/li&gt;
&lt;li&gt;Test 83: Z-test for the uncertainty of events&lt;/li&gt;
&lt;li&gt;Test 84: Z-test for comparing sequential contingencies across two groups using the ‘log odds ratio’&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/100_statistical_tests.jpg" alt="100 statistical tests" style="width: 300px;"/&gt;&lt;/p&gt;
&lt;p&gt;I listed out these to further the point that the Z-test is not just one test, it’s a type of test that makes sense in a variety of scenarios.&lt;/p&gt;
&lt;h2&gt;Formulas&lt;/h2&gt;
&lt;p&gt;The math is similar to the discussion in the CLT post. We’re sampling a distribution and computing a test statistic, and assuming that it follows a normal distribution $ N(\mu, \sigma^2) $. In an A/B test, we have two normal distributions, $ N(\mu_A, \sigma_A^2) $ and $ N(\mu_B, \sigma_B^2) $ with samples sizes $N_A$ and $N_B$, and the test statistic is $ N(\mu_A, \sigma_A^2) - N(\mu_B, \sigma_B^2) = N(\mu = \mu_B - \mu_A, \sigma^2 = \sigma_A^2 + \sigma_B^2) $ by the &lt;a href="https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables"&gt;addition rule for normal distributions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The test statistic then is $ Z = \frac{ \mu }{ \sigma } $, we use this to get the p-value for the experiment. This is simply the normalized distance from the mean of the distribution. With this normalized distance, we can use a table for a standard normal distribution table and read off the p-value. In the age of computers, we actually don’t have to do this final normalization step to get Z, we can just get the p-value from the original $ N(\mu, \sigma^2) $ distribution.&lt;/p&gt;
&lt;p&gt;In an A/B test setting, $ \mu_A $ and $ \mu_B $ are known. The trick is, what are the standard deviations $ \sigma_A^2 $ and $ \sigma_B^2 $? We compute it from the sample standard devation $ s^2 $, like $ \sigma^2 = s^2/N $. The sample standard deviation is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for conversion, the population distribution is &lt;a href="https://en.wikipedia.org/wiki/Bernoulli_distribution"&gt;Bernoulli&lt;/a&gt;, so $ s^2 = \mu(1-\mu) $&lt;/li&gt;
&lt;li&gt;for timespent, you can compute the standard error from the data directly $ s^2 = \frac{1}{N} \sum{(\mu - x_i)^2} $, where $x_i$ are the invididual timespents per user, and $ \mu = \frac{1}{N} \sum{ x_i } $.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Sample size&lt;/h2&gt;
&lt;p&gt;Before running an A/B test, we have to decide 2 things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ \alpha $, the False Positive Rate (FPR): if B is actually not better than A, by chance the measurement can still show B to be better. We can reduce this by collecting more samples. But we need to set an FPR that we are okay with with. Usually people set this to 0.05, but as I discuss this in the previous post &lt;a href="http://bytepawn.com/ab-tests-moving-fast-vs-being-sure.html"&gt;A/B tests: Moving Fast vs Being Sure&lt;/a&gt;, startups should favor velocity over certainty, using 0.1 or 0.2 is fine.&lt;/li&gt;
&lt;li&gt;$ 1 - \beta $, the True Positive Rate (TPR) or power: if B is actually better than A, how likely are we to actually measure B to be better at the above $ \alpha $? If we don't account for this, by default the math will work out set $ \beta = 0.5 $, which means we will only find half of the good Bs. In real life we usually set power to 0.8. For more on power, see &lt;a href="https://influentialpoints.com/Training/statistical_power_and_sample_size-principles-properties-assumptions.htm"&gt;this article&lt;/a&gt;. We usually set power to 0.8.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Simulating an A/B test&lt;/h2&gt;
&lt;p&gt;Let’s pretend we’re running an A/B test on funnel conversion. A is the current, B is the new version of the funnel. We want to know whether B is better. By looking at our funnel dashboard, we know that A is historically converting around 9-11%.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Formulate the action hypothesis: B has higher conversion than A, meaning we're doing a one-sided test.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; We set $ \alpha = 0.10 $ and $ 1 - \beta = 0.80 $. This means we're okay with 10% false positives and we will capture 80% of improvements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3.&lt;/strong&gt; Decide traffic split. Let’s say we will keep 80% in A, 20% in B. This is how much of a chance we take, B could be worse, buggy, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4.&lt;/strong&gt; Figure out how many samples we need to collect, given the historic conversion, traffic split, alpha and the kind of lift we’re looking. The code below computes sample size based on the math above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy.stats&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;alpha_to_z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_sided&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;one_sided&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ppf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;power_to_z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;power&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;norm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ppf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pos&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_delta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;traffic_ratio_A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_sided&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;z_alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha_to_z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_sided&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;z_power&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;power_to_z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mu_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mu_delta&lt;/span&gt;
    &lt;span class="n"&gt;traffic_ratio_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;traffic_ratio_A&lt;/span&gt;
    &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;traffic_ratio_A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;traffic_ratio_B&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;z_alpha&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;z_power&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_A&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ceil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that in real-life, there are other considerations. For example, if possible we should run tests for complete days and/or weeks, to capture users who are active at different times. So when we calculate the sample size, in real life we compare that to the number of users going through the funnel per day/week, and "round up". To take this into account in the simulation below, I will multiply by 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5.&lt;/strong&gt; Create a random seed for the A/B test and save it server-side. We generate a new seed for each A/B test. Let’s say we generate the string for this one: &lt;code&gt;OkMdZa18pfr8m5sy2IL52pW9ol2EpLekgakJAIZFBbgZ&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 6.&lt;/strong&gt; Perform test by splitting users randomly between A and B according to the above proportions. Users coming, identified by a &lt;code&gt;user_id&lt;/code&gt; (or &lt;code&gt;cookie_id&lt;/code&gt;), should be put in the same funnel. We can accomplish this by hashing the &lt;code&gt;test_id&lt;/code&gt;, where &lt;code&gt;test_id = seed + user_id&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hashlib&lt;/span&gt;

&lt;span class="n"&gt;test_seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;OkMdZa18pfr8m5sy2IL52pW9ol2EpLekgakJAIZFBbgZ&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;funnel_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_seed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;test_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hashlib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;md5&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_seed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ascii&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ascii&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hexdigest&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;bits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;bin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bit&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bits&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Step 7.&lt;/strong&gt; Run the test. We're simulating the real-world here, so we will have to pick the actual conversions for A and B. This is not known to the test, this is what it's trying to estimate, so we call this a hidden variable:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.105&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.115&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run_test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;funnel_user_func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;test_outcomes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;conversions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;conversions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;}}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;which_funnel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;funnel_user_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# returns &amp;#39;A&amp;#39; or &amp;#39;B&amp;#39;&lt;/span&gt;
        &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;which_funnel&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;which_funnel&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;which_funnel&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conversions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Step 8.&lt;/strong&gt; Compute the p-value and compare it with the $ \alpha $ we set to decide whether to accept or reject B:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;p_value&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_sided&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sigma_A_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;N_A&lt;/span&gt;
    &lt;span class="n"&gt;sigma_B_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;N_B&lt;/span&gt;
    &lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma_A_squared&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sigma_B_squared&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_B&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;z_to_p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_sided&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;

&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;
&lt;span class="n"&gt;power&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.80&lt;/span&gt;
&lt;span class="n"&gt;base_conversion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;
&lt;span class="n"&gt;valuable_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
&lt;span class="n"&gt;base_traffic_split&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;

&lt;span class="n"&gt;N_required&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_conversion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mu_delta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valuable_diff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;traffic_ratio_A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N_actual&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;N_required&lt;/span&gt; &lt;span class="c1"&gt;# because eg. we run it for a whole week&lt;/span&gt;

&lt;span class="c1"&gt;# hidden_conversion_params is how our funnels actually perform:&lt;/span&gt;
&lt;span class="c1"&gt;# the difference between the two is what we&amp;#39;re trying to establish&lt;/span&gt;
&lt;span class="c1"&gt;# with statistical confidence, using an A/B test&lt;/span&gt;
&lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.105&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.115&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;test_seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;OkMdZa18pfr8m5sy2IL52pW9ol2EpLekgakJAIZFBbgZ&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;test_outcomes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;run_test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;N_actual&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;funnel_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_seed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mu_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conversions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;mu_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conversions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Measured conversion for A: &lt;/span&gt;&lt;span class="si"&gt;%.3f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Measured conversion for B: &lt;/span&gt;&lt;span class="si"&gt;%.3f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p_value&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;p-value: &lt;/span&gt;&lt;span class="si"&gt;%.3f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;B is better, deploy&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;We&amp;#39;re not sure if B is better than A&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The outcome depends on chance, here's one run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{&amp;#39;A&amp;#39;: {&amp;#39;N&amp;#39;: 43464, &amp;#39;conversions&amp;#39;: 4620}, &amp;#39;B&amp;#39;: {&amp;#39;N&amp;#39;: 10812, &amp;#39;conversions&amp;#39;: 1222}}
Measured conversion for A: 0.106
Measured conversion for B: 0.113
p-value: 0.023
Action: B is better, deploy
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you run this repeatedly, sometimes it will indicate B is better, sometimes it will not. It will find B better more often:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;at higher $ N $, if in fact B is better&lt;/li&gt;
&lt;li&gt;if the conversion advantage of B is greater in the hidden conversion parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Consistency test for FPR ($\alpha$)&lt;/h2&gt;
&lt;p&gt;We can set the A and B equal in the hidden conversion parameters, repeatedly perform the A/B test, and count the ratio of times it finds B to be better than A at the $ \alpha $ level, ie. the ratio of false positives. It should be equal to the $ \alpha $ we set to compute the sample size!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;num_tests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;
&lt;span class="n"&gt;base_conversion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;
&lt;span class="n"&gt;valuable_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
&lt;span class="n"&gt;base_traffic_split&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;
&lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_conversion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mu_delta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valuable_diff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;traffic_ratio_A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# get rid of this of the hashing, it&amp;#39;s slow, we don&amp;#39;t need it for a simulation&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;funnel_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run_tests&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_tests&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;funnel_user_func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;num_successes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_tests&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;test_outcomes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;run_test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;funnel_user_func&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;mu_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conversions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;mu_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conversions&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;p_value&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mu_B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;num_successes&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;num_successes&lt;/span&gt;

&lt;span class="n"&gt;num_successes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;run_tests&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;num_tests&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;funnel_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_successes&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;num_tests&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;False Positive Rate = &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s2"&gt; (expected = &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It prints:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;False Positive Rate = 0.10 (expected = 0.10)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Consistency test for TPR ($1 - \beta$)&lt;/h2&gt;
&lt;p&gt;We can set the A and B hidden conversion parameters exactly like the assumption we used to compute the sample size (ie. 10% and 11%), repeatedly perform the A/B test, and count the ratio of times it finds B to be better than A at the $ \alpha $ level. It should be equal to the $ 1 - \beta $ power we set to compute the sample size!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;num_tests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;
&lt;span class="n"&gt;power&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.80&lt;/span&gt;
&lt;span class="n"&gt;base_conversion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;
&lt;span class="n"&gt;valuable_diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
&lt;span class="n"&gt;base_traffic_split&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;
&lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.11&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_samples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mu_A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_conversion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mu_delta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valuable_diff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;traffic_ratio_A&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;num_successes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;run_tests&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;num_tests&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;hidden_conversion_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;funnel_user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_traffic_split&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;tpr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num_successes&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;num_tests&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;True Positive Rate: &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s2"&gt; (expected = &lt;/span&gt;&lt;span class="si"&gt;%.2f&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tpr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It prints:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;True Positive Rate: 0.80 (expected = 0.80)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Scaling behaviour&lt;/h2&gt;
&lt;p&gt;Let's fix all parameters but one, and see how many samples we need.&lt;/p&gt;
&lt;p&gt;Vary base conversion $\mu_A$, with fixed $ \mu_B - \mu_A, \alpha, 1 - \beta$, traffic split. Because the formula for $z$ includes a term like $\mu_A(1-\mu_A)$, this should be highest at $\mu_A=0.5$.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/base_conversion_N.png" alt="Base conversion vs N" style="width: 400px;"/&gt;&lt;/p&gt;
&lt;p&gt;Vary conversion difference $\mu_B - \mu_A$, with fixed $ \mu_A, \alpha, 1 - \beta$, traffic split. A higher conversion difference requires less samples to detect.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/conversion_diff.png" alt="Conversion difference vs N" style="width: 400px;"/&gt;&lt;/p&gt;
&lt;p&gt;Vary $\alpha$, with fixed $ \mu_A, \mu_B - \mu_A, 1 - \beta$, traffic split. Lower $\alpha$ means we want less false positives, which requires more samples.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/alpha_N.png" alt="Alpha vs N" style="width: 400px;"/&gt;&lt;/p&gt;
&lt;p&gt;Vary the power $1 - \beta$, with fixed $ \mu_A, \mu_B - \mu_A, \alpha$, traffic split. Higher $1 - \beta$ translates to higher probability of detecting positive outcomes, which requires more samples.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/power_N.png" alt="Power vs N" style="width: 400px;"/&gt;&lt;/p&gt;
&lt;p&gt;Vary the traffic split to A, with fixed $ \mu_A, \mu_B - \mu_A, \alpha, 1 - \beta$. The sample size is constrained by the smaller sample size of the two, so an equal split requires the least amount of samples.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/traffic_split_N.png" alt="Traffic split vs N" style="width: 400px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Z-tests are simple, if you remember the CLT and are careful about controlling false positive rate and true negatives rates. If in doubt, write simulation code like above and make sure the way you set your parameters gets you the results you want. Also remember that there are other types of tests, such as the &lt;a href="https://en.wikipedia.org/wiki/Chi-squared_test"&gt;$\chi^2$-test&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/Student%27s_t-test"&gt;t-test&lt;/a&gt;, to be discussed in the next posts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related links:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/the-art-of-a-b-testing-5a10c9bb70a4"&gt;The Art of A/B Testing&lt;/a&gt; - good post on the same topic&lt;/li&gt;
&lt;/ul&gt;</content><category term="ab-testing"></category></entry><entry><title>Optimizing waits in Airflow</title><link href="/optimizing-waits-in-airflow.html" rel="alternate"></link><published>2020-02-01T00:00:00+01:00</published><updated>2020-02-01T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2020-02-01:/optimizing-waits-in-airflow.html</id><summary type="html">&lt;p&gt;Sometimes I get to put on my Data Engineering hat for a few days. I enjoy this because I like to move up and down the Data Science stack and I try to keep myself sharp technically. Recently I was able to spend a few days optimizing our Airflow ETL for speed. &lt;br/&gt;&lt;br/&gt; &lt;img src="/images/airflow-dag.png" alt="Airflow DAG" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sometimes I get to put on my Data Engineering hat for a few days. I enjoy this because I like to move up and down the Data Science stack and I try to keep myself sharp technically. Recently I was able to spend a few days optimizing our &lt;a href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt; ETL for speed. We noticed that DWH jobs with a lots of waits are taking a lot of time to complete the waits (not counting the actual waiting time). Below is a list of changes I made to improve our waiting time.&lt;/p&gt;
&lt;h2&gt;Our history of waiting on tables&lt;/h2&gt;
&lt;p&gt;The basic premise is this. Suppose you have a DWH job that creates the latest &lt;code&gt;ds&lt;/code&gt; partition for &lt;code&gt;result_table&lt;/code&gt;, and the &lt;code&gt;INSERT&lt;/code&gt; is a result of a &lt;code&gt;SELECT&lt;/code&gt; like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;result_table&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;table1&lt;/span&gt;
&lt;span class="k"&gt;INNER&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt;
    &lt;span class="n"&gt;table2&lt;/span&gt;
&lt;span class="k"&gt;ON&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;INNER&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt;
    &lt;span class="n"&gt;table3&lt;/span&gt;
&lt;span class="k"&gt;ON&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In our ETL, we would write this like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;result_table&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;curent_ds_wait&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;table1&lt;/span&gt;
&lt;span class="k"&gt;INNER&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt;
    &lt;span class="n"&gt;curent_ds_wait&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;table2&lt;/span&gt;
&lt;span class="k"&gt;ON&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;INNER&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt;
    &lt;span class="n"&gt;curent_ds_wait&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;table3&lt;/span&gt;
&lt;span class="k"&gt;ON&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our framework parses the SQL snippets and extracts tables names after &lt;code&gt;current_ds_wait::&lt;/code&gt;. These are the list of tables where we need to wait for today's &lt;code&gt;ds&lt;/code&gt; partition to land before we can run the &lt;code&gt;SELECT&lt;/code&gt; (otherwise the result would be incomplete).&lt;/p&gt;
&lt;p&gt;I described &lt;code&gt;ds&lt;/code&gt; partitions in an &lt;a href="http://bytepawn.com/fetchr-airflow.html#fetchr-airflow"&gt;earlier post&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The other major design pattern from Facebook is the idea of daily partitioned tables. This is a feature available on Hive, and not really practical on eg. Redshift. Essentially we store (complete) daily, write-once slices of each table, which are generated by daily jobs. The partitions are called &lt;code&gt;ds&lt;/code&gt; at Facebook and logically show up as a column of the table, and you’ll find plenty of references to it if you read the Hive docs (because Hive was written at Facebook). Physically, these are essentially directories, each one holding the data files for that day’s data. We use S3, so in our case it looks something like &lt;code&gt;s3://dwh-bucket/&amp;lt;table&amp;gt;/&amp;lt;ds&amp;gt;/&amp;lt;data_files&amp;gt;&lt;/code&gt;. For example, &lt;code&gt;s3://dwh-bucket/company_metrics/2018-03-01/datafile&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So when our framework generates the DAG for this DWH job, it generates an &lt;code&gt;insert&lt;/code&gt; task (&lt;code&gt;PrestoOperator&lt;/code&gt; operator), which depends on 3 &lt;code&gt;wait&lt;/code&gt; tasks (&lt;code&gt;DsPartitionSensor&lt;/code&gt; operators), one for each table. There a bunch of other tasks that we generate (such as tasks for running &lt;code&gt;CREATE TABLE IF NOT EXISTS&lt;/code&gt;), but let’s ignore that.&lt;/p&gt;
&lt;p&gt;So this part of the DAG looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    insert
    |
    +-- wait_table1
    |
    +-- wait_table2
    |
    +-- wait_table3
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Chaining waits&lt;/h2&gt;
&lt;p&gt;Initially, the &lt;code&gt;wait&lt;/code&gt; jobs issued a Presto SQL statement like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SHOW&lt;/span&gt; &lt;span class="n"&gt;PARTITIONS&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="k"&gt;table&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;ds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{ds}&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;LIMIT&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first thing we noticed is that this overloaded our Presto cluster. We have ~100 jobs, and each has a couple of &lt;code&gt;wait&lt;/code&gt;s, so this results in hundreds of &lt;code&gt;wait&lt;/code&gt;s trying to run at the same time. Also, since we only have a limited number of worker slots on our Airflow worker, sometimes the &lt;code&gt;wait&lt;/code&gt;s would use up all the slots, and the actual &lt;code&gt;insert&lt;/code&gt;s never ran, or spent a long time in the queue, waiting to be executed.&lt;/p&gt;
&lt;p&gt;So one of the initial optimizations was to chain the waits on the DAG, like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    insert
    |
    +-- wait_table1
          |
          +-- wait_table2
               |
               +-- wait_table3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This way each DAG only ever has one &lt;code&gt;wait&lt;/code&gt; job running. The &lt;code&gt;wait&lt;/code&gt; jobs per DAG run sequentally. This change was easy to make, because we don't construct our DAGs "by hand" for each table, we have a helper function which does this (which also does the &lt;code&gt;current_ds_wait::&lt;/code&gt; stuff), so we just needed to make this change in one place.&lt;/p&gt;
&lt;h2&gt;Task pools&lt;/h2&gt;
&lt;p&gt;The second thing we tried was to use Airflow’s pool feature. With this, tasks can be assigned to pools, and per pool limits can be set on execution. So if we have 32 worker slots, we can set up a &lt;code&gt;wait&lt;/code&gt; pool with 24 slots, so no more than 24 &lt;code&gt;wait&lt;/code&gt;s can be running.&lt;/p&gt;
&lt;p&gt;Unfortunately, this feature in Airflow is buggy/broken. In our setup, where we’re running a separate master and worker, and using &lt;a href="https://airflow.apache.org/docs/1.10.6/howto/executor/use-celery.html"&gt;Celery&lt;/a&gt; for running worker tasks, the Airflow scheduler doesn’t respect the limits, &lt;a href="https://issues.apache.org/jira/browse/AIRFLOW-584"&gt;similar to this bug report&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Hive instead of Presto&lt;/h2&gt;
&lt;p&gt;Since all our DWH jobs run on Presto, our Hive execution engine is just sitting around idle handling metadata queries such as &lt;code&gt;CREATE TABLE&lt;/code&gt; (&lt;code&gt;create&lt;/code&gt; tasks in the DAG) . So by running the &lt;code&gt;SHOW PARTITION&lt;/code&gt; (the syntax starts the same, but it’s a bit different on Hive) on Hive, we can get rid of 95% of the jobs on the Presto cluster, which were taking a long time to run, even though they’re just checking for the presence of a partition. The Hive engine can handle these metadata queries easily, returning in less than a second.&lt;/p&gt;
&lt;h2&gt;Multiwaits&lt;/h2&gt;
&lt;p&gt;In the example above, we’re waiting on 3 tables, and we generate 3 &lt;code&gt;wait&lt;/code&gt; jobs. We realized this is inefficient, and we can just have one &lt;code&gt;multi_wait&lt;/code&gt; task which checks all 3 partitions at once. We just generate a HQL with several &lt;code&gt;SHOW PARTITION&lt;/code&gt; statements separated by &lt;code&gt;;&lt;/code&gt; and parse the resulting string to see what’s there and what’s missing. So the final DAG looks very simple:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    insert
    |
    +-- multi_wait
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is such an obvious idea..&lt;/p&gt;
&lt;h2&gt;Reducing time between jobs&lt;/h2&gt;
&lt;p&gt;Looking at the scheduler logs, we still have a problem: the &lt;code&gt;multi_wait&lt;/code&gt; finishes at time X, but the insert is only launched at time X+5 minutes. This is a generic issue with Airflow, not specific to &lt;code&gt;wait&lt;/code&gt;s. Why does Airflow need 5 minutes to figure out that a task’s dependencies are all finished?&lt;/p&gt;
&lt;p&gt;To understand this I looked through the logs, and found that indeed, the first time in the logs that Airflow notices that the insert can run is several minutes after the &lt;code&gt;multi_wait&lt;/code&gt; finishes. To understand this I took the log line and looked it up in the Airflow source code. What happens is this:&lt;/p&gt;
&lt;p&gt;Every 30 seconds the Airflow scheduler (as configured) lists out all .py files in the &lt;code&gt;dags&lt;/code&gt; folder. It saves these known &lt;code&gt;.py&lt;/code&gt; files, and then in a “round-robin” manner, executes them: it runs &lt;code&gt;one.py&lt;/code&gt;, &lt;code&gt;two.py&lt;/code&gt;, &lt;code&gt;three.py&lt;/code&gt;, and so on, where each of the &lt;code&gt;.py&lt;/code&gt; files is a DAG definition in our case. Each time it executes the &lt;code&gt;.py&lt;/code&gt; file, it looks at instances of the &lt;code&gt;DAG&lt;/code&gt; class in the global namespace, and those are the &lt;code&gt;DAG&lt;/code&gt;s it executes. &lt;strong&gt;The problem is, the Airflow scheduler only checks for new runnable tasks (ie. all dependencies are finished) when it’s running the appropriate &lt;code&gt;.py&lt;/code&gt; file!&lt;/strong&gt; This is a very unfortunate architectural choice. And this explains why it takes ~5 minutes between task executions: we have about ~100 ETL jobs in ~100 &lt;code&gt;.py&lt;/code&gt; files, and running a &lt;code&gt;.py&lt;/code&gt; file takes 3-5 seconds. The reason it takes 3-5 seconds to execute a 100 line Python program is because they have to &lt;code&gt;import&lt;/code&gt; Airflow libraries (to get &lt;code&gt;DAG&lt;/code&gt; class, etc), and those Airflow &lt;code&gt;import&lt;/code&gt;s take time:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.operators&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PrestoOperator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DropDsPartitionOperator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DsPartitionSensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PostgresOperator&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.operators.hive_operator&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;HiveOperator&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.hooks.presto_hook&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PrestoHook&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow.hooks.base_hook&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BaseHook&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;airflow&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DAG&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I asked the following question in the &lt;a href="https://apache-airflow-slack.herokuapp.com/"&gt;Airflow slack #support channel&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I'm trying to debug my Airflow (1.8.2), we've been using it in prod for ~2 yrs. My issue is that it takes a long time between task runs. Ie. task X is waiting on task Y to finish in a DAG. Y finishes, and then it takes ~5 minutes for X to get queued and executed. Overall these 5 mins add up and add hours to the ETL running time.&lt;/p&gt;
&lt;p&gt;I've been doing some debugging, and looking at the Airflow source code; what I found so far:
- for a task to be run, all upstream tasks have to finished, and the task has to be in &lt;code&gt;SCHEDULED&lt;/code&gt; state: &lt;code&gt;jobs.py::_execute_task_instances()&lt;/code&gt; called like &lt;code&gt;_execute_task_instances(simple_dag_bag, (State.SCHEDULED,))&lt;/code&gt;
- a task goes from &lt;code&gt;None&lt;/code&gt; state to &lt;code&gt;SCHEDULED&lt;/code&gt; state in &lt;code&gt;jobs.py::process_file()&lt;/code&gt;, which corresponds to lines like &lt;code&gt;Started a process (PID: 28897) to generate tasks for ...&lt;/code&gt; lines in my syslog
- by default my tasks are in &lt;code&gt;None&lt;/code&gt; state (I see this in Task Instance Details view on web UI).
- I have ~100 DAG python files, each takes ~3 seconds to execute to collect the DAG, so a "roundtrip" takes ~300secs = 5mins
- so I'm guessing this is what's causing the ~5 minute delay, that each DAG python file is re-read every 5 mins, and that's when Airflow realizes that the deps are good and makes it &lt;code&gt;SCHEDULED&lt;/code&gt;. Correct me if I'm wrong.
What's confusing to me is, why does Airflow need to re-read the file to notice that all upstream tasks are good to go?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I received no clear answer, other than a link about &lt;a href="https://www.astronomer.io/blog/profiling-the-airflow-scheduler/"&gt;profiling the Airflow scheduler&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Given this limitation, the obvious workaround is to put several DAGs into one &lt;code&gt;.py&lt;/code&gt; file, thus saving time on &lt;code&gt;import&lt;/code&gt;s. For example, right now we have one &lt;code&gt;.py&lt;/code&gt; file per table &lt;code&gt;import&lt;/code&gt; from the production database, which is very nice in terms of code layout in the IDE, and in terms of following changes on &lt;code&gt;git&lt;/code&gt;. But we could put all these into one big &lt;code&gt;.py&lt;/code&gt; file, and have one big &lt;code&gt;.py&lt;/code&gt; file per “type” of DAG (eg. one file for imports, one for exports, etc).&lt;/p&gt;
&lt;p&gt;I haven’t yet made up my mind whether we should do this: it feels wrong to sacrifice everyday engineering UX for an accidental architectural flaw in the ETL system.&lt;/p&gt;
&lt;h2&gt;Further optimizations&lt;/h2&gt;
&lt;p&gt;Other ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;talk straight to the Hive metastore&lt;/li&gt;
&lt;li&gt;cache existing partitions once we know they're there&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are good idea, but at this point (repeatedly) querying Hive with &lt;code&gt;SHOW PARTITION&lt;/code&gt;s is not a bottleneck, so it wouldn't help us.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Airflow is the 5th ETL tool I use: we wrote 3 hand-rolled ETL system at Prezi (one in bash, one in Haskell, one in Go), at Facebook we used &lt;a href="https://asiliconvalleyinsider.com/2016/05/01/data-engineering-facebook/"&gt;Dataswarm&lt;/a&gt;, and at Fetchr we use Airflow (which is based on Dataswarm). I think it’s great that we have Airflow, because it’s miles better than a hand-rolled ETL system. Also, as it matures, it will get better!&lt;/p&gt;
&lt;p&gt;Having said that, I hope the open source Data Engineering community will improve Airflow in the future to address these issues. My problems with Airflow are three-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bugs:&lt;ul&gt;
&lt;li&gt;the unreliable pools feature&lt;/li&gt;
&lt;li&gt;Airflow can get stuck in various ways:&lt;ul&gt;
&lt;li&gt;all worker slots are used up by tasks which block; in this case, we have to &lt;code&gt;ps ax | grep airflow | grep wait | awk ' { print  kill -9 $1 } '&lt;/code&gt; on the worker&lt;/li&gt;
&lt;li&gt;sometimes tasks get stuck in &lt;code&gt;null&lt;/code&gt; or &lt;code&gt;queued&lt;/code&gt; status; in this case, we have to manually re-kick them on the UI&lt;/li&gt;
&lt;li&gt;sometimes the scheduler itself runs into a bug and gets stuck; in this case we have to restart the scheduler itself on the master with &lt;code&gt;systemctl restart airflow-scheduler.service&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;architectural shortcomings&lt;ul&gt;
&lt;li&gt;only making progress on the DAG when re-running the &lt;code&gt;.py&lt;/code&gt; file containing the &lt;code&gt;DAG&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;right now, for every task instance Airflow launches a new, expensive Python process on the worker node, which takes hundreds of MBs of memory and the turn-around time is quite slow; it'd be nice to come up with an "in-process" way to launch small inexpensive checks quickly (like &lt;code&gt;wait&lt;/code&gt;s)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;resource hungry: we run two beefy EC2 instances, one airflow-master and one airflow-worker, but all these really do is manage a relatively small DAG (~100 DAGs each with ~10 tasks); the actual work is performed on a third node (actually, a cluster), the EC2 nodes that are running Presto and the various ML jobs; still, both nodes show 2-3 load with &lt;code&gt;top&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are real issues that affect our production every day in terms of landing time, dollars and engineering time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Airflow is slow to make progress&lt;/li&gt;
&lt;li&gt;we often have to manually kill / clear / re-kick jobs&lt;/li&gt;
&lt;li&gt;we run two EC2 nodes just for Airflow (master and worker)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I definitely don’t regret using Airflow, but it would be nice if the core engine itself would be more efficient, performant, and less wasteful. 10 years ago I was a proud C++ programmer, &lt;a href="https://github.com/scalien/scaliendb"&gt;building database kernels and storage engines&lt;/a&gt;, optimizing away bits and bytes. Today, because I try to &lt;em&gt;move fast and focus on impact&lt;/em&gt;—which is the right thing to do, despite these issues—throwing hardware and money at a simple problem of managing a small DAG is the best option. Feels weird.&lt;/p&gt;</content><category term="data"></category><category term="airflow"></category><category term="python"></category></entry><entry><title>SQL best practices for Data Scientists and Analysts</title><link href="/sql-best-practices-for-data-scientists-and-analysts.html" rel="alternate"></link><published>2020-01-26T00:00:00+01:00</published><updated>2020-01-26T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2020-01-26:/sql-best-practices-for-data-scientists-and-analysts.html</id><summary type="html">&lt;p&gt;My list of SQL best practices for Data Scientists and Analysts, or, how I personally write SQL code. I picked this up at Facebook, and later improved it at Fetchr. &lt;br/&gt;&lt;br/&gt; &lt;img src="/images/sql-constr.jpg" alt="SQL code" style="width: 300px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;a href="http://bytepawn.com/how-i-write-sql-code.html"&gt;previous post was about SQL code style&lt;/a&gt;, like uppercasing, indentation and overall structure like &lt;code&gt;WITH&lt;/code&gt;. This is about how to write readable, concise and efficient &lt;code&gt;SELECT&lt;/code&gt; statements that minimize bugs.&lt;/p&gt;
&lt;p&gt;For completeness, I will quickly list out the coding style suggestions from the &lt;a href="http://bytepawn.com/how-i-write-sql-code.html"&gt;last post&lt;/a&gt;, without explanations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use all upper-case for keywords like &lt;code&gt;SELECT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Keep &lt;code&gt;SELECT .. FROM .. WHERE .. GROUP BY&lt;/code&gt; unindented&lt;/li&gt;
&lt;li&gt;Line up &lt;code&gt;WHERE&lt;/code&gt; conditions&lt;/li&gt;
&lt;li&gt;Write &lt;code&gt;GROUP BY 1, 2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Indent &lt;code&gt;WHEN&lt;/code&gt; relative to &lt;code&gt;CASE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;WITH&lt;/code&gt; instead of nested sub-&lt;code&gt;SELECT&lt;/code&gt;s&lt;/li&gt;
&lt;li&gt;Long lines are okay&lt;/li&gt;
&lt;li&gt;Break the rules for readability and flow of code&lt;/li&gt;
&lt;li&gt;Follow the same rules when writing interactive SQL code&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Without further ado, my SQL best practices for Data Scientists and Analysts follow below.&lt;/p&gt;
&lt;h2&gt;Use &lt;code&gt;COUNT(DISTINCT id)&lt;/code&gt; instead of &lt;code&gt;COUNT(*)&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;COUNT(*)&lt;/code&gt; is dangerous, because you could be counting things do don't want to. For example, a table called &lt;code&gt;dispatches&lt;/code&gt; might include multiple dispatch events for a delivery order. If you're counting dispatches for a week, do you want to double-count dispatches for the same order, if the order was dispatched but didn't get delivered the first time (customer was unreachable), and was re-dispatched again? Also, tables can sometimes get polluted with bad data, eg. if there was a software issue issue in production and the warehouse staff triggered multiple dispatch events. Also, although we expect data engineers to pre-clean data, sometimes new modes of dirtyness appear. And sometimes it happens that a bug is introduced in the ETL pipeline, and eg. all rows are duplicated in a table. Although it's not the analyst's fault, it's better to be defensive. Example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;                    &lt;span class="c1"&gt;-- BAD:  what are we counting here?&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;-- GOOD: much clearer, we&amp;#39;re counting unique orders&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;dispatches&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Use &lt;code&gt;ROW()&lt;/code&gt; when computing metrics over multiple days&lt;/h2&gt;
&lt;p&gt;In the above example, suppose we want to count dispatches for 7 days, and we want to count re-dispatches of the same package. In this case &lt;code&gt;COUNT(DISTINCT tracking_id)&lt;/code&gt; won't work, because it won't double count double dispatches. And we said &lt;code&gt;COUNT(*)&lt;/code&gt; is evil. The way out is to &lt;code&gt;DISTINCT&lt;/code&gt; on both days and orders, and the way to do that is with &lt;code&gt;ROW()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;                              &lt;span class="c1"&gt;-- BAD:  what are we counting here?&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;            &lt;span class="c1"&gt;-- GOOD: unique orders dispatched&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;ROW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;-- GOOD: dispatches, but we only count an order once a day&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;dispatches&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="k"&gt;day&lt;/span&gt; &lt;span class="k"&gt;BETWEEN&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-06&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-12&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Always name columns with &lt;code&gt;AS&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Continuing with the previous example, we should always name our columns:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;clown_town&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_unique_orders_dispatched&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;ROW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_dispatches&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;dispatches&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="k"&gt;day&lt;/span&gt; &lt;span class="k"&gt;BETWEEN&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-06&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-12&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Use &lt;code&gt;CASE WHEN&lt;/code&gt; instead of writing multiple queries&lt;/h2&gt;
&lt;p&gt;Suppose we want to count both dispatches and deliveries, and there's a table &lt;code&gt;core_events&lt;/code&gt; which has both. We can accomplish this with one &lt;code&gt;SELECT&lt;/code&gt;, we don't have to write two:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;CASE&lt;/span&gt; &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;event&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;dispatched&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="k"&gt;ROW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;END&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_dispatches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;CASE&lt;/span&gt; &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;event&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;delivered&amp;#39;&lt;/span&gt;  &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="k"&gt;ROW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;END&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;CASE&lt;/span&gt; &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;event&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;dispatched&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;           &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;END&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_unique_orders_dispatched&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;CASE&lt;/span&gt; &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;event&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;delivered&amp;#39;&lt;/span&gt;  &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;           &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;END&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_unique_orders_delivered&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;core_events&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="k"&gt;day&lt;/span&gt; &lt;span class="k"&gt;BETWEEN&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-06&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-12&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Remember, &lt;code&gt;COUNT()&lt;/code&gt; doesn't count &lt;code&gt;NULL&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;In the above example, we expect:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;num_delivered = num_unique_orders_delivered&lt;/code&gt; because an order can only be delivered once&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_unique_orders_dispatched &amp;lt;= num_dispatches&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_delivered &amp;lt;= num_dispatches&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Use &lt;code&gt;IN (...)&lt;/code&gt; instead or &lt;code&gt;OR&lt;/code&gt;s&lt;/h2&gt;
&lt;p&gt;In the above example, for clarity and efficiency, we should only include &lt;code&gt;dispatched&lt;/code&gt; and &lt;code&gt;delivered&lt;/code&gt; rows. Instead of writing &lt;code&gt;event = 'dispatched' OR event = 'delivered'&lt;/code&gt;, use &lt;code&gt;IN()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;core_events&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="n"&gt;event&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;dispatched&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;delivered&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Use &lt;code&gt;BETWEEN&lt;/code&gt; for dates instead of &lt;code&gt;&amp;gt;=&lt;/code&gt; and &lt;code&gt;=&amp;lt;&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Don't write &lt;code&gt;day &amp;gt;= DATE('2020-01-06') AND day &amp;lt;= DATE('2020-01-12')&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Write &lt;code&gt;day BETWEEN DATE('2020-01-06') AND DATE('2020-01-12')&lt;/code&gt;, it's much more readable.&lt;/p&gt;
&lt;p&gt;Remember that &lt;code&gt;BETWEEN&lt;/code&gt; is inclusive, so &lt;code&gt;x BETWEEN 1 AND 3&lt;/code&gt; is the same as &lt;code&gt;x IN (1, 2, 3)&lt;/code&gt; for an &lt;code&gt;INT&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Use “advanced” aggregation functions such as &lt;code&gt;MAX_BY()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;If you're doing analytics work, use a database meant for it, like &lt;a href="https://prestodb.io/"&gt;Presto&lt;/a&gt;. A good database meant for analytics work will have lots of useful aggregation functions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;MAX_BY()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MIN_BY()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ARRAY_AGG()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ARBITRARY()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;COUNT_IF()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;APPROX_PERCENTILE()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;KURTOSIS()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SKEWNESS()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand what these are, check the &lt;a href="https://prestodb.io/docs/current/functions/aggregate.html"&gt;Presto docs&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;When taking &lt;code&gt;AVG()&lt;/code&gt;, also check min/max/median/p10/p90 values&lt;/h2&gt;
&lt;p&gt;We often write aggregations and compute the mean summary statistic with &lt;code&gt;AVG()&lt;/code&gt;. In the logistics/delivery world, the most common example is to compute the metric &lt;strong&gt;average Deliveries per Driver&lt;/strong&gt; for a fleet. So we write the query, the fleet average comes out to &lt;code&gt;DPD=30.4&lt;/code&gt;, which means on average a driver makes 30.4 deliveries per day. Sounds reasonable. But there could be a lot of junk in there:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;drivers who are just hired and in training, and making 2-3 deliveries/day; probably shouldn't be counted&lt;/li&gt;
&lt;li&gt;internal drivers making internal deliveries; probably shouldn't be counted&lt;/li&gt;
&lt;li&gt;suppose the company just introduced self-pickups, but technically in the production system these show up as a &lt;code&gt;SELF_PICKUP&lt;/code&gt; driver, ie. all self-pickups are under one virtual driver's accounts, who has 1000+ deliveries; probably shouldn't be counted&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See my earlier post &lt;a href="http://bytepawn.com/beat-the-averages.html#beat-the-averages"&gt;Beat the averages&lt;/a&gt; for more on this.&lt;/p&gt;
&lt;p&gt;There's lots of ways to catch problems like this, but one cheap way is to check the edges of the distribution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;WITH&lt;/span&gt;
&lt;span class="n"&gt;daily_dpd&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;SELECT&lt;/span&gt;
        &lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;driver_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_delivered&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt;
        &lt;span class="n"&gt;deliveries&lt;/span&gt;
    &lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="p"&gt;),&lt;/span&gt;

&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;AVG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;dpd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="c1"&gt;-- let&amp;#39;s get some additional statistics to make sure we&amp;#39;re&lt;/span&gt;
    &lt;span class="c1"&gt;-- not fooling ourselves by reporting the average&lt;/span&gt;
    &lt;span class="k"&gt;MIN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;min_delivered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;MIN_BY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;driver_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;min_delivered_driver_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;MAX&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;max_delivered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;MAX_BY&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;driver_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;max_delivered_driver_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;APPROX_PERCENTILE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;dpd_p10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;APPROX_PERCENTILE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;dpd_p50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;-- aka median&lt;/span&gt;
    &lt;span class="n"&gt;APPROX_PERCENTILE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_delivered&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;dpd_p90&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;daily_dpd&lt;/span&gt;
&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
    &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Don’t forget to &lt;code&gt;COALESCE()&lt;/code&gt; when doing &lt;code&gt;LEFT/RIGHT/OUTER JOIN&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Similar to the above example, suppose you want to list out driver's names and daily DPDs for 2020-01-06:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;WITH&lt;/span&gt;
&lt;span class="n"&gt;daily_dpd&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;SELECT&lt;/span&gt;
        &lt;span class="n"&gt;driver_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="n"&gt;tracking_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_delivered&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt;
        &lt;span class="n"&gt;deliveries&lt;/span&gt;
    &lt;span class="k"&gt;WHERE&lt;/span&gt;
        &lt;span class="k"&gt;day&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-06&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
        &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="n"&gt;COALESCE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="c1"&gt;-- COALESCE() returns the first non-NULL argument passed to it&lt;/span&gt;
        &lt;span class="n"&gt;drivers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;daily_dpd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;driver_id&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="nb"&gt;VARCHAR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;driver_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;daily_dpd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_delivered&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;daily_dpd&lt;/span&gt;
&lt;span class="k"&gt;LEFT&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt;
    &lt;span class="n"&gt;drivers&lt;/span&gt;
&lt;span class="k"&gt;ON&lt;/span&gt;
    &lt;span class="n"&gt;daily_dpd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;driver_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;drivers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Also, always write out the table aliases (&lt;code&gt;daily_dpd&lt;/code&gt; and &lt;code&gt;drivers&lt;/code&gt; in this example) for clarity. It may be clear to you now where each column is coming from, but will you know in 3 months? Will the next guy know? &lt;/p&gt;
&lt;h2&gt;Double-check your parentheses in &lt;code&gt;WHERE&lt;/code&gt; when using &lt;code&gt;OR&lt;/code&gt;s&lt;/h2&gt;
&lt;p&gt;A nasty source of bugs in SQL code is a list of &lt;code&gt;AND&lt;/code&gt;, with an &lt;code&gt;OR&lt;/code&gt; hiding in there, with no parentheses, like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="n"&gt;country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;UAE&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="k"&gt;day&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;fleet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B2C&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;OR&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Acme Bank&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is almost certainly not what the writer intended. Notice I didn't follow my indentation rules here, which would help catch this. What we really want here is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
        &lt;span class="n"&gt;country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;UAE&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="k"&gt;day&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2020-01-16&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fleet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B2C&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;OR&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Acme Bank&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Write &lt;code&gt;1000*1000&lt;/code&gt; instead of &lt;code&gt;100000&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Notice how I left out a 0 above?&lt;/p&gt;
&lt;p&gt;Unfortunately, we cannot put commas into the numbers to improve readability in SQL (or other programming languages), so we can't write &lt;code&gt;1,000,000&lt;/code&gt;. The problem with &lt;code&gt;1000000&lt;/code&gt; is that it's hard to see whether we got the number of zeros right. This can lead to nasty bugs. It's better to pretend commas by multiplying like &lt;code&gt;1000*1000&lt;/code&gt; or &lt;code&gt;500*1000&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Use &lt;code&gt;DECIMAL&lt;/code&gt; not &lt;code&gt;DOUBLE&lt;/code&gt; when dealing with money&lt;/h2&gt;
&lt;p&gt;When dealing with money, never use &lt;code&gt;DOUBLE&lt;/code&gt;. There is a type for it, called &lt;code&gt;DECIMAL&lt;/code&gt;. In SQL, like in many programming languages, doubles are &lt;a href="https://en.wikipedia.org/wiki/IEEE_754"&gt;IEEE 754 floating points&lt;/a&gt;, and there's weird precision behaviour that may introduce nasty bugs. There's &lt;a href="https://0.30000000000000004.com/"&gt;entire sites&lt;/a&gt; dedicated to explaining this. Tldr = &lt;code&gt;DOUBLE&lt;/code&gt; is meant to be used for math like sine and cosine, &lt;code&gt;DECIMAL&lt;/code&gt; for money. Note that by default, if you write a literal like &lt;code&gt;0.2&lt;/code&gt;, it will be &lt;code&gt;DOUBLE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here's a simple example that may surprise you:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;does_math_work&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;result_double_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="nb"&gt;DECIMAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="nb"&gt;DECIMAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;does_this_other_math_work&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="nb"&gt;DECIMAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="nb"&gt;DECIMAL&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;result_decimal_type&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This returns on my Presto DB:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;-----------------+---------------------+---------------------------+---------------------+&lt;/span&gt;
&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;does_math_work&lt;/span&gt;  &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;result_double_type&lt;/span&gt;  &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;does_this_other_math_work&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;result_decimal_type&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt;
&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;-----------------+---------------------+---------------------------+---------------------+&lt;/span&gt;
&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="k"&gt;false&lt;/span&gt;           &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;30000000000000004&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="k"&gt;true&lt;/span&gt;                      &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;                 &lt;span class="o"&gt;|&lt;/span&gt;
&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="c1"&gt;-----------------+---------------------+---------------------------+---------------------+&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="data"></category><category term="programming"></category><category term="sql"></category></entry><entry><title>How I write SQL code</title><link href="/how-i-write-sql-code.html" rel="alternate"></link><published>2020-01-24T00:00:00+01:00</published><updated>2020-01-24T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2020-01-24:/how-i-write-sql-code.html</id><summary type="html">&lt;p&gt;This is a simple post about SQL code formatting. Most of this comes from my time as a Data Engineer at Facebook. &lt;br/&gt;&lt;br/&gt; &lt;img src="/images/sql.png" alt="SQL code" style="width: 600px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a simple post about SQL code formatting. Most of this comes from my time as a Data Engineer at Facebook.&lt;/p&gt;
&lt;p&gt;I’ve always cared a lot about writing readable code. Readability has several components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;directory layout&lt;/li&gt;
&lt;li&gt;breaking code into files (eg. what to put in headers, how many)&lt;/li&gt;
&lt;li&gt;code layed out in files (eg. ordering of includes, templates, classes, functions)&lt;/li&gt;
&lt;li&gt;naming of files, classes, functions and variables&lt;/li&gt;
&lt;li&gt;indentation, line width&lt;/li&gt;
&lt;li&gt;comments in code files&lt;/li&gt;
&lt;li&gt;the modular structure of the code itself (eg. design patterns)&lt;/li&gt;
&lt;li&gt;high-level documentation that explains design choices&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Readable code should be relatively easy to explore and read. I say “relatively”, because most programs have a lot of innate complexity that you have to wrap your head around, even if it’s well-written.&lt;/p&gt;
&lt;p&gt;A famous example of high quality and readable (C++) code is the &lt;a href="https://github.com/mtrencseni/quake3"&gt;Quake3&lt;/a&gt; and &lt;a href="https://github.com/mtrencseni/doom3"&gt;Doom3&lt;/a&gt; code by &lt;a href="https://en.wikipedia.org/wiki/John_Carmack"&gt;John Carmack&lt;/a&gt;. When I was working on &lt;a href="https://github.com/scalien/scaliendb"&gt;ScalienDB&lt;/a&gt; many years ago, I tried to write C++ code in a similar style. A good C++ book on the above points is &lt;a href="https://www.amazon.com/Large-Scale-Software-Design-John-Lakos/dp/0201633620"&gt;Large Scale Software Design&lt;/a&gt; by John Lakos.&lt;/p&gt;
&lt;p&gt;Writing good SQL code is much simpler than writing good C++ code. SQL code has no templates, classes, and in my experience analytical “data mining” SQL code also has no functions (=stored procedures). It’s really just a lot of standalone &lt;code&gt;SELECT&lt;/code&gt;s, sometimes chained with &lt;code&gt;WITH&lt;/code&gt;, or by dropping results into tables and &lt;code&gt;SELECT&lt;/code&gt;ing out of those tables in the next step. Having said that, I still see a lot of data people writing hard to read SQL code.&lt;/p&gt;
&lt;p&gt;Without further ado, my rules for writing SQL code follow below.&lt;/p&gt;
&lt;h2&gt;Use all upper-case for keywords like &lt;code&gt;SELECT&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Also use upper-case for built-in types and functions like &lt;code&gt;BIGINT&lt;/code&gt; and &lt;code&gt;COUNT()&lt;/code&gt;.
Use lower-case for column names and table names.&lt;/p&gt;
&lt;h2&gt;Keep &lt;code&gt;SELECT .. FROM .. WHERE .. GROUP BY&lt;/code&gt; unindented&lt;/h2&gt;
&lt;p&gt;But indent the rest. An example so far:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="n"&gt;customer_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;customer_street&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;company_metrics&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="n"&gt;customer_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Marton Trencseni&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Line up &lt;code&gt;WHERE&lt;/code&gt; conditions&lt;/h2&gt;
&lt;p&gt;God is merciful because &lt;code&gt;AND⎵&lt;/code&gt; is 4 characters, a good tab width, so &lt;code&gt;WHERE&lt;/code&gt; conditions are to be lined up like (same for &lt;code&gt;JOIN&lt;/code&gt; conditions):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;deliveries&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
        &lt;span class="n"&gt;country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;UAE&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="k"&gt;day&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2019-07-01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;DAY_OF_WEEK&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
    &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;scheduled_accuracy_meters&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Write &lt;code&gt;GROUP BY 1, 2&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Always put the grouped columns first in the column list, and write &lt;code&gt;GROUP BY 1, 2 .. N&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="n"&gt;region_fleet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;CASE&lt;/span&gt; &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;status&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Delivered&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Delivered&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Not Delivered&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;END&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;DATE_TRUNC&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;week&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;week&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;ROW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;so_number&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_orders&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;DISTINCT&lt;/span&gt; &lt;span class="k"&gt;CASE&lt;/span&gt; &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;scheduled_accuracy_meters&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="k"&gt;ROW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;day&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;so_number&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt; &lt;span class="k"&gt;END&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;num_accurate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;AVG&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scheduled_accuracy_meters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;scheduled_accuracy_meters&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;deliveries&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;GROUP&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt;
    &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Indent &lt;code&gt;WHEN&lt;/code&gt; relative to &lt;code&gt;CASE&lt;/code&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...,&lt;/span&gt;
    &lt;span class="k"&gt;CASE&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Ticker&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Ticker&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;CallCenter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;CSA&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Callcenter&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;CallCenterBlind&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;CSA-BD&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blind&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AutoReschedule&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;AI-AutoReschedul&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Auto Schedule&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;DriverReschedule&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Rest&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;BulkSchedule&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Bulk&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;CallCenterSelfPickupPoint&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;CallCenterSelfPickupWarehouse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Self-pickup&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;AI-AutoSchedulin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;AI_Rango&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;AI-Rango&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;AI&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MWeb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;MWEB&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mobile-ios&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;mobile-android&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Mweb/apps (various)&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Whatsapp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Callcenter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Blind&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Driver (various)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Bulk&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;AI&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt;
        &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Rest&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;END&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;schedule_channel&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;deliveries&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Use &lt;code&gt;WITH&lt;/code&gt; instead of nested sub-&lt;code&gt;SELECT&lt;/code&gt;s&lt;/h2&gt;
&lt;p&gt;Sub-&lt;code&gt;SELECT&lt;/code&gt;s with indenting are hard to read. Instead, create aliases with &lt;code&gt;WITH&lt;/code&gt;, and chain them. Put the &lt;code&gt;WITH&lt;/code&gt; on a separate line, and then write the aliases. If I have no better idea, I call the aliases &lt;code&gt;step1, step2 ...&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For example, suppose table &lt;code&gt;deliveries&lt;/code&gt; has columns &lt;code&gt;scheduled_coordinates&lt;/code&gt; and &lt;code&gt;actual_coordinates&lt;/code&gt; as &lt;code&gt;lat, lon&lt;/code&gt; string, and you want to compute the meter distance with the &lt;a href="https://en.wikipedia.org/wiki/Haversine_formula"&gt;Haversine formula&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;WITH&lt;/span&gt;
&lt;span class="n"&gt;step1&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;SELECT&lt;/span&gt;
        &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;TRIM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SPLIT_PART&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actual_coordinates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;act_lat_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;TRIM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SPLIT_PART&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actual_coordinates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;act_lon_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;TRIM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SPLIT_PART&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scheduled_coordinates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;sch_lat_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;TRIM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SPLIT_PART&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scheduled_coordinates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;sch_lon_str&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt;
        &lt;span class="n"&gt;deliveries&lt;/span&gt;
&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="n"&gt;step2&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;SELECT&lt;/span&gt;
        &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;CASE&lt;/span&gt;
            &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;REGEXP_LIKE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;act_lat_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;^[0-9]+\.[0-9]+$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;act_lat_str&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;DOUBLE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt;
        &lt;span class="k"&gt;END&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;actual_lat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;CASE&lt;/span&gt;
            &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;REGEXP_LIKE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;act_lon_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;^[0-9]+\.[0-9]+$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;act_lon_str&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;DOUBLE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt;
        &lt;span class="k"&gt;END&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;actual_lon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;CASE&lt;/span&gt;
            &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;REGEXP_LIKE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sch_lat_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;^[0-9]+\.[0-9]+$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sch_lat_str&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;DOUBLE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt;
        &lt;span class="k"&gt;END&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;scheduled_lat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="k"&gt;CASE&lt;/span&gt;
            &lt;span class="k"&gt;WHEN&lt;/span&gt; &lt;span class="n"&gt;REGEXP_LIKE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sch_lon_str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;^[0-9]+\.[0-9]+$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;THEN&lt;/span&gt; &lt;span class="k"&gt;CAST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sch_lon_str&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;DOUBLE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;ELSE&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt;
        &lt;span class="k"&gt;END&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;scheduled_lon&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt;
        &lt;span class="n"&gt;step1&lt;/span&gt;
&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="n"&gt;step3&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="k"&gt;SELECT&lt;/span&gt;
        &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="mi"&gt;6371&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ASIN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SQRT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;POW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;SIN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;   &lt;span class="n"&gt;RADIANS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scheduled_lat&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual_lat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="mi"&gt;2&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="n"&gt;COS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RADIANS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;actual_lat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;COS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RADIANS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scheduled_lat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;POW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;SIN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;   &lt;span class="n"&gt;RADIANS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scheduled_lon&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;actual_lon&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="p"&gt;),&lt;/span&gt;
                &lt;span class="mi"&gt;2&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;scheduled_accuracy_meters&lt;/span&gt;
    &lt;span class="k"&gt;FROM&lt;/span&gt;
        &lt;span class="n"&gt;step2&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Long lines are okay&lt;/h2&gt;
&lt;p&gt;As you can see, long lines are okay in my opinion. We all have widescreen aspect ratio screens (Macbooks), let's use that space. Shorter lines &lt;em&gt;are&lt;/em&gt; more readable, but breaking everything into shorter lines, re-breaking when making changes, it's too much work in my opinion, especially when dealing with lenghty "business logic" in SQL. &lt;/p&gt;
&lt;h2&gt;Break the rules for readability and flow of code&lt;/h2&gt;
&lt;p&gt;Rules are made to be broken. If doing it in another way leads to better readability, break the rules.&lt;/p&gt;
&lt;p&gt;For example, sometimes we &lt;code&gt;SELECT&lt;/code&gt; out a horizontal/vertical part of a table in a quick sub-&lt;code&gt;SELECT&lt;/code&gt; to help the query optimizer. In cases like this I don't use &lt;code&gt;WITH&lt;/code&gt; and keep it in one line, like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;FROM&lt;/span&gt;
    &lt;span class="n"&gt;deliveries&lt;/span&gt; &lt;span class="n"&gt;dls&lt;/span&gt;
&lt;span class="k"&gt;LEFT&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;drivers&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;country&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;UAE&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;drv&lt;/span&gt;
&lt;span class="k"&gt;ON&lt;/span&gt;
    &lt;span class="n"&gt;dls&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;driver_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;drv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Follow the same rules when writing interactive SQL code&lt;/h2&gt;
&lt;p&gt;I follow these same rules when writing one-off queries in an interactive console. That way it’s easier to stick to them when writing long-lived code that goes into ETL and the repo. Also, one-off code oftens ends up being ETL’d.&lt;/p&gt;
&lt;h2&gt;Alternative ways to write SQL&lt;/h2&gt;
&lt;p&gt;Finally, some other, more comprehensive guides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.sqlstyle.guide/"&gt;Simon Holywell's SQL style guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://about.gitlab.com/handbook/business-ops/data-team/sql-style-guide/"&gt;Gitlab's SQL style guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mattm/sql-style-guide"&gt;Matt Mazur's SQL style guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="data"></category><category term="programming"></category><category term="sql"></category></entry><entry><title>Metrics Atlas</title><link href="/metrics-atlas.html" rel="alternate"></link><published>2019-08-29T00:00:00+02:00</published><updated>2019-08-29T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2019-08-29:/metrics-atlas.html</id><summary type="html">&lt;p&gt;The idea is simple: write a document which helps new and existing people—both managers and individual contributors—get an objective, metrics-based picture of the business. This is helpful when new people join, when people start working in new segments of the business, and to understand other parts of the company.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/metrics_atlas.png" alt="Metrics atlas" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;The idea is simple: &lt;strong&gt;write a document which helps new and existing people—both managers and individual contributors—get an objective, metrics-based picture of the business.&lt;/strong&gt; This is helpful when new people join, when people start working in new segments of the business, and to understand other parts of the company. &lt;/p&gt;
&lt;p&gt;Companies with a data/analytics team end up with lots of dashboards and reports, plus hundreds of ad-hoc, non-dashboarded analysis that data scientists produce over the years. The dashboards are usually discoverable because they're in one system, but there will be a lot of them. But it's hard mental work to look through 50-100 dashboards and make sense of it all. Many people won't do that. And the ad-hoc reports and presentations are locked up in documents, notebooks, email threads, not discoverable by others, even though many of them contain valuable insights.&lt;/p&gt;
&lt;p&gt;The idea is similar to when an engineering team concludes their system design work by writing &lt;a href="https://arxiv.org/abs/1302.3860"&gt;a document explaining the architecture they chose, why they chose it, trade-offs, lessons learned&lt;/a&gt;. A written document, with lots of charts and links, that somebody can read and get a good picture of the product/business. &lt;strong&gt;This is one, long, stand-alone document.&lt;/strong&gt; Wikis are good, but wikis will have a lot of sub-pages and are better for search-and-find consumption use-cases. With the atlas, the user experience is that the reader will go through the whole thing and will have a good feeling that they now understand the business better. And because it has links, they now know where to look for metrics, dashboards, wikis, or who to ask. It doesn't have to be to-the-day up to date, it's good enough to update it every 3-6 months.&lt;/p&gt;
&lt;p&gt;The first time I had this idea was at Prezi, but we never got around to it. At Facebook I was working on &lt;a href="https://www.facebook.com/workplace"&gt;Workplace&lt;/a&gt;, which at that time was so new and changing so fast that this didn't make sense. At Fetchr, we were able to allocate a few days to this recently, and it turned out very useful!&lt;/p&gt;
&lt;p&gt;For us it's a Google doc, we co-edit it, and share it within the company. &lt;strong&gt;Right now it's about 40 pages, it will probably come in at 50 pages in the end.&lt;/strong&gt; Since we've been working with this data and metrics every day for the last 2 years, writing this was not a big effort.&lt;/p&gt;
&lt;p&gt;This is what the first page looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/metrics_atlas.png" alt="Metrics atlas" style="width: 800px;"/&gt;&lt;/p&gt;</content><category term="data"></category><category term="fetchr"></category></entry><entry><title>5 things that happened in Data Science in 2018</title><link href="/five-things-2018-data-science.html" rel="alternate"></link><published>2019-01-09T00:00:00+01:00</published><updated>2019-01-09T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2019-01-09:/five-things-2018-data-science.html</id><summary type="html">&lt;p&gt;2018 was a hot year for Data Science and AI. Here we picked out 5 highlights, which in our opinion shaped the field in the past year.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/q3deepmind.png" alt="Deepmind playing CTF" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;2018 was a hot year for Data Science and AI. Here we picked out 5 highlights, which in our opinion shaped the field in the past year.&lt;/p&gt;
&lt;h2&gt;OpenAI Five achieves human-level DOTA team gameplay&lt;/h2&gt;
&lt;p&gt;OpenAI has started to achieve professional-level 1v1 playback in 2017, but 2018 was the year when team gameplay became human level. In DOTA, two teams of 5 players (or bots) battle for domination of resources on a map and try to kill each other off. OpenAI uses Reinforcement Learning with an LSTM network and self-play for learning, playing 180 years against itself every day on 256 GPUs and 128,000 CPU cores.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/dota.png" alt="OpenAI Five" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;At a high level, one interesting aspect of OpenAI’s success is that no deep theoretical breakthrough was necessary (so far) to achieve this; the basic ideas of the AI are similar to Google DeepMind’s AlphaGo, which defeated Lee Sedol in 2016. But Go, although a very deep, strategic game, still seemed very “discrete” compared to a Real Time Strategy game like DOTA. It will be interesting to see how far the OpenAI architecture will scale: can it defeat the top professional human team in 2019? I wouldn’t bet against the AI.&lt;/p&gt;
&lt;p&gt;OpenAI resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://blog.openai.com/openai-five/"&gt;OpenAI Five&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gym.openai.com/"&gt;OpenAI Gym&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Human-level performance in Quake 3 Arena&lt;/h2&gt;
&lt;p&gt;Staying with games, 2018 was also the year when DeepMind achieved human-level performance in Quake 3, using only pixels and game points as input. Although this result received less coverage than the earlier AlphaGo and the OpenAI successes, for people who grew up playing Doom and Quake, this is perhaps even cooler.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/q3deepmind.png" alt="Deepmind playing CTF" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;Like the other game playing bots, this one is also based on Reinforcement Learning, but here thousands of bots are “alive” at any given time, playing in teams against themselves in randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning and selects actions using a hierarchical representation that enables the agent to reason at multiple timescales. The videos show that the bots display human-like behaviors such as navigating, following, and defending:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1807.01281"&gt;DeepMind paper: Human-level performance in first-person multiplayer games with population-based deep reinforcement learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=dltN4MxV1RI"&gt;Video supplement to the paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=MvFABFWPBrw&amp;amp;t=96s"&gt;Video explanation of the DeepMind Quake3 architecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Self-driving cars did not arrive in 2018&lt;/h2&gt;
&lt;p&gt;Self-driving cars are the “killer-app” of the current AI hype cycle, and many people look at it as an indicator tent pole. Everybody understand this application, most people's life would be affected, and it’s hard to argue with the cool-factor. The stakes are high, many big players are working on this disruptive advancement: who will supply the brain of these self-driving cars? who will sell the most self-driving cars? which self-driving car service will dominate? and equally importantly, what will drivers who no longer have to drive do? whose content will they consume?&lt;/p&gt;
&lt;p&gt;Progress is steady, with Tesla pushing out new versions over the air, and other major car manufacturing also experimenting with self-driving features, but it seems Google’s Waymo is the most far ahead in terms of commercial deployment: in 2018 Waymo launched its first commercial self-driving car service called "Waymo One", where users in the Phoenix metropolitan area can use an app to request the service.&lt;/p&gt;
&lt;p&gt;But in 2018, we still were not able to get in a self-driving car, put our kids in the back, and get from San Francisco to Los Angeles without touching the wheel.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/waymo.jpg" alt="Deepmind playing CTF" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;Instead, we had to cope with sad news. There were fatal crashes related to self-driving technology of Uber and Tesla, which triggered widespread debate about how the technology should be marketed, tested and deployed.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.theverge.com/2018/6/22/17492320/safety-driver-self-driving-uber-crash-hulu-police-report"&gt;Uber crash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.wired.com/story/tesla-autopilot-self-driving-crash-california/"&gt;Tesla crash&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PyTorch reached 1.0&lt;/h2&gt;
&lt;p&gt;Data Scientists and ML Researchers have a wide variety of tools and frameworks to chose from, with Tensorflow, Keras and PyTorch being the most widely used Deep Learning ones. PyTorch stands out to us because it is a “hacker’s framework”. Tensorflow came out of Google, where most ML jobs operate on vast amounts of data, which lead to design decisions to trade ease of use and debuggability for raw performance.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/pytorch.png" alt="PyTorch logo" style="width: 500px;"/&gt;&lt;/p&gt;
&lt;p&gt;For smaller scale, more lean problems and teams, we believe PyTorch is a better fit. PyTorch is easier to work with than Tensorflow, the execution model is nicely integrated with Python, for example, step-by-step &lt;code&gt;print()&lt;/code&gt;s work---at the expense of some performance. But unless you’re working at Google scale, the speed of iterating on your model and code should come before the speed of training. To get an introduction to PyTorch, check out the excellent tutorial lecture by Stefan Otte:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=_H3aw6wkCv0"&gt;Deep Neural Networks with PyTorch - Stefan Otte&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Reinforce AI Conference announced!&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://reinforceconf.com/"&gt;Reinforce Conf&lt;/a&gt; gives product managers, data scientists and engineers insights into success stories, lessons learned, best practices and new approaches, &lt;strong&gt;in the beautiful European city of Budapest, on March 20-22, 2019&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/reinforce.png" alt="Reinforce" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;Reinforce brings together a diverse group of leading experts and practitioners to share their knowledge and experience on the fast-moving field of Machine Learning and Artificial Intelligence. Some of the speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Andrea Pasqua, Data Science Manager, Uber&lt;/li&gt;
&lt;li&gt;Christian Szegedy, Staff Research Scientist, Google&lt;/li&gt;
&lt;li&gt;Cibele Montez Halasz, Machine Learning Engineer, Twitter&lt;/li&gt;
&lt;li&gt;Kush R. Varshney, Principal Research Staff Member, and Manager, IBM Watson&lt;/li&gt;
&lt;li&gt;Patrick van der Smagt, Director Artificial Intelligence, Volkswagen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Use promo code &lt;code&gt;KDNuggets&lt;/code&gt; to get a 20% discount.&lt;/p&gt;</content><category term="data"></category><category term="openai"></category><category term="waymo"></category><category term="deepmind"></category><category term="tesla"></category><category term="reinforce"></category></entry><entry><title>Warehouse locations with k-means</title><link href="/warehouse-locations-with-kmeans.html" rel="alternate"></link><published>2018-09-26T00:00:00+02:00</published><updated>2018-09-26T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-09-26:/warehouse-locations-with-kmeans.html</id><summary type="html">&lt;p&gt;Sometimes, the seven gods of data science, Pascal, Gauss, Bayes, Poisson, Markov, Shannon  and Fisher, all wake up in a good mood, and things just work out. Recently we had such an occurence at Fetchr, when the Operational Excellence team posed the following question: &lt;i&gt;if we could pick our Saudi warehouse locations, where would be put them? What is the ideal number of warehouses, and, what does ideal even mean? Also, what should our “delivery radius” be?&lt;/i&gt;&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/pickup_locations_riyadh.png" alt="" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sometimes, the seven gods of data science, &lt;a href="https://en.wikipedia.org/wiki/Blaise_Pascal"&gt;Pascal&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss"&gt;Gauss&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Thomas_Bayes"&gt;Bayes&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Sim%C3%A9on_Denis_Poisson"&gt;Poisson&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Andrey_Markov"&gt;Markov&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Claude_Shannon"&gt;Shannon&lt;/a&gt;  and &lt;a href="https://en.wikipedia.org/wiki/Ronald_Fisher"&gt;Fisher&lt;/a&gt;, all wake up in a good mood, and things just work out. Recently we had such an occurence at Fetchr, when the Operational Excellence team posed the following question: &lt;i&gt;if we could pick our KSA (=Kingdom of Saudi Arabia) warehouse locations, where would be put them? What is the ideal number of warehouses, and, what does ideal even mean? Also, what should our “delivery radius” be?&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;For those of us ignorant of Middle East geography, some facts about KSA:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the biggest country in the Middle East&lt;/li&gt;
&lt;li&gt;mostly desert&lt;/li&gt;
&lt;li&gt;not very far from global conflict locations such as Iraq, Syria, Lebanon&lt;/li&gt;
&lt;li&gt;about 6x as big as Germany (a “big” European country), with 0.4x of the population&lt;/li&gt;
&lt;li&gt;about 25x as big as UAE (where Dubai, Fetchr’s HQ is)&lt;/li&gt;
&lt;li&gt;about 24x as big as Hungary (my home)&lt;/li&gt;
&lt;li&gt;responsible for 13% of the world’s oil production&lt;/li&gt;
&lt;li&gt;e-commerce is exploding, lots of people are ordering stuff online&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/ksa_map.png" alt="KSA map" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Note: I will describe this fun little project as best as I can without giving away sensitive information. In some parts I will use synthetic data and in cases where the information is public/discoverable ("Fetchr has a warehouse in Riyadh"), I will just show the real thing.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;Metrics and k-means&lt;/h2&gt;
&lt;p&gt;The first question is, what is “good” here? What are “good” warehouse locations? We need to find a metric to minimize/maximize. This is pretty straightforward: we need to find warehouse locations, and for each order, we assign it to the nearest warehouse (we assume it would dispatch from there), and we calculate the distance. The goal is then to find warehouse locations which minimizes the average distance across all orders.&lt;/p&gt;
&lt;p&gt;We can write this out as a (naive) algorithm:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/avg_distance.png" alt="Code for avg_distance()" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Given this choice of metric, we can evaluate a set of warehouse locations, and compare it to another.&lt;/p&gt;
&lt;p&gt;The next question is, how do we actually pick the warehouse locations? Putting aside the question of how many warehouse locations we should have, assuming we know we want N locations, there is an algorithm just for this: &lt;a href="https://en.wikipedia.org/wiki/K-means_clustering"&gt;k-means clustering&lt;/a&gt;. You give k-means a set of points and a parameter N, and it returns the best N cluster centroid locations which minimizes the average distance to the nearest centroid. (This problem is actually NP-hard, so k-means returns an approximation of the solution.) At Fetchr we are standardized on Python and &lt;a href="http://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt;, so we used SKL’s excellent k-means implementation; it’s just 2 lines of code:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/skl_kmeans.png" alt="k-means in scikit-learn" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;The next question is, how do we pick N, the number of warehouses?&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Note: There are other clustering algorithms such as &lt;a href="https://en.wikipedia.org/wiki/DBSCAN"&gt;DBSCAN&lt;/a&gt; that do not depend on N as an input, and tell you the best N as an output. But there is no free lunch, so even with DBSCAN you have to specifiy an epsilon parameter as an input, and it uses that epsilon to tell apart “core” and “noise” points, with clusters being dense “core” regions surrounded by sparse “noise” regions. So DBSCAN is also not parameter-free.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;To keep things parameter-free and &lt;a href="https://en.wikipedia.org/wiki/KISS_principle"&gt;simple&lt;/a&gt; to interpret, we  ran k-means from &lt;code&gt;N = 1 .. N_max&lt;/code&gt; for a large &lt;code&gt;N_max&lt;/code&gt;, and plot the average distance achieved for each N, then read off the “best value” in some sense. Later we will see that being able to vary N is actually a good thing, because we can get easy-to-interpret insights from it. So k-means it is. With this, the basic approach of our analysis is, in pseudo-code:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/kmeans_skeleton.png" alt="" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;We can also compute the average distance with out actual, current KSA warehouse locations, and see how good a job the Operations team did picking them out. Running this analysis with different N, also showing actual warehouse locations, this is what the average distance curve might look like:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/wh_dist_1.png" alt="" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This (the real version of this) shows us:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How much average distance we could gain by keeping the number of hubs constant, but if we had optimal locations (move down vertically from blue cross to red line).&lt;/li&gt;
&lt;li&gt;How many warehouses we could close and remain at the same average distance (move left from the blue cross to the red line).&lt;/li&gt;
&lt;li&gt;Our Operations team did a good job, our current warehouse locations are pretty good.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thinking it through, it’s obvious that the average distance will not converge to some non-zero value. It will go to zero as N (the number of warehouse locations) approaches infinity (the number of input points), since in theory we can achieve zero average distance by placing a warehouse next to each delivery location. The next best thing to check is the “derivative” of this line, ie. how much distance we “gain” (well, lose), if we add +1 warehouse. From this we will see at which N we get the biggest gains, and where the gains level off.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/wh_dist_2.png" alt="" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Here we can see that initially we gain a lot, then at some N our gains even out (but are non-zero). Then, a higher N the gains drop further. These are interesting points to investigate further.&lt;/p&gt;
&lt;p&gt;With these same tools we can also investigate the question of “delivery radius”. Delivery companies often have no-service zones where they don’t accept packages, because they can’t efficiently (=profitably) service these areas. Or these areas are serviced, but only once or twice a week. To get a feel for this, we took our actual warehouse locations and put a circle of radius R kilometers around them. We investigaged as a function of the R cut-off:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What percentage of orders lie outside of R?&lt;/li&gt;
&lt;li&gt;If we cut these outliers and re-run k-means, how much average distance do we gain?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For us, it turns out we can make a strong argument: there is a given R for which we only cut off a very low percent of orders, but we gain a lot of average distance!&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/wh_dist_3.png" alt="" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This also shows that by performing both optimizations (distance cut-off and k-means) we can actually gain a lot of efficiency; moving from the blue cross to the green line is significant. For more on outliers and outlier detection, see my previous post &lt;a href="http://bytepawn.com/beat-the-averages.html#beat-the-averages"&gt;Beat the averages&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Locations&lt;/h2&gt;
&lt;p&gt;So far our analysis has been very quantitative in that we looked at plots and curves. We can also look at the actual recommended locations (latitude, longitude) on a map. As a starting point, we look at the k-means recommended warehouse locations when running at &lt;code&gt;N = N_actual&lt;/code&gt;, here we expect the recommended locations to resemble our actuals; this is a sanity check on k-means. And it actually works out! For example, k-means correctly places warehouses in the center of the biggest KSA cities (eg. Riyadh, Jeddah, etc, where all medium-to-large delivery companies like Fetchr must have warehouses).&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/wh_locations_1.png" alt="" style="width: 650px;"/&gt;
&lt;center&gt;Red is actual, yellow is k-means at &lt;code&gt;N = N_actual&lt;/code&gt;.&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The interesting thing happens as we start to decrease N; essentially k-means starts finding more optimal configurations and/or recommends to merge warehouses:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/wh_locations_2.png" alt="" style="width: 650px;"/&gt;
&lt;center&gt;Red is actual, yellow is k-means at &lt;code&gt;N &amp;lt; N_actual&lt;/code&gt;.&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;As we decrease N (the number of warehouses), it essentially makes recommendations, like &lt;i&gt;“if you want to decrease the warehouse locations, join these two, and try to place the new one in the middle”.&lt;/i&gt; In real life this may or may not be feasible, because the middle may be just desert.&lt;/p&gt;
&lt;h2&gt;Conclusion and impact&lt;/h2&gt;
&lt;p&gt;This analysis was not a prescriptive (&lt;i&gt;“rent N warehouses here”&lt;/i&gt;), it’s a discussion starter for our operations and business teams. For this reason, we can get away with using as-the-crow-flies distances instead of proper road routed distances. But still, based on this, we were able to make a good N recommendation for number of warehouses (less than actual) and locations, but we also learned that our actual warehouse locations are not too bad. We also investigated delivery radius and found that if we cut orders at a certain R distance away from a warehouse, we only cut a few % of orders, but average distance drops significantly.&lt;/p&gt;
&lt;p&gt;As a bonus, a few weeks later the seven gods of data science smiled on us again. It turns out there is a very similar logistics problem to picking warehouse locations: pickup locations, where customers can go and self-pickup their packages. This time, the question was: &lt;i&gt;“What would the best pickup locations be in Riyadh? How many should we even have?”.&lt;/i&gt; We were able to re-use the same analytical framework, only this time running it on just Riyadh city data. The analysis says: &lt;i&gt;"put the first 5 pickup locations into central Riyadh, and the next one into Al Muzahimiyah, the next one into... and so on."&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/pickup_locations_riyadh.png" alt="" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;It’s interesting that all examples on the k-means Wikipedia page are cases where the distance metric is synthetic (vector quantization, image recognition, NLP). In this beautiful logistics use-case the distance is physical Euclidian, the real thing. It’s amazing how an old-school algorithm like k-means can deliver so much impact in unexpected places!&lt;/p&gt;</content><category term="data"></category><category term="data-science"></category><category term="metrics"></category><category term="fetchr"></category></entry><entry><title>Growth Accounting and Backtraced Growth Accounting</title><link href="/backtraced-growth-accounting.html" rel="alternate"></link><published>2018-09-16T00:00:00+02:00</published><updated>2018-09-16T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-09-16:/backtraced-growth-accounting.html</id><summary type="html">&lt;p&gt;Previously I wrote two articles about data infra and data engineering at Fetchr. This time I want to move up the stack and talk about a simple piece of metrics engineering that proved to be very impactful: Growth Accounting and Backtraced Growth Accounting.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/ga_read_off.png" alt="Backtraced Growth Accounting" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Previously I wrote two articles about &lt;a href="http://bytepawn.com/fetchr-data-science-infra.html#fetchr-data-science-infra"&gt;data infra and data engineering at Fetchr&lt;/a&gt;. This time I want to move up the stack and talk about a simple piece of metrics engineering that proved to be very impactful: &lt;a href="https://www.facebook.com/FacebookforDevelopers/videos/growth-accounting-triangle-heatmap-explanation/3707283286197/"&gt;Growth Accounting&lt;/a&gt; and Backtraced Growth Accounting.&lt;/p&gt;
&lt;h2&gt;Standard Growth Accounting&lt;/h2&gt;
&lt;p&gt;Let’s start with Growth Accounting, a standard framework for understanding user lifecycles and churn in the SaaS world. I’ve implemented and ran Growth Accounting in previous jobs, but really grasped the importance (and how to do it properly) during my time at Facebook.&lt;/p&gt;
&lt;p&gt;The basic framework of Growth Accounting is to assign a state to every object, every day. The possible states are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;New&lt;/li&gt;
&lt;li&gt;Retained&lt;/li&gt;
&lt;li&gt;Churned&lt;/li&gt;
&lt;li&gt;Stale&lt;/li&gt;
&lt;li&gt;Resurrected&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, what is an ‘object’? An object can be anything, like a FB user, a FB group, a Prezi presentation or a Fetchr client company. The second part of Growth Accounting is determining which actions (or lack of them) lead to state transitions. For a user, actions are usually any activity, for a group is any activity in the group, for a Prezi it may be views. The third part of Growth Accounting is fixing the time horizon; this is usually either 1 day (Daily Growth Accounting), 7 days (Weekly) or 28 days (Monthly).&lt;/p&gt;
&lt;p&gt;So, before Growth Accounting, we need to decide the:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;object&lt;/li&gt;
&lt;li&gt;actions&lt;/li&gt;
&lt;li&gt;time horizon&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The most common use-case for Growth Accounting is users, on a monthly basis, so I’ll stick to that example in the first part:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;object = users&lt;/li&gt;
&lt;li&gt;actions = any activity by user&lt;/li&gt;
&lt;li&gt;time horizon = 28 days&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Growth Accounting then says:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a user is a Monthly Active User (MAU) if they had any activity in the last 28 days&lt;/li&gt;
&lt;li&gt;run the below state transitions daily to get the user’s Monthly Growth Accounting state:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/ga_state_transitions.png" alt="Growth Accounting State Transitions" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The same state transition table works for Daily Growth Accounting by replacing MAU with DAU (Daily Active User), and Weekly Growth Accounting by replacing MAU with WAU (Weekly Active User).&lt;/p&gt;
&lt;p&gt;Some notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;users are in the New state for only one day (the first), irrespective of whether Growth Accounting is Daily/Weekly/Monthly&lt;/li&gt;
&lt;li&gt;users are in the Churned state for only one day, irrespective of whether Growth Accounting is Daily/Weekly/Monthly&lt;/li&gt;
&lt;li&gt;users are in the Resurrected state for only one day (the first active after a period of inactivity), irrespective of whether Growth Accounting is Daily/Weekly/Monthly&lt;/li&gt;
&lt;li&gt;inactive users end up in the Stale state (when speaking, we tend to say ‘churned users’, but stale is a more accurate term)&lt;/li&gt;
&lt;li&gt;continuosly active users end up in the Retained state&lt;/li&gt;
&lt;li&gt;the two states where users bulk up are Retained and Stale&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The point of doing Growth Accounting is to count every day the number of New, Retained, etc. users (plus Net New, see below) and put the counts on a time series chart (perhaps a stacked one), and look at it every day. What this usually shows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;every product churns out users, so Churned will be non-zero&lt;/li&gt;
&lt;li&gt;almost every product churns out a lot of users, so Stale will monotonically increase&lt;/li&gt;
&lt;li&gt;if Churn increases, that’s potentially bad, or the product is just growing&lt;/li&gt;
&lt;li&gt;if the product is healthy, Retained should increase&lt;/li&gt;
&lt;li&gt;The Growth Accounting equation is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Net New = New + Resurrected - Churned&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if Net New &amp;gt; 0, that’s good, the MAU count (or DAU or WAU) is increasing&lt;/li&gt;
&lt;li&gt;if Net New &amp;lt; 0, that’s bad, the MAU count is decreasing, the product is slowly “dying”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some comments about the Monthly, Weekly, Daily bit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When doing Monthly Growth Account, 1 day of activity is enough to make the user MAU for 28 subsequent days (first they will be New or Resurrected, then Retained for 27 days, then Churned for one day and then Stale, assuming they’re not active again):
&lt;code&gt;New/Resurrected (1st day) → Retained (2..28) → Churned (29) → Stale (30th day)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;When doing Daily Growth Accounting, 1 day of activity will make the user DAU for one day, the state transitions will be quick:
&lt;code&gt;New/Resurrected  (1st day) → Churned (2) → Stale (3rd day)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Usually it makes sense to run all three (D/M/W) and keep tabs on all of them. DAU and WAU respond quicker to problems (with MAU, if there’s a problem, people won’t enter Churned for 28 days after the last activity), but are also more volatile.&lt;/p&gt;
&lt;p&gt;As discussed in the introduction Growth Accounting as a framework can be separated from the object and activity definition, and can be run for different time periods. So it makes sense to write it as a modular script that can be reused by plugging in whatever object/action definition. Thinking about tables, Growth Accounting needs as an input just 2 columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;day: a date&lt;/li&gt;
&lt;li&gt;id: the id of the object&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A row like (day, id) simply means that object with id was active on that day. That’s it. Given this table, a starting date, and a choice of D/W/M, a Growth Accounting framework can generate the output:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;day&lt;/li&gt;
&lt;li&gt;id&lt;/li&gt;
&lt;li&gt;state: the growth accounting state of the object with id on day&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We use Airflow, our Growth Accounting framework is baked into a function &lt;code&gt;dag_growth_accounting()&lt;/code&gt;, and it’s parameterized like:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ga_client.png" alt="B2C Client Growth Accounting" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This is the definition of our B2C Client Growth Accounting pipeline. The resulting table is &lt;code&gt;client_growth_accounting_28d&lt;/code&gt;, the first day for which states are generated is &lt;code&gt;2017-07-01&lt;/code&gt; (on that day every object with activity is New), it’s Monthly Growth Accounting, and activity is when we make a delivery for a client. Day is not an explicit column here, the &lt;code&gt;events_table&lt;/code&gt; is used as a sub-select in the script for every day when Growth Accounting runs.&lt;/p&gt;
&lt;p&gt;There is one additional feature here, there is an additional &lt;code&gt;country&lt;/code&gt; column. This makes it possible to filter the charts for country, so we can see per-country counts. This is just a property that is attached to the objects.&lt;/p&gt;
&lt;p&gt;The code for the framework function &lt;code&gt;dag_growth_accounting()&lt;/code&gt; which creates the Airflow DAG is below:&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/8c93349f12aa48478588a2ecd91d4c1b.js"&gt;&lt;/script&gt;

&lt;p&gt;Note that this also calculates the L-number for each object: the number of active days in the last X days, where X is 1, 7, 28 depending on whether we're doing D/W/M Growth Accounting.&lt;/p&gt;
&lt;h2&gt;Backtraced Growth Accounting&lt;/h2&gt;
&lt;p&gt;The challenge we faced at Fetchr is that standard Growth Accounting treats every object the same. One user equals another user, one churned user equals another churned user, and so on. There is no concept of a “more valuable user”.&lt;/p&gt;
&lt;p&gt;But when doing Growth Accounting for our B2C client companies, this is not true. If Client X gives us 5,000 deliveries per day, and Client Y gives us 10 deliveries per day, then if Client X churns, that’s a big deal, but Client Y is less important than Client X.&lt;/p&gt;
&lt;p&gt;There are various ways to go about this. One possibility is to use the standard Growth Accounting count charts, but weigh each object by their “activity weight”. For example, in our case, we could weigh each client by the number of deliveries they gave us in the last 28 days. But then how do we weigh them once they churn and then become stale? In the end we did not go down this route.&lt;/p&gt;
&lt;p&gt;We chose a relatively simple approach, which works well for us:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Picture #1: Show Standard Growth Accounting charts to show how clients behave, with each client being weighted equally&lt;/li&gt;
&lt;li&gt;Picture #2: Also show our historic B2C deliveries, with each delivery (from Client X) colored according to Client X’s current (today’s) Growth Accounting state, historically, all the way back. This “backtracing” is what gives the name of Backtraced Growth Accounting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/ga_picture1.png" alt="Standard Growth Accounting" style="width: 650px;"/&gt;
&lt;center&gt;Standard Growth Accounting for clients on synthetic data (Picture #1).&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ga_picture_2.png" alt="Backtraced Growth Accounting" style="width: 650px;"/&gt;
&lt;center&gt;The order-wise view on synthetic data (Picture #2).&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ga_picture_2_pct.png" alt="Backtraced Growth Accounting" style="width: 650px;"/&gt;
&lt;center&gt;The same view, percentage-wise split (Picture #2). This is what I call a sciccor chart.&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ga_read_off.png" alt="Backtraced Growth Accounting" style="width: 650px;"/&gt;
&lt;center&gt;We can read off what our order-wise “retention” is from a year ago on this synthetic dataset.&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt;How to read the “scissor chart” above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Today, all deliveries are Retained ( if we made a delivery in the last 28 days for a client, that client is Monthly Active, so they will be Retained or Resurrected)&lt;/li&gt;
&lt;li&gt;As we go back in time, more and more deliveries are “colored” Stale, because they are for clients that don't use us anymore (no delivery in the last 28 days), so they are Stale&lt;/li&gt;
&lt;li&gt;Going back in time it's easy to read off what % of deliveries were for companies that have already churned out&lt;/li&gt;
&lt;li&gt;Whales churning/retaining is easy to read off: going backward in time, at the point where a whale churned there will be a jump in Stale deliveries (when they stopped using us)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In terms of code, to perform the backtrace (the sciccor view) is trivial once we have Growth Accounting:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/backtrace_view.png" alt="Backtraced Growth Accounting" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The big risk of this Picture #2 is that it hides onboarding problems: if a lot of clients come on, try us out by giving us low volume and then leave, it will not dramatically impact such a weighted view. That's not a problem though, that's why we also look Standard Growth Accounting (Picture #1).&lt;/p&gt;
&lt;p&gt;Backtraced Growth Accounting like this also makes sense for more common SaaS use-cases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Color (state ↔ color) historic pageviews, message sends, document creates, time spent by the current GA state of the user&lt;/li&gt;
&lt;li&gt;Color views of a document by the current GA state of the document (eg. a Prezi)&lt;/li&gt;
&lt;li&gt;Color posts in a group by the current GA state of the group (eg. a FB group)&lt;/li&gt;
&lt;li&gt;Color deliveries made by a driver by whether the driver is still active with the company (eg. at Fetchr or Uber)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Backtraced Growth Accounting picture has become a standard part of our thinking when it comes to client retention, but as the examples above show it can also be useful in other product and business settings. Implementing a framework is simple and fun, it can be deployed repeatedly (we run it for both B2C and C2C use-cases), it’s a relatively easy way to get insight and have top-level impact.&lt;/p&gt;
&lt;p&gt;At Fetchr, implementing Growth Accounting and Backtraced Growth Accounting had a major impact on our B2C thinking. Before this, other teams ran one-off analysis with arbitrary cuts on (if client X gives us Y orders, but next month they give us 0.6 * Y, then...), which could be tuned to give any desired answer. Standardized Growth Accounting as described in this article gives a clear, parameter-free picture of both our client-wise and order-wise retention. For us, it showed the business is healthy and growing. The Growth Accouting charts today are on CxOs dashboards and are also presented to investors to explain our B2C business.&lt;/p&gt;</content><category term="data"></category><category term="data-science"></category><category term="metrics"></category><category term="growth-accounting"></category><category term="fetchr"></category></entry><entry><title>Fetchr Data Science Infra at 1 year</title><link href="/fetchr-data-science-infra.html" rel="alternate"></link><published>2018-08-14T00:00:00+02:00</published><updated>2018-08-14T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-08-14:/fetchr-data-science-infra.html</id><summary type="html">&lt;p&gt;A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/fetchr-data-science-infra-update.png" alt="Fetchr Data Science Infra" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a quick follow-up to my &lt;a href="http://bytepawn.com/fetchr-airflow.html#fetchr-airflow"&gt;previous post describing the Fetchr Data Science infra and philosophy&lt;/a&gt;. The platform has doubled in the last 6 months, and I'm currently approaching the end of my first year at Fetchr, so it's a good time to post an update.&lt;/p&gt;
&lt;p&gt;The basic principles behind our infrastructure have not changed, but we have scaled it out horizontally in key areas. We have also added a small ML prediction cluster, which is already in production and having a big impact on on-the-ground operations. As of today, the Data Science infra is about 20 nodes and looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/fetchr-data-science-infra-update.png" alt="Fetchr Data Science Infra" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;Architecture:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;S3 buckets (all data lives here)&lt;/li&gt;
&lt;li&gt;two Presto clusters&lt;ul&gt;
&lt;li&gt;5 node Presto cluster for ETL and dashboards&lt;/li&gt;
&lt;li&gt;5 node Presto cluster for analytics queries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Airflow&lt;ul&gt;
&lt;li&gt;1 node for scheduler + webserver&lt;/li&gt;
&lt;li&gt;1 node for workers&lt;/li&gt;
&lt;li&gt;1 node for staging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Superset&lt;ul&gt;
&lt;li&gt;1 node for dashboarding&lt;/li&gt;
&lt;li&gt;1 node for analytics queries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Jupyter host (Machine Learning notebooks)&lt;/li&gt;
&lt;li&gt;2 node ML prediction cluster (blue+green)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;S3&lt;/h2&gt;
&lt;p&gt;As before, all data lives on S3, whether it's data imported from our production databases or data produced by the ETL. Data imported is stored in flat CSV files, whereas DWH tables produced by Airflow running Presto jobs are stored in ORC format (like at Facebook). EMR/EC2 nodes never store data.&lt;/p&gt;
&lt;p&gt;We continue to use the &lt;code&gt;ds&lt;/code&gt; partitioned DWH architecture. This means that every night the ETL imports a fresh copy of all production tables into a new ds partition, like &lt;code&gt;ds=2018-08-01&lt;/code&gt;, and all subsequent tables are also re-created in a new partition. Because all tables are backed on S3, this is also mirrored in our S3 path hierarchy. For example, the backing files for our main &lt;code&gt;company_metrics&lt;/code&gt; table are divided like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/company_metrics_s3.png" alt="A table on S3." style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;When querying, we always specify the &lt;code&gt;ds&lt;/code&gt; like &lt;code&gt;WHERE ds='2018-08-13'&lt;/code&gt;, otherwise we're looking at multiple copies of the table. This is cumbersome, so for most of our tables, the Airflow jobs create a &lt;code&gt;xyz_latest&lt;/code&gt; view that just points to the latest partition, like &lt;code&gt;CREATE OR REPLACE VIEW xyz_latest AS SELECT * FROM xyz WHERE ds = '{{ ds }}'&lt;/code&gt;. This way analysts can usually just query the &lt;code&gt;_latest&lt;/code&gt; and not think about it.&lt;/p&gt;
&lt;p&gt;There are many upsides to this scheme: (i) if today's ETL fails, people can continue working with yesterday's data and (ii) since partitions are write-once, data never changes, so it's always easy to reproduce a number sent out 6 months ago (just run the query on that &lt;code&gt;ds&lt;/code&gt; partition, like &lt;code&gt;WHERE ds='2018-02-08'&lt;/code&gt;). The downside is that it's a lot of duplicate data, but with S3 being super-cheap this is a non-issue.&lt;/p&gt;
&lt;p&gt;Okay, but still, this is wasteful and slow in terms of ETL time? After all, we just imported all these tables last night, do we need to re-import the whole dataset again? As the company and our data volume grew, the nightly import actually started taking too long, so we were forced to optimize this: for our big tables, now we import the historic tail once a week on weekends, and on a daily basis we only import data on orders that we received in the last ~3 months. This ensures our ETL finishes on time every night.&lt;/p&gt;
&lt;h2&gt;Presto&lt;/h2&gt;
&lt;p&gt;We currently run two EMR+Presto clusters, each 5 nodes. As before we don't run any ETL or queries on Hive/MapReduce, we exclusively use Presto for compute, since our queries never touch more than 10-100M rows.&lt;/p&gt;
&lt;p&gt;We introduced a secondary cluster for ad-hoc analytics queries because, in cases when our ETL is slow and running during the day, or our regular hourly ETLs are running during the day, it kept blocking us from getting our work done.&lt;/p&gt;
&lt;p&gt;Since all our data lives in S3, having two clusters see (read and write) the same data is not very hard. All we had to do is make sure our schemas are in sync on the two clusters. Since 99% of our schema operations are managed through Airflow jobs (&lt;code&gt;CREATE TABLE IF NOT EXISTS ...&lt;/code&gt;), we just had to modify our Airflow framework to also execute these on the secondary cluster. Additionally, when we manually make changes to an existing table (&lt;code&gt;ALTER TABLE ...&lt;/code&gt;), we have to execute this on both clusters, which is an inconvenience, but a minor one, and quite managable at this scale.&lt;/p&gt;
&lt;h2&gt;Airflow&lt;/h2&gt;
&lt;p&gt;We continue to use Airflow to be the backbone of our data pipelines with great success. We have two nodes for production: one running the scheduler and webserver, and one running the worker processes. Since these nodes don't do any compute themselves (they just launch Presto &lt;code&gt;INSERT INTO ... SELECT&lt;/code&gt; jobs), we did not need to scale out here so far, nor do we expect this to happen in the next year.&lt;/p&gt;
&lt;p&gt;One the other hand, we have deepened our investment into Airflow as our standard ETL system wrt code. We have identified the 4-5 common Airflow use-cases we have (import from Postgres to S3, run an ETL job on Presto, export data to the BI team's Redshift DWH, create dashbord screenshots and send in email, run Growth Accounting) and we have created helper functions to encapsulate them. As a result, the vast majority of our Airflow DAGs don't create Airflow operators directly, instead they call these library functions, which construct and return the DAGs. As a result our Airflow jobs look very clean, with all of the messy complexity hidden away:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/airflow_code_example.png" alt="A table on S3." style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This had a big pay-off when we introduced our secondary Presto cluster a few months ago, and we needed to automtically create all our schemas there. We just added extra operators to our library functions to run schema operations on the secondary, and the next night when the ETL ran, all our table schemas were created on the secondary Presto cluster, pointing to the backing S3 files, ready to go. We were running analytics queries on the secondary cluster the day after we spun it up!&lt;/p&gt;
&lt;p&gt;Currently we have 76 DAGs in Airflow, importing and exporting from 5-10 data sources (3 production databases, S3, 2 Presto clusters, Redshift, DynamoDB, various custom extracts sent to clients and the ML cluster).&lt;/p&gt;
&lt;h2&gt;Superset&lt;/h2&gt;
&lt;p&gt;Superset is both a dashboarding system and has an SQL IDE (called SQL Lab), which our Data Scientists use as their primary tool for accessing data. We continue to use Superset for dashboarding and have spun up a secondary Superset instance just for queries.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/sql-lab.png" alt="Fetchr Data Science Infra" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;We have hit a limitation with Superset, where Superset will launch a new gunicorn process for each page request, and if the page happens to be a dashboard, for each chart on the dashboard. The chart processes will launch a Presto query each, which could take 10-60 seconds to return. Some of our dashboards have a lot of charts on them (20+), plus we have many concurrent users accessing dashboards (or running queries). In cases like this, Superset runs out of worker processes, and it becomes totally unresponsive. Each worker process eats up 1-2G of RAM, so the number of processes it can run are limited.&lt;/p&gt;
&lt;p&gt;As an initial workaround, we split the dashboarding and querying use-case into two Superset instances, so analysts are not blocked by the dashboards. Additionally, we broke large dashboards into smaller pieces (which is unfortunate). This way, running 32 worker processes each, both instances are good for every-day work at current loads.&lt;/p&gt;
&lt;p&gt;Both Airflow and Superset are still rough around the edges, but since Superset is user-facing, it can create more problems. It still happens that we want to look at a dashboard but the webserver times out because Superset ran out worker processes (maybe because the Presto queries are slow, because they're running on the same cluster as the ETL, and the ETL is slow, because something changed in production). Right now we get by with work-arounds (for example, in the previous case, we re-direct the Superset dashboarding traffic to the analytics cluster temporarily by changing a connection string, until ETL finishes). So far Superset is good enough for internal Data Science / Understand dashboards, and we do have a fair number of colleagues using it on a daily basis for basic reporting. But admittedly we will need to invest more time into understanding how to tune if we want to deploy it to a company-wide 1000+ person audience and feel good about it.&lt;/p&gt;
&lt;p&gt;Currently we have 26 dashboards in Superset, many of them viewed by CxOs and country General Managers every day.&lt;/p&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;In the last 3 months we have rolled out a prediction model in production. We perform the Data Science work to arrive at the models on our laptops and/or on a dedicated Jupyter host, in ipython notebooks. Once we're happy with the result, we deploy the model to our ML prediction cluster. We don't yet have CI/CD set up for it, deployment is manual and requires domain knowledge.&lt;/p&gt;
&lt;p&gt;The model is already running in production and is being used for on-the-ground delivery operations, so downtime is not acceptable. We quickly arrived at a 2 node blue/green model: both nodes are running identical code/data, we deploy to green first, if it goes well, then to blue. Both are behind an Elastic Load Balancer (&lt;code&gt;predict&lt;/code&gt; happens over a HTTP API call), so things keep going if one of them is down, even if down for a long time.&lt;/p&gt;
&lt;p&gt;Fresh data is loaded every night by an Airflow process: first it creates a daily dump for the ML models, uploads it to S3, and then triggers a special &lt;code&gt;load&lt;/code&gt; API call, first on the green, then the blue host. The Airflow job for blue depends on green, so if green fails, it won't touch blue, so production will not be impacted.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/model_dag.png" alt="A table on S3." style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;At this scale/complexity, this simple model works quite well and we have excellent uptime; we have more problems coming from software bugs than availability.&lt;/p&gt;
&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;This post focused on what is usually called Data Infrastructure and Data Engineering, but actually to build and run this platform only took about 0.5-1 FTE average effort over time (though very senior FTEs). The rest of our time directly focused on operational and business impact by building metrics and dashboards, running ad-hoc analytics, building operational models for forecasting and sizing, building ML models, and most important of all, explaining it all to our colleagues so our work is adopted and has impact on the ground. It's interesting to note that just 5 years ago, in a similar scenario, good-enough open source tools like Airflow and Superset were not available, so we had to roll our own and ended up spending an order of magnitude more time on DI/DE work.&lt;/p&gt;
&lt;p&gt;Overall, in the last year, our small Data Science team was able to have dollar-measurable outsized impact on Fetchr by using data to understand and optimize operations and business processes. Today, many important operational decisions are based on data and driven by Data Scientists, including sizing our fleets and warehouses, understanding their performance, and ML models optimizing and automating human labor.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="airflow"></category><category term="fetchr"></category><category term="model"></category><category term="ml"></category></entry><entry><title>Beat the averages</title><link href="/beat-the-averages.html" rel="alternate"></link><published>2018-07-07T00:00:00+02:00</published><updated>2018-07-07T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-07-07:/beat-the-averages.html</id><summary type="html">&lt;p&gt;When working with averages, we have to be careful. There are pitfalls lurking to pollute our statistics and results reported.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/averages-7.png" alt="Probability distribution" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;The numbers in this article are made up, but the lessons come from real life.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When doing Data Science, we almost always report averages. This is natural, because it stands for a simple model that everybody can understand. For example, in the delivery business, a topline metric is Deliveries per Driver (DPD), the average deliveries made per driver per day. This is a simple model the CEO can also remember: if our fleet is performing at DPD = 40, and we have 1,000 drivers, we make 40,000 deliveries per day. Being able to multiply two topline numbers and get a third one is a good thing.&lt;/p&gt;
&lt;p&gt;When working with averages, we have to be careful though: there are pitfalls lurking to pollute our statistics and results reported. It is important to note that &lt;strong&gt;there is nothing wrong with averages themselves, we just have to be careful with them&lt;/strong&gt;. I don’t believe that for most reporting purposes averages should or can be replaced (eg. by reporting the median), it is simply the job of the Data Science team to make sure the metrics make sense.&lt;/p&gt;
&lt;h2&gt;Outliers&lt;/h2&gt;
&lt;p&gt;When we say that our DPD is 40 and we have 1,000 drivers, the natural inclination (even for data people) is to &lt;em&gt;imagine&lt;/em&gt; 1,000 equivalent drivers, each performing exactly 40 deliveries every day. But we know that the world isn’t this simple. Things like driver performance tend to follow some more interesting distribution. The simplest thing we can imagine is that it follows a normal distribution. The plot below shows a normal distribution, the mean (green) and median (red) coincide. Gauss is happy.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-1.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;But almost always &lt;strong&gt;there are outliers&lt;/strong&gt;. In the case of drivers, there are various special circumstances which can cause a driver to have very low or very high DPD. For example, maybe the driver got sick, interrupted his route and went home early. Below is a the same distribution as above, with some stragglers introduced. We can see that this shifts the mean (green) down.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-2.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The shift in the mean is important, because it signals that something is going on: a bunch of our drivers got sick and went home early. Maybe tomorrow they are not coming to work. So monitoring both the average and median is important to detect and understand deviations.&lt;/p&gt;
&lt;p&gt;Apart from the median, which is also called the 50th &lt;a href="https://en.wikipedia.org/wiki/Percentile"&gt;percentile&lt;/a&gt;, &lt;strong&gt;checking out the bottom and top percentiles&lt;/strong&gt; is also very helpful. Below is the same two plots, with p10 and p90 also showing in red.&lt;/p&gt;
&lt;div&gt;
&lt;img src="/images/averages-3.png" alt="Probability distribution" style="width: 325px;"/&gt;

&lt;img src="/images/averages-4.png" alt="Probability distribution" style="width: 325px;"/&gt;
&lt;/div&gt;

&lt;p&gt;Something really useful happened! After we introduced the stragglers, the p10 dropped from about 27 to about 8!&lt;/p&gt;
&lt;p&gt;In general, &lt;strong&gt;showing percentiles is a useful technique, because as the example above shows, they can dramatically speed up detection of anomalies&lt;/strong&gt;. In real life work, looking at distribution doesn’t happen on a daily basis, but a timeseries showing the historic DPD can also show p10, median and p90, and can show such anomalies. The chart below shows such a made-up example, showing p10, p50 and p90 in red, the average in green, for the last 30 days for the fleet. On the 25th day a flu started spreading between our drivers, introducing the stragglers as shown in the distribution above. The mean and the median separate somewhat, but &lt;strong&gt;the p10 gives it away&lt;/strong&gt;.
It’s worth showing all four lines, at least on internal, debug dashboards.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-5.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;It’s worth noting the outliers can come from another source too: data bugs. Another good trick is to periodically examine low and high performers in a table, attached to the bottom of the internal report/dashboard.&lt;/p&gt;
&lt;p&gt;Finally, outlier/anomaly detection can also be automated, for example Facebook does this internally for various metrics. It’s important to automate at least the visualization of anomalies/distributions/stragglers in a debug dasboard, because in the long-run Data Scientists will forget to check manually (export and plot in ipython takes time).&lt;/p&gt;
&lt;h2&gt;Several populations&lt;/h2&gt;
&lt;p&gt;Another reason averages can be polluted is because of multiple populations (outliers can also be thought of as a population). In the delivery business, it is not uncommon to have many separate fleets of drivers, for different purposes. For example, we may have a B2C and a C2C fleet. Another distinction is cars vs bikes. Uber could have a fleet for passengers and a totally separate fleet for UberEats. Below is a (made-up) distribution that’s actually two fleets, a C2C fleet performing at DPD=20 and a B2C fleet performing at DPD=40.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-6.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;In cases like this, &lt;strong&gt;reporting on the blended mean may be misleading&lt;/strong&gt;. For example, if country X has a B2C and a C2C fleet, while country Y only has a B2C fleet, then reporting just on country-wise DPD will be misleading. For country X the C2C fleet will pull the DPD down, but this doesn’t mean that the Ops team in country X is performing worse, in fact it’s possible their B2C fleet is outperforming country Y’s. Report the per-fleet mean instead.&lt;/p&gt;
&lt;h2&gt;Skewed distributions&lt;/h2&gt;
&lt;p&gt;Sometimes distributions are not symmetric, they can be lopsided. In this case the median, mode (the most frequent outcome, the maximum of the distribution) and mean can be at different locations, which is often unintuitive for people. This isn’t a problem wrt the mean, but it’s good to know. The &lt;a href="https://en.wikipedia.org/wiki/Log-normal_distribution"&gt;Log-normal distribution&lt;/a&gt; is one such example:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-7.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Percentage instead of mean&lt;/h2&gt;
&lt;p&gt;Sometimes, when building a metric, the mean is not a good choice. Let’s take pageload times an an example. Suppose we measure the average pageload time in miliseconds, and we see that it is 4,200ms; too high. After rolling out changes, it goes down to 3,700ms; but, 3,700 is still too high. Does that mean the rollout wasn't successful?&lt;/p&gt;
&lt;p&gt;In situations like this, it makes sense to &lt;strong&gt;bake the goal into the metric&lt;/strong&gt;. Suppose our goal is 2,000ms, which we deem pleasant from a UX perspective. Then a better way to define the metric is "% of pageloads that are within 2,000ms". If it was 57% before, and 62% after the rollout, it’s &lt;strong&gt;more natural to understand what happened&lt;/strong&gt;: an additional 5% of people now have a pleasant experience when loading our page. If there are 1,000,000 users per month, we impacted 50,000 users per month with the rollout. Not bad! A metric like this is also &lt;strong&gt;more motivating for product teams to work on&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Another big advantage of using percentages is &lt;strong&gt;increased resiliency to outliers&lt;/strong&gt;. While the mean could be polluted by outliers (users on slow connections, bots, data bugs), in the % it will be “just” a constant additive factor.&lt;/p&gt;
&lt;h2&gt;Ratios instead of means&lt;/h2&gt;
&lt;p&gt;Our delivery business also has C2C, ie. people can call a driver and send a package to another person, on demand. For example, if my partner is at the airport, but they forgot their passport at home, I can use the C2C app to fetch a car and send her the passport in an envelope. As such, the C2C app has standard metrics such as Daily Active Users (DAU) and Monthly Active Users (MAU). These are topline metrics, but we also need a metric which expresses how often people use the product. One way to do it using means would be to count, for each user, how many days they were DAU of the last 28 days. Suppose we call this Average DAU, and it’s 5.2. This is not that hard to understand, but could still be confusing. For example, people always forget the definition of a metric, in this case they would forget if the metric is 28 or 30 or 7 day based. Also, increments like this don’t feel natural: a +1% increment corresponds to +0.28 active days or 6.72 hours.&lt;/p&gt;
&lt;p&gt;A better metric is simply to divide DAU per MAU. This is a common metric also used inside Facebook. This feels more natural: if we are honest with ourselves, a user is essentially a MAU, because somebody who hasn’t used the product for 28 days is probably not coming back (For products with more sporadic usage, the base could be a 3*28 days). Thinking like this DAU/MAU is a very natural metric: it is the % of "users" who use the product daily.&lt;/p&gt;
&lt;h2&gt;Daily variations&lt;/h2&gt;
&lt;p&gt;Suppose our fleet’s average DPD is 40. Looking at driver X, his DPD yesterday was 29. Is he a low performer? Our first intuition might be to ask what the standard deviation of the fleet is (suppose it is 10), and then argue that this value is not “significantly” off. But from a business perspective, variance is irrelevant: if the COO wants to improve DPD and is looking for low performing drivers to cut, "cutting" at mean minus one sigma is a valid approach.&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;it’s possible that our drivers have significant daily variation in their performance&lt;/strong&gt;. It’s possible that this driver had a DPD of 29 yesterday, but the previous day it was 47, and their historic average is actually 42. Always &lt;strong&gt;compare averages to averages&lt;/strong&gt;. In this case, compare the fleet’s average DPD over a long enough timeframe (probably at least 28 days) to the driver’s average DPD in the same 28 days. That is a more fair comparison to make, because it smooths daily variation. Of course, remember what was said here, for example don’t count days when the driver was sick, and compare him to his own fleet. &lt;/p&gt;
&lt;h2&gt;In summary&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Using averages is okay most of the time.&lt;ul&gt;
&lt;li&gt;Reporting on medians is probably not feasible in a business/product setting.&lt;/li&gt;
&lt;li&gt;Instead, make sure the average is meaningful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Watch out for outliers.&lt;ul&gt;
&lt;li&gt;Check median/p10/p90 and distributions regularly.&lt;/li&gt;
&lt;li&gt;Prune/separate outliers.&lt;/li&gt;
&lt;li&gt;Split up populations (B2C/C2C, car/bike), etc. to make sure the reported average (or median) is a meaningful number.&lt;/li&gt;
&lt;li&gt;Outliers can be real outliers, or issues in the data (eg. Self Pickup as a driver)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sometimes the population is homogeneous, but the distribution is skewed to one side or bimodal, in this case the average may be intuitively misleading.&lt;/li&gt;
&lt;li&gt;Sometimes, using %s instead of averages makes a better metric (Pageloads within 2000ms vs Average Pageload time).&lt;/li&gt;
&lt;li&gt;Sometimes, using a ratio instead of averages makes a better metric (example: DAU/MAU vs average number of DAUs in the last 28 days).&lt;/li&gt;
&lt;li&gt;Be careful when comparing daily snapshots and averages, there may be significant daily variation in performance.&lt;/li&gt;
&lt;/ol&gt;</content><category term="statistics"></category><category term="data"></category></entry><entry><title>Building the Fetchr Data Science Infra on AWS with Presto and Airflow</title><link href="/fetchr-airflow.html" rel="alternate"></link><published>2018-03-14T00:00:00+01:00</published><updated>2018-03-14T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-03-14:/fetchr-airflow.html</id><summary type="html">&lt;p&gt;We used Hive/Presto on AWS together with Airflow to rapidly build out the Data Science Infrastructure at Fetchr in less than 6 months.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/warehouse-dag.png" alt="Warehouse DAG" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Our goal at &lt;a href="https://fetchr.us"&gt;Fetchr&lt;/a&gt; is to build a world-class Data Science team. To do this, we need a world-class Data Science platform. I was fortunate enough to work at Facebook previously, which over the years arrived at a very efficient way of doing Data Science. So, when it came to building the platform I decided to follow the basic design patterns that I saw at Facebook.&lt;/p&gt;
&lt;p&gt;Based on the last 6 months, building a platform (including computation jobs, dashboarding) that is simple but allows us to move fast is feasible in just a 3-6 month period. So what does our platform look like? Like most things at Fetchr, we run on AWS. Our infra consists of 5-10 nodes right now (5 EMR, 2 Airflow, a few more for Supersets and others).&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/fetchr-ds-arch.png" alt="Fetchr Data Science Infra" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;We use &lt;a href="https://aws.amazon.com/emr/"&gt;EMR&lt;/a&gt; to get a Hadoop instance, with S3 as the backing storage. We actually don’t use the Hive query engine or MapReduce. We just use &lt;a href="http://hadoop.apache.org/"&gt;Hadoop&lt;/a&gt; as a metadata store (table definitions) for &lt;a href="https://prestodb.io/"&gt;Presto&lt;/a&gt;. Each EMR node also runs a Presto worker. Right now we use 1+4 nodes, with plans to scale it out to ~10.&lt;/p&gt;
&lt;p&gt;The data warehouse (DWH) philosophy is again based on the Facebook design pattern. We use flat tables, no fact/dimension tables; usually you can look at a table and see a complete picture. This makes the tables very usable and allows us to move fast, for example writing quick queries against tables is easy because it doesn’t require a lot of JOINs to get readable strings.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/flat-table.png" alt="Flat DWH table" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The other major design pattern from Facebook is the idea of daily partitioned tables. This is a feature available on Hive, and not really practical on eg. &lt;a href="https://aws.amazon.com/redshift/"&gt;Redshift&lt;/a&gt;. Essentially we store (complete) daily, write-once slices of each table, which are generated by daily jobs. The partitions are called &lt;code&gt;ds&lt;/code&gt; at Facebook and logically show up as a column of the table, and you’ll find plenty of references to it if you read the &lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual"&gt;Hive docs&lt;/a&gt; (because Hive was written at Facebook). Physically, these are essentially directories, each one holding the data files for that day’s data. We use S3, so in our case it looks something like &lt;code&gt;s3://dwh-bucket/&amp;lt;table&amp;gt;/&amp;lt;ds&amp;gt;/&amp;lt;data_files&amp;gt;&lt;/code&gt;. For example, &lt;code&gt;s3://dwh-bucket/company_metrics/2018-03-01/datafile&lt;/code&gt;. For technical reasons, when importing data from our production (Postgresql) database, we use .csv, for later computed warehouse tables we use &lt;a href="https://orc.apache.org/"&gt;ORC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The advantage of this is that we have a complete history of the data warehouse going back as far as we’d like (old partitions can be deleted from a script after the desired retention period expires). There’s two ways to use &lt;code&gt;ds&lt;/code&gt; partitions, cumulative and events: each partition can store a complete copy of its data up to that day (cumulative), or each partition just stores that day’s worth of (event) data. For aggregate tables, it’s usually the first, for raw event tables, it’s usually the second. For example, our &lt;code&gt;company_metrics&lt;/code&gt; has complete cumulative data in each &lt;code&gt;ds&lt;/code&gt;, while our &lt;code&gt;driver_telemetry&lt;/code&gt; table has just that day’s worth of telemetry events. The advantage of this is that if something breaks, there’s almost never a big problem; we can always refer to yesterday’s data, and get away with it. Data will never be unavailable, it may just be late. Also, if there’s ever a question why a number changed, it’s easy to see what the reported number was a month ago (by examining that day’s &lt;code&gt;ds&lt;/code&gt; partition).&lt;/p&gt;
&lt;p&gt;We use &lt;a href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt; for data piping, which is loosely based on &lt;a href="http://www.asiliconvalleyinsider.com/asiliconvalleyinsider/Blog_A_Silicon_Valley_Insider/Entries/2016/5/1_Data_Engineering_%40_Facebook.html"&gt;Facebook’s Dataswarm system&lt;/a&gt;. Airflow allows us to write jobs as Directed Acyclic Graphs (DAGs) of tasks, with each task getting something useful done, like a database &lt;code&gt;INSERT&lt;/code&gt;. In Airflow, each DAG has a schedule, which uses the &lt;a href="https://airflow.apache.org/scheduler.html"&gt;cron format&lt;/a&gt;, so it can be daily, hourly, or just run every Wednesday at 3:15PM. On each of these runs, Airflow creates an instance of the DAG (identified by the timestamp), and executes the tasks, taking into account the dependencies between them. We have 2 types of DAGs: imports, for importing tables from the production database to the DWH, and compute jobs, which take existing (imported or computed) tables and make a new, more useful table. Fundamentally, each table is its own DAG.&lt;/p&gt;
&lt;p&gt;This poses a question: how do we make sure that a table’s DAG only runs once another table that is required (eg. it’s used in the &lt;code&gt;FROM&lt;/code&gt; part) is available (the latest &lt;code&gt;ds&lt;/code&gt; is available). This is accomplished with having special sensor tasks, which continuously check something (in this case whether a table’s partition is there), and only succeed if the check succeed; until then these “wait” tasks block the DAG from executing. For example, this is what a typical DAG looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/warehouse-dag.png" alt="Warehouse DAG" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;There are two waits (one for a table called &lt;code&gt;deliveries&lt;/code&gt;, one for this table but yesterday’s &lt;code&gt;ds&lt;/code&gt; partition, which is a kind of self-dependency), there is a &lt;code&gt;create&lt;/code&gt; task which creates the table in case it doesn’t exist, the &lt;code&gt;drop_partition&lt;/code&gt; drops the partition in case it already exists (in case we’re re-running the job), the &lt;code&gt;insert&lt;/code&gt; does the actual &lt;code&gt;INSERT INTO … SELECT ... FROM ...&lt;/code&gt;, and then some useful views are created (eg. for a table called &lt;code&gt;company_metrics&lt;/code&gt;, the view task creates a view called &lt;code&gt;company_metrics_latest&lt;/code&gt;, which points to the latest &lt;code&gt;ds&lt;/code&gt; partition).&lt;/p&gt;
&lt;p&gt;DAGs for import jobs are simpler:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/import-dag.png" alt="Import DAG" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;s3copy&lt;/code&gt; is the task which dumps the table from the production Postgresql into a local file and then copies it to S3, to the appropriate path. The &lt;code&gt;notice&lt;/code&gt; lets Hive now that we “manually” created a new partition on the backing storage, and triggers the metadata store to re-scan for new partitions by issuing &lt;code&gt;MSCK REPAIR TABLE &amp;lt;table&amp;gt;&lt;/code&gt;. (The &lt;code&gt;notice&lt;/code&gt; in the upper DAG is actually not required, since it’s a Presto job.)&lt;/p&gt;
&lt;p&gt;Airflow creates daily instances (for daily jobs) of these DAGs, and has a very helpful view to show progress/completion.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/dag-runs.png" alt="Warehouse DAG" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The UI also allows for tasks to be cleared, re-run, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/task-actions.png" alt="Task actions" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Each DAG is implemented as Python code, in our case one &lt;code&gt;.py&lt;/code&gt; file per DAG. Most of these DAGs are highly repetitive, so we wrote a small library to save us time. For example, since we’re importing from a Postresql database, which is itself a relational database, it’s enough to say which table we want to import, our scripts figure out what the source table’s schema is, it knows how to map Postgresql types to Hive types, handle column names which are not allowed on Hive, etc. This makes importing a table as easy as:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/import-code.png" alt="Import code" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;All the logic is contained in the &lt;code&gt;dag_import_erp_table()&lt;/code&gt; function, which is re-used for all imports.&lt;/p&gt;
&lt;p&gt;We wrote similar helper functions for our common warehouse jobs, which take existing tables to build a new, more useful one. We specify the name of the output table, the &lt;code&gt;schedule_interval&lt;/code&gt;, the Hive columns (which is used to generate the &lt;code&gt;CREATE TABLE&lt;/code&gt; task), and the Presto &lt;code&gt;SELECT&lt;/code&gt; query, which will be placed after the &lt;code&gt;INSERT&lt;/code&gt; part in the insert task. Note the use of the &lt;code&gt;wait::&lt;/code&gt; prefix in the &lt;code&gt;FROM&lt;/code&gt; part. The helper functions automatically parses out these and generates wait tasks for these tables. A number of other such features were added to make it easy, fast and convenient to write jobs, without having to go outside the use of these helper functions. The &lt;code&gt;{{ ds }}&lt;/code&gt; macro will be replaced by the Airflow runtime with the proper ds, like &lt;code&gt;2018-02-20&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/warehouse-code.png" alt="Warehouse code" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Right how we have around 50 jobs, about half are “real” computations, the rest are imports. At this point we are able to move really fast: writing a new job and deploying it to production takes about an hour, and new joiners can ramp up quickly. Because we use Presto/Hive on top of S3 (versus &lt;a href="https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c"&gt;Airbnb runs their own Hadoop cluster&lt;/a&gt;) this introduced some low-level difficulties, so we had to write our own Operators, for example a PrestoOperator. Overall this code, plus the helper code is about 1-2k LOC, so it wasn’t too much work. To be fair, we never hit any data size problems, since compared to the capabilities of these tools, we have "small data". Our biggest tables are ~100M rows (these are part of 10-way &lt;code&gt;JOINs&lt;/code&gt;), but Hive/Presto can easily handle this with zero tuning. We expect to grow 10x within a year, but we expect that naive linear scaling will suffice.&lt;/p&gt;
&lt;p&gt;Maintaining a staging data warehouse is not practical in our experience, but maintaining a staging Airflow instance is practical and useful. This is because of Airflow’s brittle execution model: DAG’s &lt;code&gt;.py&lt;/code&gt; files are executed by the main webserver/scheduler process, and if there’s a syntax error then bad things happen, for example certain webserver pages don’t load. So it’s best to make sure that scripts deployed to the production Airflow instance are already working. So we set up a second, staging Airflow instance, which writes to the same data warehouse, (we have only one) but has its own internal state. Our production Airflow instance runs on two EC2 nodes. One for the webserver and the scheduler, one for the workers. The staging runs on a third, all 3 components on the same host.&lt;/p&gt;
&lt;p&gt;Overall, getting here was fast, mostly because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the database components (Hive, Presto) were open sourced by Facebook&lt;/li&gt;
&lt;li&gt;Amazon runs them for us as part of EMR&lt;/li&gt;
&lt;li&gt;we don't have to manage storage because of S3&lt;/li&gt;
&lt;li&gt;other former Facebook engineers built Airflow and Airbnb open sourced it&lt;/li&gt;
&lt;li&gt;because of the common background (Facebook) everything made sense.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having said that, Airflow still feels very “beta”. It’s not hard to “confuse” it, where it behaves in weird ways, pages don’t load, etc. For example, if a DAG’s structure changes too much, Airflow seems to get confused and exceptions are thrown; for cases like this we wrote a custom scripts which wipes Airflow’s memory of this DAG completely (we didn’t find a way to do this with the provided CLI or UI). But, once we understood how it works and learned its quirks, we found a way to use it for our use-case. This process took about 1-2 months. We now rarely run into Airflow issues, perhaps once a month.&lt;/p&gt;
&lt;p&gt;The limits of this architecture is that it's very batch-y. For "real-time" jobs, we use hourly or 15-minute jobs to get frequent updates, but we apply manual filters on data size to make these run fast(er). Overall, this is inconvenient, and won't scale very well, eventually we'll have to look at other technologies for this use-case. Overall, we feel this is inconveniance/limitation/techdebt is a small price to pay for all the high-level product and business impact that we were able to deliver with this architecture.&lt;/p&gt;
&lt;p&gt;Airflow is now under Apache incubation, with lots of development activity, so it will surely get even better in the coming years. Going with Airflow was a bet that payed off, and we expect that Airflow will become the defacto open source ETL tool, if it’s not already that.&lt;/p&gt;
&lt;p&gt;In the next part about Fetchr's Data Science Infra, I’ll talk about how we use Superset for dashboarding and SQL.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="airflow"></category><category term="fetchr"></category></entry><entry><title>Don’t build cockpits, become a coach</title><link href="/data-science-coaching.html" rel="alternate"></link><published>2016-11-09T00:00:00+01:00</published><updated>2016-11-09T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-11-09:/data-science-coaching.html</id><summary type="html">&lt;p&gt;I used to think that a good analogy for using data is the instrumentation of a cockpit in an airliner. Lots of instruments, and if they fail, the pilot can’t fly the plane and bad things happen. There’s no autopilot for companies. The problem with this analogy is that planes aren’t built in mid-air. Product teams and companies constantly need to build and ship new products.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/hosszu-shane.jpg" alt="A big complicated cockpit" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I used to think that a good analogy for using data is the instrumentation of a cockpit in an airliner. Lots of instruments, and if they fail, the pilot can’t fly the plane and bad things happen. There’s no autopilot for companies.&lt;/p&gt;
&lt;p&gt;The problem with this analogy is that planes aren’t built in mid-air. Product teams and companies constantly need to build and ship new products. Facebook is very good at this.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/cockpit.jpg" alt="A big complicated cockpit" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;A better model is a person, let’s say Petie, who is overweight and wants to lose weight. We’re going to need a scale, to see how much Petie actually weighs. If there’s no scales around, we’ll need to build one. While we’re at it, we can also collect other numbers, like body fat, circumference, etc. Then, we need to get Petie to actually look at the numbers. So we send these numbers to Petie every day in an email, or maybe we build a dashboard for him. Sounds good! But at this point all we did is make Petie know precisely how overweight he is. We probably successfully made Petie depressed about himself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What Petie really needs is a coach who helps him get in shape. He needs a workout plan and somebody to work with him on reaching his goals.&lt;/strong&gt; He needs somebody to help figure out what the workout should be, and set goals for the workout sessions. Once he’s reached his goals, he needs help figure out the next phase, what other exercises to do. Less running, more lifting, maybe do an experiment to see what diet works better for him. Knowing numbers is part of it, but the point it to somehow get him to go and do the things which will make him lose weight, keep track of how he’s doing, make sure he’s on track, help him make changes on the way.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/hosszu-shane.jpg" alt="A big complicated cockpit" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Many professional sports teams already use data heavily. The most famous example I know is the british cycling team, first described in this &lt;a href="http://www.bbc.co.uk/sport/olympics/19174302"&gt;2012 BBC article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I think being good coach is really hard. A good coach needs to know the sport, the athlete, the equipment, how training works, how the season and races work, everything. A good coach is on excellent terms with the athlete and is a great communicator. The coach needs to be able to convince the athlete to perform deliberate practice, which is hard and painful stuff. And to make it harder, painstakingly take precise measurements while he’s doing it. A good coach is not made overnight. A good idea is to learn from other, more experienced coaches, who have successfully helped athletes reach their goals. Facebook has good coaches (data scientists).&lt;/p&gt;
&lt;p&gt;Whenever I use an analogy I set off the Elon-alarm in my head. &lt;a href="http://jamesclear.com/first-principles"&gt;Elon Musk famously said&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I think it is important to reason from first principles rather than by analogy. The normal way we conduct our lives is we reason by analogy. [When reasoning by analogy] we are doing this because it’s like something else that was done or it is like what other people are doing — slight iterations on a theme.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But, this one seems useful enough to ignore Elon.&lt;/p&gt;</content><category term="data"></category><category term="science"></category><category term="product"></category><category term="analytics"></category></entry><entry><title>Luigi vs Airflow vs Pinball</title><link href="/luigi-airflow-pinball.html" rel="alternate"></link><published>2016-02-06T00:00:00+01:00</published><updated>2016-02-06T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-02-06:/luigi-airflow-pinball.html</id><summary type="html">&lt;p&gt;A spreadsheet comparing the three opensource workflow tools for ETL.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/airflow-luigi-pinball.png" alt="Comparison" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;After reviewing these three ETL worflow frameworks, I compiled a table comparing them. Here's the original &lt;a href="https://docs.google.com/spreadsheets/d/1KCXtuht_wZPFROFwdeg7IXrNPUhFI277y4h-xnc8mgk/edit#gid=0"&gt;Gdoc spreadsheet&lt;/a&gt;. If I had to build a new ETL system today from scratch, &lt;strong&gt;I would use Airflow&lt;/strong&gt;. If you find any mistakes, please let me know at &lt;a href="mailto:mtrencseni@gmail.com"&gt;mtrencseni@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;style type="text/css"&gt;.ritz .waffle a { color: inherit; }.ritz .waffle .s1{border-bottom:1px SOLID #000000;text-align:center;font-weight:bold;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s4{text-align:center;color:#000000;background-color:#fce5cd;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s6{text-align:center;color:#000000;background-color:#d9ead3;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s9{text-align:left;color:#000000;background-color:#ffffff;font-family:'arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s0{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;text-align:left;font-weight:bold;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s3{text-align:center;text-decoration:underline;color:#1155cc;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s5{text-align:center;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s8{text-align:center;color:#000000;background-color:#d9ead3;font-family:'arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s2{border-right:1px SOLID #000000;text-align:left;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s7{border-right:1px SOLID #000000;text-align:left;font-weight:bold;text-decoration:underline;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}
.row-headers-background {display:none;}
.column-headers-background {display:none;}
.freezebar-cell {display:none;}
&lt;/style&gt;

&lt;div class="ritz grid-container" dir="ltr"&gt;&lt;table class="waffle" cellspacing="0" cellpadding="0"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th class="row-header freezebar-vertical-handle"&gt;&lt;/th&gt;&lt;th id="0C0" style="width:183px" class="column-headers-background"&gt;A&lt;/th&gt;&lt;th id="0C1" style="width:295px" class="column-headers-background"&gt;B&lt;/th&gt;&lt;th id="0C2" style="width:329px" class="column-headers-background"&gt;C&lt;/th&gt;&lt;th id="0C3" style="width:295px" class="column-headers-background"&gt;D&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R0" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;1&lt;/div&gt;&lt;/th&gt;&lt;td class="s0"&gt;&lt;/td&gt;&lt;td class="s1" dir="ltr"&gt;Luigi&lt;/td&gt;&lt;td class="s1" dir="ltr"&gt;Airflow&lt;/td&gt;&lt;td class="s1" dir="ltr"&gt;Pinball&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style="height:3px" class="freezebar-cell freezebar-horizontal-handle"&gt;&lt;/th&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R1" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;2&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;repo&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://github.com/spotify/luigi"&gt;https://github.com/spotify/luigi&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://github.com/airbnb/airflow"&gt;https://github.com/airbnb/airflow&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://github.com/pinterest/pinball"&gt;https://github.com/pinterest/pinball&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R2" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;3&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;docs&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://luigi.readthedocs.org"&gt;http://luigi.readthedocs.org&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://airflow.readthedocs.org"&gt;https://airflow.readthedocs.org&lt;/a&gt;&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;none&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R3" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;4&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;my review&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://bytepawn.com/luigi.html"&gt;http://bytepawn.com/luigi.html&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://bytepawn.com/airflow.html"&gt;http://bytepawn.com/airflow.html&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://bytepawn.com/pinball.html"&gt;http://bytepawn.com/pinball.html&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R4" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;5&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;github forks&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;750&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;345&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;58&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R5" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;6&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;github stars&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;4029&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;1798&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;506&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R6" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;7&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;github watchers&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;319&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;166&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;47&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R7" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;8&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;commits in last 30 days&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots of commits&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots of commits&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;3 commits&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R8" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;9&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;architecture&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R9" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;10&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;web dashboard&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;not really, minimal&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;very nice&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R10" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;11&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;code/dsl&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;code&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;code&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python dict + python code&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R11" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;12&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;files/datasets&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, targets&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;not really, as special tasks&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R12" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;13&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;calendar scheduling&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no, use cron&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, LocalScheduler&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R13" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;14&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;datadoc&amp;#39;able [1]&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;maybe, doesn&amp;#39;t really fit&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;probably, by convention&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, dicts would be easy to parse&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R14" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;15&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;backfill jobs&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R15" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;16&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;persists state&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;kindof&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, to db&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, to db&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R16" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;17&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;tracks history&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, in db&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, in db&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R17" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;18&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;code shipping&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, pickle&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;workflow is shipped using pickle, jobs are not?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R18" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;19&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;priorities&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R19" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;20&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;parallelism&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, workers, threads per workers&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, workers&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R20" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;21&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;control parallelism&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, resources&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, pools&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R21" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;22&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;cross-dag deps&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, using targets&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, using sensors&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R22" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;23&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;finds new deployed tasks&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R23" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;24&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;executes dag&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no, have to create special sink task&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R24" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;25&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;multiple dags&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no, just one&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, also several dag instances (dagruns)&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R25" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;26&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;scheduler/workers&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R26" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;27&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;starting workers&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;users start worker procceses&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;scheduler spawns workers processes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;users start worker procceses&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R27" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;28&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;comms&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;scheduler&amp;#39;s HTTP API&lt;/td&gt;&lt;td class="s8" dir="ltr"&gt;minimal, in state db&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;through master module using Swift&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R28" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;29&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;workers execute&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;worker can execute tasks that is has locally&lt;/td&gt;&lt;td class="s8" dir="ltr"&gt;worker reads pickled tasks from db&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;worker can execute tasks that is has locally?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R29" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;30&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;contrib&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R30" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;31&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;hadoop&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R31" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;32&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;pig&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;doc mentions PigOperator, it&amp;#39;s not in the source&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R32" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;33&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;hive&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R33" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;34&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;pgsql&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R34" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;35&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;mysql&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R35" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;36&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;redshift&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R36" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;37&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;s3&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R37" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;38&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;source&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R38" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;39&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;written in&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R39" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;40&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;loc&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;18,000&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;21,000&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;18,000&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R40" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;41&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;tests&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;minimal&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R41" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;42&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;maturity&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;fair&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;low&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;low&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R42" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;43&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;other serious users&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;not really&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R43" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;44&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;pip install&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;broken&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R44" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;45&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;niceties&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;-&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;sla, xcom, variables, trigger rules, celery, charts&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;pass data between jobs&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R45" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;46&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;does it for you&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R46" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;47&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;sync tasks to workers&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R47" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;48&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;scheduling&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R48" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;49&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;monitoring&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R49" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;50&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;alerting&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;slas, but probably not enough&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;sends emails&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R50" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;51&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;dashboards&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;

&lt;script type='text/javascript'&gt;
function posObj(sheet, id, row, col, x, y) {
  var rtl = false;
  var sheetElement = document.getElementById(sheet);
  if (!sheetElement) {
    sheetElement = document.getElementById(sheet + '-grid-container');
  }
  if (sheetElement) {
    rtl = sheetElement.getAttribute('dir') == 'rtl';
  }
  var r = document.getElementById(sheet+'R'+row);
  var c = document.getElementById(sheet+'C'+col);
  if (r &amp;&amp; c) {
    var objElement = document.getElementById(id);
    var s = objElement.style;
    var t = y;
    while (r) {
      t += r.offsetTop;
      r = r.offsetParent;
    }
    var offsetX = x;
    while (c) {
      offsetX += c.offsetLeft;
      c = c.offsetParent;
    }
    if (rtl) {
      offsetX -= objElement.offsetWidth;
    }
    s.left = offsetX + 'px';
    s.top = t + 'px';
    s.display = 'block';
    s.border = '1px solid #000000';
  }
};
function posObjs() {
};
posObjs();&lt;/script&gt;

&lt;p&gt;[1] By datadoc'able I mean: could you write a script which reads and parses the ETL jobs, and generates a nice documentation about your datasets and which ETL jobs read/write them. At Prezi we did this, we called it datadoc.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="luigi"></category><category term="airflow"></category><category term="pinball"></category></entry><entry><title>Pinball review</title><link href="/pinball.html" rel="alternate"></link><published>2016-02-06T00:00:00+01:00</published><updated>2016-02-06T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-02-06:/pinball.html</id><summary type="html">&lt;p&gt;Pinball is an ETL tool written by Pinterest. Like Airflow, it supports defining tasks and dependencies as Python code, executing and scheduling them, and distributing tasks across worker nodes. It supports calendar scheduling (hourly/daily jobs, also visualized on the web dashboard). Unfortunately, I found Pinball has very little documentation, very few recent commits in the Github repo and few meaningful answers to Github issues by maintainers, while it's architecture is complicated and undocumented.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/pinterest/pinball"&gt;Pinball&lt;/a&gt; is Pinterest’s open sourced workflow manager / ETL system. It supports defining several workflows (DAGs) consisting of jobs, and dependencies within jobs. Workflows are defined using a combination of declarative-style Python dictionary objects (like JSON) and Python code referenced in these objects. Pinball comes with a dashboard for checking currently running and past workflows.&lt;/p&gt;
&lt;p&gt;This review will be shorter than the previous &lt;a href="/luigi.html"&gt;Luigi&lt;/a&gt; and &lt;a href="/airflow.html"&gt;Airflow&lt;/a&gt; reviews, because Pinball turned out to be not very interesting to me for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Very little &lt;a href="https://github.com/pinterest/pinball#installation"&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Very &lt;a href="https://github.com/pinterest/pinball/commits/master"&gt;few recent commits&lt;/a&gt; in the Github repo&lt;/li&gt;
&lt;li&gt;Very &lt;a href="https://github.com/pinterest/pinball/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aissue"&gt;few meaningful answers&lt;/a&gt; to Github issues from the maintainers&lt;/li&gt;
&lt;li&gt;Complicated and undocumented architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately &lt;code&gt;pip install pinball&lt;/code&gt; doesn’t work and &lt;a href="https://github.com/pinterest/pinball/issues/9"&gt;the maintainers don’t care&lt;/a&gt;, so I didn't invest time in actually trying out Pinball, I just read the source code. Since this review is short and opinionated, I recommend also reading the Pinterest posts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://engineering.pinterest.com/blog/pinball-building-workflow-management"&gt;Pinball: Building workflow management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineering.pinterest.com/blog/open-sourcing-pinball"&gt;Open-sourcing Pinball&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Pinball has a modularized architecture. There are 5 modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Master (sits on the DB)&lt;/li&gt;
&lt;li&gt;Scheduler (also accessed DB)&lt;/li&gt;
&lt;li&gt;Worker (also accessed DB)&lt;/li&gt;
&lt;li&gt;UI web server (also accessed DB)&lt;/li&gt;
&lt;li&gt;Command-line&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The master module sits on top of a &lt;a href="https://www.mysql.com/"&gt;Mysql&lt;/a&gt; database (no others supported) and uses &lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; for ORM. The master exposes a synchronization token API using &lt;a href="https://thrift.apache.org/"&gt;Thrift&lt;/a&gt; to the other modules, and that’s all the master does. I think this is an unnecessary layer of abstraction; the Airflow design decision is better: everybody sees the DB and uses that to communicate, get &lt;a href="https://en.wikipedia.org/wiki/ACID"&gt;ACID&lt;/a&gt; for free; no need to define and maintain an API, no need for Thrift. In the blog post, they say &lt;em&gt;“component-wise design allows for easy alterations”&lt;/em&gt;, eg. you could write a different scheduler implementation. But:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who’d ever want to write a different scheduler implementation? I'm using an opensource project to avoid writing my own ETL system.&lt;/li&gt;
&lt;li&gt;You can change the code in other architectures as well as long as it’s modularized.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moving on, the other daemon modules are the scheduler, the worker and the UI web server. The scheduler performs calendar scheduling of workflows. The workers actually execute individual jobs.&lt;/p&gt;
&lt;p&gt;An important piece of the Pinball architecture are tokens. Tokens are basically records, and the collection of all tokens is the system state. Unfortunately the different sort of tokens are not documented, and since Python is dynamic, there’s also no usable documentation in the code (eg. a header file in C++). Tokens have a &lt;code&gt;data&lt;/code&gt; member, and Python objects are pickled and stored there on the fly as the state.
At first when I read the blog posts and code, I saw &lt;a href="https://github.com/pinterest/pinball/blob/master/pinball_system.png"&gt;this diagram&lt;/a&gt; and &lt;a href="https://engineering.pinterest.com/sites/default/files/article/fields/field_image/tumblr_inline_mzxiegqh5c1s1gqll.png"&gt;then this&lt;/a&gt;, and I thought that only the master accesses the database, and the scheduler and workers don’t, everything goes through the master using tokens. But actually that’s not true, I think the architecture is  everybody accesses the database for reads (as an optimization), but only the master writes to the database. This seems like a leaky abstraction, and again it’s not clear why the modules can’t use the DB to communicate state, why the need for Thrift. Relevant parts from the &lt;a href="https://engineering.pinterest.com/blog/pinball-building-workflow-management"&gt;blog post&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Every state (token) change goes through the master and gets committed to the persistent store before the worker request returns… workers can read archived tokens directly from the persistent storage, bypassing the master, greatly improving system scalability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An interesting design decision is separation of workflow description, which is given in Python dictionaries, and the actual job codes. &lt;a href="https://github.com/pinterest/pinball/tree/master/pinball_ext/examples"&gt;See example here.&lt;/a&gt; It’s a bit wierd that the workflow references the actual job using a string. I think this is because many modules load the workflow (eg. scheduler), but only the workers actually load the jobs.&lt;/p&gt;
&lt;h2&gt;Contrib stuff&lt;/h2&gt;
&lt;p&gt;Pinball has contrib stuff for the following job types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;S3 (also EMR)&lt;/li&gt;
&lt;li&gt;Hadoop, Hive&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.qubole.com/"&gt;Qubole&lt;/a&gt; (a data processing platform-as-a-service Pinterest uses)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are no connectors to Postgres, Mysql, Redshift, Presto or any SQL databases.&lt;/p&gt;
&lt;h2&gt;Source code and tests&lt;/h2&gt;
&lt;p&gt;The main codebase is ~18,000 LOC (python), plus about ~7,000 lines of unit test code. Other Python libraries used on the server side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://thrift.apache.org/"&gt;Thrift&lt;/a&gt; for RPC&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tox.readthedocs.org/"&gt;Tox&lt;/a&gt; for testing&lt;/li&gt;
&lt;li&gt;and a few more…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think it’s cool that Pinball doesn’t have many library dependencies; for a Python project, it barely has any.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;If I had to build an ETL system from scratch today, I would not use Pinball. It’s not documented, not a lot of commits, can't find other users, and I'm suspicious of the architecture. I would use Airflow.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="pinball"></category></entry><entry><title>Airflow review</title><link href="/airflow.html" rel="alternate"></link><published>2016-01-06T00:00:00+01:00</published><updated>2016-01-06T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-01-06:/airflow.html</id><summary type="html">&lt;p&gt;Airflow is a workflow scheduler written by Airbnb. It supports defining tasks and dependencies as Python code, executing and scheduling them, and distributing tasks across worker nodes. It supports calendar scheduling (hourly/daily jobs, also visualized on the web dashboard), so it can be used as a starting point for traditional ETL. It has a nice web dashboard for seeing current and past task state, querying the history and making changes to metadata such as connection strings.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/airflow-main-view.png" alt="Airflow" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Airflow is a workflow scheduler written by Airbnb. It supports defining tasks and dependencies as Python code, executing and scheduling them, and distributing tasks across worker nodes. It supports calendar scheduling (hourly/daily jobs, also visualized on the web dashboard), so it can be used as a starting point for traditional ETL. It has a nice web dashboard for seeing current and past task state, querying the history and making changes to metadata such as connection strings.
I wrote this after my &lt;a href="/luigi.html"&gt;Luigi review&lt;/a&gt;, so I make comparisons to Luigi throughout the article.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: Airflow has come a long way since I wrote this. Also, I've been using Airflow in production at Fetchr for a while. Check out &lt;a href="http://bytepawn.com/fetchr-airflow.html"&gt;Building the Fetchr Data Science Infra on AWS with Presto and Airflow&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Airflow is designed to store and persist its state in a relational database such as Mysql or Postgresql. It uses &lt;a href="http://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt; for abstracting away the choice of and querying the database. As such much of the logic is implemented as database calls.
It would be fair to call the core of Airflow “an SQLAlchemy app”. This allows for very clean separation of high-level functionality, such as persisting the state itself (done by the database itself), and scheduling, web dashboard, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/airflow-main-view.png" alt="Airflow" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Similarly to Luigi, workflows are specified as a DAG of tasks in Python code. But there are many differences. Luigi knows that tasks operate on targets (datasets, files) and includes this abstraction; eg. it checks the existence of targets when deciding whether to run a task (if all output targets exists, there’s no need to run the task). This concept is missing from Airflow, it never checks for the existence of targets to decide whether to run a task. Like in Luigi, tasks depend on each other (and not on datasets). Unlike Luigi, Airflow supports the concept of calendar scheduling, ie. you can specify that a DAG should run every hour or every day, and the Airflow scheduler process will execute it. Unlike Luigi, Airflow supports shipping the task’s code around to different nodes using &lt;code&gt;pickle&lt;/code&gt;, ie. Python binary serialization.&lt;/p&gt;
&lt;p&gt;Airflow also has a webserver which shows dashboards and lets users edit metadata like connection strings to data sources. Since everything is stored in the database, the web server component of Airflow is an independent &lt;a href="http://gunicorn.org/"&gt;gunicorn&lt;/a&gt; process which reads and writes the database.&lt;/p&gt;
&lt;h2&gt;Execution&lt;/h2&gt;
&lt;p&gt;In Airflow, the unit of execution is a &lt;code&gt;Task&lt;/code&gt;. DAG’s are made up of tasks, one &lt;code&gt;.py&lt;/code&gt; file is a DAG. &lt;a href="http://pythonhosted.org/airflow/tutorial.html"&gt;See tutorial.&lt;/a&gt; Although you can tell Airflow to execute just one task, the common thing to do is to load a DAG, or all DAGs in a subdirectory. Airflow loads the &lt;code&gt;.py&lt;/code&gt; file and looks for instances of class &lt;code&gt;DAG&lt;/code&gt;. DAGs are identified by the textual &lt;code&gt;dag_id&lt;/code&gt; given to them in the &lt;code&gt;.py&lt;/code&gt; file. This is important, because this is used to identify the DAG (and it’s hourly/daily instances) throughout Airflow; changing the &lt;code&gt;dag_id&lt;/code&gt; will break dependencies in the state!&lt;/p&gt;
&lt;p&gt;The DAG contains the first date when these tasks should (have been) run (called &lt;code&gt;start_date&lt;/code&gt;), the recurrence interval if any (called &lt;code&gt;schedule_interval&lt;/code&gt;), and whether the subsequent runs should depend on each other (called &lt;code&gt;depends_on_past&lt;/code&gt;). Airflow will interleave slow running DAG instances, ie. it will start the next hour’s jobs even if the last hour hasn’t completed, as long as dependencies permit and overlap limits permit. An instance of a &lt;code&gt;DAG&lt;/code&gt;, eg. one that is running for 2016-01-01 06:00:00 is called a &lt;code&gt;DAGRun&lt;/code&gt;. A &lt;code&gt;DAGRun&lt;/code&gt; is identified by the id of the DAG postfixed by the &lt;code&gt;execution_date&lt;/code&gt; (not when it’s running, ie. not &lt;code&gt;now()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Tasks, like DAGs are also identified by a textual id. Internally, instances of tasks are instances of &lt;code&gt;TaskInstance&lt;/code&gt;, identified by the task’s &lt;code&gt;task_id&lt;/code&gt; plus the &lt;code&gt;execution_date&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The tasks in a DAG may define dependencies on other tasks using &lt;code&gt;set_upstream()&lt;/code&gt; and &lt;code&gt;set_downstream()&lt;/code&gt;. Airflow will raise an exception when it finds cycles in the DAG.&lt;/p&gt;
&lt;p&gt;A task is a parameterized operator. Airflow provides many types of operators, such as &lt;code&gt;BashOperator&lt;/code&gt; for executing a bash script, &lt;code&gt;HiveOperator&lt;/code&gt; for executing Hive queries, and so on. All these operators derive from &lt;code&gt;BaseOperator&lt;/code&gt;. In line with Airflow being “an SQLAlchemy app”, &lt;code&gt;BaseOperator&lt;/code&gt; is derived from SQLAlquemy's &lt;code&gt;Base&lt;/code&gt; class, so objects can be pushed to the database; this pattern happens throughout Airflow. Operators don’t actually contain the database specific API calls (eg. for Hive or Mysql); this logic is contained in hooks, eg. class &lt;code&gt;HiveCliHook&lt;/code&gt;. All hooks are derived from class &lt;code&gt;BaseHook&lt;/code&gt;, a common interface for connecting and executing queries. So, whereas Luigi has one &lt;code&gt;Target&lt;/code&gt; class (and subclasses), in Airflow this logic is distributed into operators and hooks.&lt;/p&gt;
&lt;p&gt;There are 3 main type of operators (all three use the same hook classes to accomplish their job):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sensor:&lt;/strong&gt; Waits for events to happen. This could be a file appearing in HDFS, the existence of a Hive partition, or waiting for an arbitrary MySQL query to return a row.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Remote Execution:&lt;/strong&gt; Triggers an operation in a remote system. This could be an HQL statement in Hive, a Pig script, a map reduce job, a stored procedure in Oracle or a Bash script to run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data transfers:&lt;/strong&gt; Move data from one system to another. Push data from Hive to MySQL, from a local file to HDFS, from Postgres to Oracle, or anything of that nature.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most interesting are sensors. They allow tasks to depend on special “sensor tasks”, which are actually files or datasets. A sensor let’s you specify how often it should be checked (default 1 minute), and when it should time out (default 1 week). These are all derived from class &lt;code&gt;BaseSensorOperator&lt;/code&gt;. There is a special sensor called &lt;code&gt;ExternalTaskSensor&lt;/code&gt;, which lets a task depend on another task (specified by a &lt;code&gt;dag_id&lt;/code&gt; and a &lt;code&gt;task_id&lt;/code&gt; and &lt;code&gt;execution_date&lt;/code&gt;) in another DAG, since this is not supported “by default”. &lt;code&gt;ExternalTaskSensor&lt;/code&gt; actually just checks what the specified record looks like in the Airflow state database.&lt;/p&gt;
&lt;p&gt;All operators have a &lt;code&gt;trigger_rule&lt;/code&gt; argument which defines the rule by which the generated task get triggered. The default value for &lt;code&gt;trigger_rule&lt;/code&gt; is &lt;code&gt;all_success&lt;/code&gt; and can be defined as “trigger this task when all directly upstream tasks have succeeded. Others are: &lt;code&gt;all_failed&lt;/code&gt;, &lt;code&gt;all_done&lt;/code&gt;, &lt;code&gt;one_failed&lt;/code&gt;, &lt;code&gt;one_success&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Scheduling and executors&lt;/h2&gt;
&lt;p&gt;Recap: Airflow supports calendar scheduling (hour/daily tasks). Each such run is an instance of a DAG (internally, a &lt;code&gt;DAGRun&lt;/code&gt; object), with tasks and their dependencies. As mentioned previously, DAGs can depend on their previous runs (&lt;code&gt;depends_on_past&lt;/code&gt;), and additionally, specific task dependencies across DAGs is possible with the &lt;code&gt;ExternalTaskSensor&lt;/code&gt; operator. The maximum number of DAG runs to allow per DAG can be limited with &lt;code&gt;max_active_runs_per_dag&lt;/code&gt; in &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When running Airflow, we have to specify what sort of executor to use in &lt;code&gt;airflow.cfg&lt;/code&gt;: &lt;code&gt;SequentialExecutor&lt;/code&gt;, &lt;code&gt;LocalExecutor&lt;/code&gt; or &lt;code&gt;CeleryExecutor&lt;/code&gt;; all three derive from &lt;code&gt;BaseExecutor&lt;/code&gt;. The sequential executor runs locally in a single process/thread, and waits for each task to finish before starting the next one; it should only be used for testing/debugging. The &lt;code&gt;LocalExecutor&lt;/code&gt; also runs tasks locally, but spawns a new process for each one using &lt;code&gt;subprocess.popen()&lt;/code&gt; to run a new &lt;code&gt;bash&lt;/code&gt;; the maximum number of processes can be configured with &lt;code&gt;parallelism&lt;/code&gt; in &lt;code&gt;airflow.cfg&lt;/code&gt;. Inside the &lt;code&gt;bash&lt;/code&gt;, it runs an &lt;code&gt;airflow&lt;/code&gt;, parameterized to just run the a given &lt;code&gt;dag_id&lt;/code&gt; &lt;code&gt;task_id&lt;/code&gt; &lt;code&gt;execution_date&lt;/code&gt; combination using the &lt;code&gt;airflow&lt;/code&gt; run command line parametrization. The python code belonging to the task is read back from the database (where it was stored by the scheduler using &lt;code&gt;pickle&lt;/code&gt;). The &lt;code&gt;CeleryExecutor&lt;/code&gt; works similarly, except the job is pushed inside a distributed &lt;a href="http://www.celeryproject.org/"&gt;celery&lt;/a&gt; queue.&lt;/p&gt;
&lt;p&gt;When running Airflow, internally a number of jobs are created. A job is a long running something that handles running  smaller units of work; all jobs derive from &lt;code&gt;BaseJob&lt;/code&gt;. There is &lt;code&gt;SchedulerJob&lt;/code&gt;, which manages a single DAG (creates DAG runs, task instances, manages priorities),  &lt;code&gt;BackfillJob&lt;/code&gt; for backfilling a specific DAG, and &lt;code&gt;LocalTaskJob&lt;/code&gt; when running a specific &lt;code&gt;dag_id&lt;/code&gt; &lt;code&gt;task_id&lt;/code&gt; &lt;code&gt;execution_date&lt;/code&gt; combination (as requested by the &lt;code&gt;LocalExecutor&lt;/code&gt; or the &lt;code&gt;CeleryExecutor&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;When running the airflow scheduler, the &lt;code&gt;SchedulerJob&lt;/code&gt; supports loading DAGs from a folder: in this case, new code added/changed is automatically detected and loaded. This is very convenient, because new code just has to be placed on the production server, and it’s automatically picked up by Airflow.&lt;/p&gt;
&lt;p&gt;So in Airflow there is no need to start worker processes: workers are spawned as subprocesses by the &lt;code&gt;LocalExecutor&lt;/code&gt; or remotely by celery. Also, more than one scheduler/executor/main process can run, sitting on the main database. When running tasks, Airflow creates a lock in the database to make sure tasks aren’t run twice by schedulers; other parallelism is enforced by unique database keys (eg. only one &lt;code&gt;dag_id&lt;/code&gt; &lt;code&gt;execution_date&lt;/code&gt; combination allowed to avoid schedulers creating multiple &lt;code&gt;DAGRun&lt;/code&gt; copies). &lt;em&gt;Note: I’m not sure what the point would be of running several schedulers, other than redundancy, and whether this truly works without hiccups; the TODO file includes this todo item: “Distributed scheduler”.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Airflow supports pools to limit parallelism of certain types of tasks (eg. limit number of bash jobs, limit number of Hive connections); this is similar to Luigi resources. Priorities are also supported: The default &lt;code&gt;priority_weight&lt;/code&gt; is 1, and can be bumped to any number. When sorting the queue to evaluate which task should be executed next, Airflow uses the &lt;code&gt;priority_weight&lt;/code&gt;, summed up with all of the &lt;code&gt;priority_weight&lt;/code&gt; values from tasks downstream from this task.&lt;/p&gt;
&lt;p&gt;Airflow supports heartbeats. Each job will update a heartbeat entry in the database. If a job hasn’t updated it’s heartbeat for a while, it’s assumed that it has failed and it’s state is set to &lt;code&gt;SHUTDOWN&lt;/code&gt; in the database. This also allows for any job to be killed externally, regardless of who is running it or on which machine it is running. &lt;em&gt;Note: I’m not sure how this works, because from my reading of the code, the actual termination of the process that didn’t send the heartbeat should be performed by the process itself; but if it stuck or blocked and didn’t send a heartbeat, then how will it notice it should shut itself down?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Other interesting features&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SLAs:&lt;/strong&gt; Service Level Agreements, or time by which a task or DAG should have succeeded, can be set at a task level as a timedelta. If one or many instances have not succeeded by that time, an alert email is sent detailing the list of tasks that missed their SLA. The event is also recorded in the database and made available in the web UI under Browse -&amp;gt; Missed SLAs where events can be analyzed and documented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;XCom:&lt;/strong&gt; XComs let tasks exchange messages, allowing more nuanced forms of control and shared state. The name is an abbreviation of “cross-communication”. XComs are principally defined by a key, value, and timestamp, but also track attributes like the task/DAG that created the XCom and when it should become visible. Any object that can be pickled can be used as an XCom value, so users should make sure to use objects of appropriate size. XComs can be “pushed” (sent) or “pulled” (received). When a task pushes an XCom, it makes it generally available to other tasks. Tasks can push XComs at any time by calling the &lt;code&gt;xcom_push()&lt;/code&gt; method. In addition, if a task returns a value (either from its Operator’s &lt;code&gt;execute()&lt;/code&gt; method, or from a &lt;code&gt;PythonOperator&lt;/code&gt;’s &lt;code&gt;python_callable()&lt;/code&gt; function), then an XCom containing that value is automatically pushed. Tasks call &lt;code&gt;xcom_pull()&lt;/code&gt; to retrieve XComs, optionally applying filters based on criteria like key, source &lt;code&gt;task_id&lt;/code&gt;s, and source &lt;code&gt;dag_id&lt;/code&gt;. By default, &lt;code&gt;xcom_pull()&lt;/code&gt; filters for the keys that are automatically given to XComs when they are pushed by being returned from execute functions (as opposed to XComs that are pushed manually).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variables:&lt;/strong&gt; Variables are a generic way to store and retrieve arbitrary content or settings as a simple key value store within Airflow. Variables can be listed, created, updated and deleted from the UI (Admin -&amp;gt; Variables) or from code. While your pipeline code definition and most of your constants and variables should be defined in code and stored in source control, it can be useful to have some variables or configuration items accessible and modifiable through the UI.&lt;/p&gt;
&lt;h2&gt;Contrib stuff&lt;/h2&gt;
&lt;p&gt;Like Luigi, Airflow has an impressive library of stock operator classes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash&lt;/li&gt;
&lt;li&gt;Mysql&lt;/li&gt;
&lt;li&gt;Postgresql&lt;/li&gt;
&lt;li&gt;MSSQL&lt;/li&gt;
&lt;li&gt;Hive&lt;/li&gt;
&lt;li&gt;Presto&lt;/li&gt;
&lt;li&gt;HDFS&lt;/li&gt;
&lt;li&gt;S3&lt;/li&gt;
&lt;li&gt;HTTP sensor&lt;/li&gt;
&lt;li&gt;and many more...
Redshift is currently not supported.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Source code and tests&lt;/h2&gt;
&lt;p&gt;The main codebase is ~21,000 LOC (python, js, html), plus  about ~1,200 lines of unit test code.
Other Python libraries used on the server side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jinja.pocoo.org/"&gt;Jinja&lt;/a&gt; for templating (why, if we’re using Python code to define jobs anyway?)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gunicorn.org/"&gt;Gunicorn&lt;/a&gt; and &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt; for HTTP&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.python.org/pypi/dill"&gt;Dill&lt;/a&gt; for pickling&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tox.readthedocs.org"&gt;Tox&lt;/a&gt; for testing
and many more...&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Airflow’s design decisions are very close to my heart: the fact that it’s an SQLAlchemy app make managing state, restarting the daemon, or running more in parallel very easy.  It has lots of contrib stuff baked in, so it’s easy to get started. The dashboard is very nice, and also shows historic runs nicely color-coded. If I were to build a new ETL system, I would definitely consider using Airflow (over Luigi, since Airflow has many more features out of the box).&lt;/p&gt;
&lt;p&gt;What I don’t like about Airflow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Apart from special sensor operators, doesn’t deal with files/datasets as inputs/outputs of tasks directly. This I find an odd design decision, as it leads to some complications:&lt;ul&gt;
&lt;li&gt;The state database stores the state of tasks, not the datasets; if the state database is lost, it’s hard to restore the historic state of the ETL, even if all the datasets are there. It’s better to separate datasets and tasks, and represent the historic state of ETL using the state of the datasets&lt;/li&gt;
&lt;li&gt;It’s harder to deal with tasks that appear to finish correctly, but don’t actually produce output, or good output. In the Airflow architecture this problem only shows up later, when a task downstream (hopefully) errors out. This can happen eg. if a bash script forgets to set -e.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I think it’d be better if workers could be started independently, and picked up tasks scheduled by a central scheduler; instead Airflow starts workers centrally.&lt;/li&gt;
&lt;li&gt;Still a work in progress, not many tests, probably will run into bugs in production. Also see the end of &lt;a href="https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb#.lzfjq4wx9"&gt;this blog post&lt;/a&gt;, they restart the Airflow process pretty often because of some bug.&lt;/li&gt;
&lt;li&gt;Personally, I'm still not convinced that the ETL-job-as-code is the right way to go.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Links, talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/airbnb/airflow"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://airflow.readthedocs.org"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/airbnb/airflow#links"&gt;Slides from Airflow users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="airflow"></category></entry><entry><title>Luigi review</title><link href="/luigi.html" rel="alternate"></link><published>2015-12-20T00:00:00+01:00</published><updated>2015-12-22T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2015-12-20:/luigi.html</id><summary type="html">&lt;p&gt;I review Luigi, an execution framework for writing data pipes in Python code. It supports task-task dependencies, it has a simple central scheduler with an HTTP API and an extensive library of helpers for building data pipes for Hadoop, AWS, Mysql etc. It was written by Spotify for internal use and open sourced in 2012. A number of companies use it, such as Foursquare, Stripe, Asana.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Luigi is an execution framework for writing data pipes in Python code. It supports task-task dependencies, it has a simple central scheduler with an HTTP API and an extensive library of helpers for building data pipes for Hadoop, AWS, Mysql etc. It was written by Spotify for internal use and open sourced in 2012. A number of companies use it, such as Foursquare, Stripe, Asana.&lt;/p&gt;
&lt;h2&gt;Execution&lt;/h2&gt;
&lt;p&gt;Suppose that part of your ETL process is to take some data A, apply transformation X on it, and save it as Y. In Luigi, you would write a &lt;code&gt;.py&lt;/code&gt; file which contains a class X, which derives from class &lt;code&gt;Task&lt;/code&gt;. X would have three methods: &lt;code&gt;requires(), run(), and output()&lt;/code&gt;.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;requires&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Then you execute luigi and pass this &lt;code&gt;.py&lt;/code&gt; file to it, like &lt;code&gt;luigi --module x X&lt;/code&gt; if the file name is &lt;code&gt;x.py&lt;/code&gt;. When given a &lt;code&gt;Task&lt;/code&gt;, luigi:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Calls the &lt;code&gt;output()&lt;/code&gt; method, which returns one or more objects deriving from class &lt;code&gt;Target&lt;/code&gt;. A &lt;code&gt;Target&lt;/code&gt; is something which has an &lt;code&gt;exists()&lt;/code&gt; method which returns either &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt;. Luigi calls &lt;code&gt;exists()&lt;/code&gt; on all the targets to see whether they exist. If all return &lt;code&gt;True&lt;/code&gt;, luigi will flag this task as &lt;code&gt;DONE&lt;/code&gt; and never call &lt;code&gt;run()&lt;/code&gt;.
   If at least one of the output targets returned &lt;code&gt;False&lt;/code&gt;, this job needs to be run.&lt;/li&gt;
&lt;li&gt;Luigi then calls the &lt;code&gt;requires()&lt;/code&gt; method to see what other tasks need to first run for this task to run successfully. &lt;code&gt;requires()&lt;/code&gt; returns one or more objects deriving from class &lt;code&gt;Task&lt;/code&gt;, and recursively performs this process for all those.
   Note: after returning, luigi checks whether the output targets of the required tasks really exists. This is encapsulated in the &lt;code&gt;complete()&lt;/code&gt; method, the default implementation just calls &lt;code&gt;exists()&lt;/code&gt; on all targets returned by &lt;code&gt;output()&lt;/code&gt;; the method can optionally be overridden in the derived &lt;code&gt;Target&lt;/code&gt; class. The purpose of &lt;code&gt;complete()&lt;/code&gt; is to make sure &lt;code&gt;run()&lt;/code&gt; was successful, because if a required target’s &lt;code&gt;run()&lt;/code&gt; didn’t raise a Python exception but didn’t actually produce the output needed, then &lt;code&gt;run()&lt;/code&gt; shouldn’t be called. In this case the required task is re-run.&lt;/li&gt;
&lt;li&gt;Luigi calls the &lt;code&gt;run()&lt;/code&gt; method and sets the task status to &lt;code&gt;DONE&lt;/code&gt; if no Python exceptions were raised.
   Note: &lt;code&gt;run()&lt;/code&gt; can also dynamically &lt;code&gt;yield&lt;/code&gt; dependencies tasks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Local and central scheduler&lt;/h2&gt;
&lt;p&gt;When luigi is launched and a task is given to it a &lt;code&gt;Worker&lt;/code&gt; object is created. Workers need to talk to a &lt;code&gt;Scheduler&lt;/code&gt;, which manages the dependency graph of tasks and tells workers what to do. So when the local worker object is created, it can either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a local scheduler in the process, or&lt;/li&gt;
&lt;li&gt;Connect to a remote scheduler using the HTTP API. This is the default.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Local scheduler:&lt;/em&gt; The local scheduler can be used by passing &lt;code&gt;--local-scheduler&lt;/code&gt; to the luigi runtime. When running with the local scheduler, the algorithm given above is run recursively, and then luigi exits. This is usually only used for testing and debugging purposes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Central scheduler:&lt;/em&gt; More interesting is the central scheduler. The central scheduler is a separate &lt;code&gt;luigid&lt;/code&gt; Python Tornado app that workers can talk to over HTTP. It performs two tasks: scheduling of tasks based on the dependency graph and serving a simple web dashboard on port 8082 (default). Note that the central scheduler:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Doesn’t see or execute the &lt;code&gt;Task&lt;/code&gt; object's code, hence it never sees or checks whether targets exist; this is always performed by workers.&lt;/li&gt;
&lt;li&gt;The task is identified by its signature:&lt;ul&gt;
&lt;li&gt;Python name of the class; in the example above it’s X.&lt;/li&gt;
&lt;li&gt;The values of the parameters passed to the task, eg. &lt;code&gt;day=2015-12-01&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Parameters are member variables in the &lt;code&gt;Task&lt;/code&gt; objects which derive from class &lt;code&gt;Parameter&lt;/code&gt;, eg.:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DateParameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;By specifying &lt;code&gt;significant=False&lt;/code&gt; in the &lt;code&gt;Parameter&lt;/code&gt; constructor, we can tell Luigi not to treat it as part of the task signature.&lt;/p&gt;
&lt;p&gt;The worker builds the local dependency graph and then uploads it to the central scheduler. Then it asks the central scheduler what it should do. The central scheduler potentially receives dependency graphs from several workers, and merges them, assuming tasks with the same name (and parameter values) uploaded from different workers are the same (generate the same &lt;code&gt;output()&lt;/code&gt; targets, contain the same &lt;code&gt;run()&lt;/code&gt; logic, etc).&lt;/p&gt;
&lt;p&gt;Given the dependency graph, the central scheduler then tells workers to start running tasks. A worker can only run tasks that it uploaded to the central scheduler, because those are the tasks that that Python process loaded. So workers are not generic workers, they can only work on the tasks that they were started with!&lt;/p&gt;
&lt;p&gt;Given a dependency graph, the scheduler will tell workers to run tasks that have no dependencies. By default, the order is non-deterministic. However, tasks can specify a priority, tasks with higher priority run first. The default priority is 0. Example:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;priority&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;something&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Because priorities are in code, the worker must evaluate them and pass it on to the central scheduler.&lt;/p&gt;
&lt;h2&gt;Local parallelism&lt;/h2&gt;
&lt;p&gt;More than 1 worker thread can be created by passing &lt;code&gt;--workers N&lt;/code&gt; to luigi. This is registered to the central scheduler, and if possible N tasks are run in parallel by one worker.
So there are multiple levels of parallelism in Luigi:
1. Multiple workers
2. Multiple threads in workers
3. Each task can have further parallelism, eg. a Hadoop MapReduce job.&lt;/p&gt;
&lt;h2&gt;Managing a library of tasks&lt;/h2&gt;
&lt;p&gt;What if we’re managing a library of 100s or 1000s of ETL jobs? While I haven’t used Luigi for this, it seems that the basic building blocks are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python &lt;code&gt;import&lt;/code&gt; statements: our jobs are distributed into different &lt;code&gt;.py&lt;/code&gt; files, so we need to &lt;code&gt;import&lt;/code&gt; them to use them in &lt;code&gt;requires()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WrapperTask&lt;/code&gt; objects: these are special sink tasks which don’t have an output, they just require other tasks to be run.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This part puts a lot of work on the user of Luigi:
1. If we create a new task and forget to add it to the sink task, it won’t be executed (unless it’s a dependency for something else).
2. If we refactor a job (eg. rename the task class, change parameters), we have to search and replace all references in subsequent &lt;code&gt;requires()&lt;/code&gt; methods. Since Python isn’t a statically typed language, this has to be done by hand.
3. If running workers on separate machines, it’s our job to synchronize the library of &lt;code&gt;.py&lt;/code&gt; files (eg. using &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;cron&lt;/code&gt; jobs to sync very often). Different versions of tasks with different logic or local, uncommitted changes propagating to the central scheduler will lead to hard to find bugs and data corruption.&lt;/p&gt;
&lt;h2&gt;Date parameters&lt;/h2&gt;
&lt;p&gt;In an ETL system, most tasks will have a date(time) parameter which tells the code which day/hour to run the scripts for. For example, a Daily Active User (DAU) script computes the number of unique DAUs for a given day. Because this is such a common use-case, Luigi has a number of helper classes for dealing with date parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateParameter"&gt;DateParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.MonthParameter"&gt;MonthParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.YearParameter"&gt;YearParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateHourParameter"&gt;DateHourParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateMinuteParameter"&gt;DateMinuteParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateIntervalParameter"&gt;DateIntervalParameter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Often tasks have to be re-run for a number of days. One way to do this is to call luigi repeatedly from the command line. Or we can use the built in &lt;code&gt;RangeDailyBase&lt;/code&gt; (also &lt;code&gt;RangeHourlyBase&lt;/code&gt;) helpers:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# instead of calling this repeatedly:&lt;/span&gt;
    &lt;span class="c1"&gt;# luigi task Task --date 2015-01-XX&lt;/span&gt;
&lt;span class="c1"&gt;# do this:&lt;/span&gt;
$ luigi --module task RangeDailyBase --of Task --start &lt;span class="m"&gt;2015&lt;/span&gt;-01-01 --stop &lt;span class="m"&gt;2015&lt;/span&gt;-01-31
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;The name of the date parameter of the task can be specified with &lt;code&gt;--param_name==&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When we pass in a large number of dates (as an interval), the &lt;code&gt;RangeXBase&lt;/code&gt; classes will instantiate a task object for each date and call &lt;code&gt;complete()&lt;/code&gt; to check whether that task needs to be run. This can be very slow, eg. if each one creates a database connection and then closes it down.
There are two optimization classes &lt;code&gt;RangeDaily&lt;/code&gt; and &lt;code&gt;RangeHourly&lt;/code&gt; that solve this problem. These are used just like the two &lt;code&gt;Base&lt;/code&gt; versions from the command line. But instead of instantiating many tasks which potentially don’t have to be run, they assume and call the task’s &lt;code&gt;bulk_complete()&lt;/code&gt; classmethod to get a list of dates which have to be run. So the user has to implement a &lt;code&gt;bulk_complete()&lt;/code&gt; to use &lt;code&gt;RangeDaily&lt;/code&gt; and &lt;code&gt;RangeHourly&lt;/code&gt;.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ luigi --module task RangeDaily --of Task --start &lt;span class="m"&gt;2015&lt;/span&gt;-01-01 --stop &lt;span class="m"&gt;2015&lt;/span&gt;-01-31
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Note: it seems Luigi doesn’t support bulk &lt;em&gt;running&lt;/em&gt; of parameter intervals.&lt;/p&gt;
&lt;h2&gt;Scheduling&lt;/h2&gt;
&lt;p&gt;Most ETL systems have jobs which need to run every hour or every day. Luigi doesn’t have a concept of calendar scheduling, this is up to the user. The recommended method by the authors is to create sink tasks and run them from &lt;code&gt;cron&lt;/code&gt; when the external input files (eg. raw log files) are likely to be available.&lt;/p&gt;
&lt;p&gt;Rescheduling failed tasks is influenced by the following parameters in the central scheduler’s &lt;code&gt;luigi.cfg&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;retry-delay&lt;/code&gt;: when to re-schedule, default 900 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;remove-delay&lt;/code&gt;: how long the central scheduler keeps tasks around that have no stakeholder; a stakeholder is a worker who uploaded that task&lt;/li&gt;
&lt;li&gt;&lt;code&gt;disable-hard-timeout&lt;/code&gt;: if a task fails again after this much time, it is disabled for good&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the worker’s &lt;code&gt;luigi.cfg&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;worker-keep-alive&lt;/code&gt;: you probably need to set this to true, so workers will stay alive when they run out of jobs to run, as long as they have some pending job waiting to be run. Otherwise workers will disconnect from the central scheduler and exit if there’s nothing to do, even if there are tasks which will be scheduled a few minutes from now.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;retry-external-tasks&lt;/code&gt;: If true, incomplete external tasks (i.e. tasks where the &lt;code&gt;run()&lt;/code&gt; method is &lt;code&gt;NotImplemented&lt;/code&gt;) will be retested for completion while Luigi is running. This means that if external dependencies are satisfied after a workflow has started, any tasks dependent on that resource will be eligible for running.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The central scheduler has a feature called task history. This logs task completion to a database, and exposes it on the dashboard.&lt;/p&gt;
&lt;p&gt;For tasks where the output is a database table, Luigi needs to keep track of successful inserts. It uses a special marker table for this (set with &lt;code&gt;marker-table&lt;/code&gt; in &lt;code&gt;luigi.cfg&lt;/code&gt;, default name is &lt;code&gt;table_updates&lt;/code&gt;). When a task finishes whose target is a database table, an entry is created in the marker table with the task’s &lt;code&gt;task_id&lt;/code&gt; (its name and parameter values). When the target’s &lt;code&gt;exists()&lt;/code&gt; method is called, this marker table is queried to check whether the task has been run (the &lt;code&gt;task_id&lt;/code&gt; is passed by the task to the &lt;code&gt;Target&lt;/code&gt; in its constructor).&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;p&gt;Resources can be used to introduce limits on task parallelism. For example, suppose we never want to run more than 10 mysql tasks, or we never want to run more than 3 instances of the hourly job &lt;code&gt;count_users&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Resources are declared in the &lt;code&gt;luigi.cfg&lt;/code&gt; file of the scheduler:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;count_users&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Resource use is given in the resources property of the task object in the Python code, like:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;mysql&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;# using 2 mysql connections in this task&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;h2&gt;Contrib stuff&lt;/h2&gt;
&lt;p&gt;Luigi has an impressive library of stock &lt;code&gt;Target&lt;/code&gt; and &lt;code&gt;Task&lt;/code&gt; classes, each with lots of functionality baked in as helper methods. This is the big reason why I think Luigi is popular and why I would consider using it.&lt;/p&gt;
&lt;p&gt;Luigi has &lt;code&gt;Task&lt;/code&gt; and &lt;code&gt;Target&lt;/code&gt; classes which support:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Bigquery&lt;/li&gt;
&lt;li&gt;Hadoop jobs&lt;/li&gt;
&lt;li&gt;Hive queries&lt;/li&gt;
&lt;li&gt;Pig queries&lt;/li&gt;
&lt;li&gt;Scalding jobs&lt;/li&gt;
&lt;li&gt;Spark jobs&lt;/li&gt;
&lt;li&gt;Postgresql, Redshift, Mysql tables&lt;/li&gt;
&lt;li&gt;and more… &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Source code and tests&lt;/h2&gt;
&lt;p&gt;I spent a fair amount of time digging through the Luigi Python source code. It’s pretty clean Python code with a lot of tests. Code size is about 18KLOC plus 16KLOC tests. It’s pretty easy to understand and extend.&lt;/p&gt;
&lt;h2&gt;Sample cases&lt;/h2&gt;
&lt;p&gt;Trying it out on a free &lt;a href="http://c9.io"&gt;cloud9&lt;/a&gt; Docker instance:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install tornado &lt;span class="c1"&gt;# luigi uses the tornado web server&lt;/span&gt;
$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$PATH&lt;/span&gt;:/home/ubuntu/workspace/luigi/bin
$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/home/ubuntu/workspace/luigi:.
$ luigid
&lt;span class="m"&gt;2015&lt;/span&gt;-12-19 &lt;span class="m"&gt;14&lt;/span&gt;:18:08,492 luigi-interface&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;11022&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; INFO: Loaded &lt;span class="o"&gt;[]&lt;/span&gt;
&lt;span class="m"&gt;2015&lt;/span&gt;-12-19 &lt;span class="m"&gt;14&lt;/span&gt;:18:08,494 luigi.server&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;11022&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; INFO: No prior state file exists at /var/lib/luigi-server/state.pickle. Starting with clean slate
&lt;span class="m"&gt;2015&lt;/span&gt;-12-19 &lt;span class="m"&gt;14&lt;/span&gt;:18:08,497 luigi.server&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;11022&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; INFO: Scheduler starting up
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;In another terminal, this is the default Luigi sample to try:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; luigi/examples
$ luigi --module top_artists AggregateArtists --date-interval &lt;span class="m"&gt;2012&lt;/span&gt;-06
&lt;span class="c1"&gt;# does the job, creates files locally!&lt;/span&gt;
$ luigi --module top_artists AggregateArtists --date-interval &lt;span class="m"&gt;2012&lt;/span&gt;-06
&lt;span class="c1"&gt;# notices files are there, doesn’t do anything&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Let’s play around with Luigi. Let’s create this x.py:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;luigi&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;WrapperTask&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;task_namespace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;examples&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Running X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;requires&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;Bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;task_namespace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;examples&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;IntParameter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Bar &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Bar touched &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LocalTarget&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/tmp/bar/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;And run it like:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ luigi --module x examples.X
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;This will create an &lt;code&gt;X&lt;/code&gt; task and 10 &lt;code&gt;Bar&lt;/code&gt; tasks. The 10 &lt;code&gt;Bar&lt;/code&gt; tasks will touch &lt;code&gt;/tmp/bar/…&lt;/code&gt; and that’s it.
Let’s delete the tmp files, and create a similarly named y.py, with identical &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Bar&lt;/code&gt; tasks, except &lt;code&gt;X&lt;/code&gt; renamed to &lt;code&gt;Y&lt;/code&gt;. Let’s launch two workers, one with x and one with y. Notice that the central scheduler will merge the dependency graphs and treat the &lt;code&gt;Bar&lt;/code&gt; tasks coming from the different workers/codes as the same, because their &lt;code&gt;task_id&lt;/code&gt; (class name plus parameters) are identical. It’s a bit weird, but this is how Luigi works. Another thing you’ll notice is that at the end of the execution, one of &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; will be unfinished (not green on the dashboard). This is because the workers are run without &lt;code&gt;--worker-keep-alive&lt;/code&gt;. So the first worker who finishes its tasks and is waiting for the other worker to finish the last &lt;code&gt;Bar&lt;/code&gt; will exit (it’s got nothing to do). If that worker was eg. the x worker, then task &lt;code&gt;X&lt;/code&gt; is not going to be run by anyone! if we turn on &lt;code&gt;--worker-keep-alive&lt;/code&gt; in the command-line, this oddity goes away.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;When designing an ETL framework, I would make (and have made) different design decisions compared to Luigi. But if I were tasked with creating a new ETL framework from scratch (eg. at a new company), I would definitely consider using Luigi. There is simply too much useful stuff there to ignore (and re-implement).
&lt;strong&gt;However, I would expect to:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find unexpected and painful behaviour in Luigi.&lt;/li&gt;
&lt;li&gt;Write significant scaffolding code to make it useful:&lt;ol&gt;
&lt;li&gt;Syncing the task library to different workers&lt;/li&gt;
&lt;li&gt;Scheduling series of tasks&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;li&gt;Alerting&lt;/li&gt;
&lt;li&gt;Dashboard for the ETL datasets and jobs (see below)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Downsides of Luigi:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sometimes unexpected behaviour: for example, a wrapper task can reach &lt;code&gt;DONE&lt;/code&gt; status without ever running the &lt;code&gt;run()&lt;/code&gt; method depending on non-deterministic execution order.&lt;/li&gt;
&lt;li&gt;The biggest downside to Luigi is that ETL jobs are specified as programmatic Python Task objects and not given is some sort of DSL. This means no external tool can reasonably/easily parse a library of tasks and extract dependency information, which would be useful for eg. generating documentation of the ETL system. Also, analysts have to learn Python.&lt;/li&gt;
&lt;li&gt;The web dashboard of the central scheduler is basically useless.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Links, talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/luigi"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/index.html"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/luigi#who-uses-luigi"&gt;Slides from Luigi users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="luigi"></category></entry><entry><title>Cargo Cult Data</title><link href="/cargo-cult-data.html" rel="alternate"></link><published>2015-01-26T00:00:00+01:00</published><updated>2015-12-22T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2015-01-26:/cargo-cult-data.html</id><summary type="html">&lt;p&gt;Cargo cult data is when you're collecting and looking at data when making decisions, but you're only following the forms and outside appearances of scientific investigation and missing the essentials, so it doesn't work.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Cargo cult science&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Richard_Feynman"&gt;R. P. Feynman&lt;/a&gt; was a Nobel-prize winning physicist who coined the term &lt;a href="https://en.wikipedia.org/wiki/Cargo_cult_science"&gt;cargo cult science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In Feynman's words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the South Seas there is a cargo cult of people. During the [second world] war they saw airplanes land with lots of good materials, and they want the same thing to happen now [after the Americans left]. So they've arranged to imitate things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas—he's the controller—and they wait for the airplanes to land. They're doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn't work. No airplanes land. So I call these things cargo cult science, because they follow all the apparent precepts and forms of scientific investigation, but they're missing something essential, because the planes don't land.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Feynman cautioned that to avoid becoming cargo cult scientists, researchers must avoid fooling themselves, be willing to question and doubt their own theories and their own results, and investigate possible flaws in a theory or an experiment. He recommended that researchers adopt an unusually high level of honesty which is rarely encountered in everyday life, and gave examples from advertising, politics, and behavioral psychology to illustrate the everyday dishonesty which should be unacceptable in science.&lt;/p&gt;
&lt;h2&gt;Cargo cult data&lt;/h2&gt;
&lt;p&gt;The same idea applies to data. Cargo cult data is when you're collecting and looking at data when making decisions, but you're only following the forms of scientific investigation and missing the essentials, so it doesn't work. &lt;em&gt;So in the end you're like the natives of the South Seas, and the planes don't land for you either.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Signs that you're doing cargo cult data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you don't have standardized logging across your products&lt;/li&gt;
&lt;li&gt;you routinely break your logging and have holes in your dataset&lt;/li&gt;
&lt;li&gt;you don't have standardized KPIs across your products and company&lt;/li&gt;
&lt;li&gt;you're not A/B testing all your releases&lt;/li&gt;
&lt;li&gt;you don't have explicit hypothesis for your experiments&lt;/li&gt;
&lt;li&gt;you don't know what statistical power is&lt;/li&gt;
&lt;li&gt;you confuse statistical significance and magnitude of change&lt;/li&gt;
&lt;li&gt;you're using online forms to evaluate A/B tests&lt;/li&gt;
&lt;li&gt;you stop A/B tests as soon as they're statistically significant (=peeking)&lt;/li&gt;
&lt;li&gt;you're not tracking your experiments and their outcomes historically&lt;/li&gt;
&lt;li&gt;you don't know what standard deviation (=confuse signal and noise)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are no easy answers how to avoid cargo cult data, just as there are no easy answers how to avoid cargo cult science. If you are thinking about this as a company, your best bet is to hire smart mathematicians or physicist for your data team and listen to what they say. Personally, it's a matter of understanding statistics and being disciplined in your work. Fortunately there are &lt;a href="https://www.coursera.org/specializations/jhu-data-science"&gt;great courses on Coursera&lt;/a&gt;, &lt;a href="http://www.amazon.com/s/ref=dp_byline_sr_book_1?ie=UTF8&amp;amp;text=Allen+B.+Downey&amp;amp;search-alias=books&amp;amp;field-author=Allen+B.+Downey&amp;amp;sort=relevancerank"&gt;great books on Amazon&lt;/a&gt; and a &lt;a href="https://en.wikipedia.org/wiki/A/B_testing"&gt;wealth of information available online&lt;/a&gt;.&lt;/p&gt;</content><category term="data"></category></entry></feed>