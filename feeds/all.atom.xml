<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bytepawn</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2018-08-14T00:00:00+02:00</updated><entry><title>Fetchr Data Science Infra at 1 year</title><link href="/fetchr-data-science-infra.html" rel="alternate"></link><published>2018-08-14T00:00:00+02:00</published><updated>2018-08-14T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-08-14:/fetchr-data-science-infra.html</id><summary type="html">&lt;p&gt;A description of our Analytics+ML cluster running on AWS, using Presto, Airflow and Superset.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is a quick follow-up to my &lt;a href="http://bytepawn.com/fetchr-airflow.html#fetchr-airflow"&gt;previous post describing the Fetchr Data Science infra and philosophy&lt;/a&gt;. The platform has doubled in the last 6 months, and I'm currently approaching the end of my first year at Fetchr, so it's a good time to post an update.&lt;/p&gt;
&lt;p&gt;The basic principles behind our infrastructure have not changed, but we have scaled it out horizontally in key areas. We have also added a small ML prediction cluster, which is already in production and having a big impact on on-the-ground operations. As of today, the Data Science infra is about 20 nodes and looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/fetchr-data-science-infra-update.png" alt="Fetchr Data Science Infra" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;Architecture:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;S3 buckets (all data lives here)&lt;/li&gt;
&lt;li&gt;two Presto clusters&lt;ul&gt;
&lt;li&gt;5 node Presto cluster for ETL and dashboards&lt;/li&gt;
&lt;li&gt;5 node Presto cluster for analytics queries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Airflow&lt;ul&gt;
&lt;li&gt;1 node for scheduler + webserver&lt;/li&gt;
&lt;li&gt;1 node for workers&lt;/li&gt;
&lt;li&gt;1 node for staging&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Superset&lt;ul&gt;
&lt;li&gt;1 node for dashboarding&lt;/li&gt;
&lt;li&gt;1 node for analytics queries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Jupyter host (Machine Learning notebooks)&lt;/li&gt;
&lt;li&gt;2 node ML prediction cluster (blue+green)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;S3&lt;/h2&gt;
&lt;p&gt;As before, all data lives on S3, whether it's data imported from our production databases or data produced by the ETL. Data imported is stored in flat CSV files, whereas DWH tables produced by Airflow running Presto jobs are stored in ORC format (like at Facebook). EMR/EC2 nodes never store data.&lt;/p&gt;
&lt;p&gt;We continue to use the &lt;code&gt;ds&lt;/code&gt; partitioned DWH architecture. This means that every night the ETL imports a fresh copy of all production tables into a new ds partition, like &lt;code&gt;ds=2018-08-01&lt;/code&gt;, and all subsequent tables are also re-created in a new partition. Because all tables are backed on S3, this is also mirrored in our S3 path hierarchy. For example, the backing files for our main &lt;code&gt;company_metrics&lt;/code&gt; table are divided like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/company_metrics_s3.png" alt="A table on S3." style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;When querying, we always specify the &lt;code&gt;ds&lt;/code&gt; like &lt;code&gt;WHERE ds='2018-08-13'&lt;/code&gt;, otherwise we're looking at multiple copies of the table. This is cumbersome, so for most of our tables, the Airflow jobs create a &lt;code&gt;xyz_latest&lt;/code&gt; view that just points to the latest partition, like &lt;code&gt;CREATE OR REPLACE VIEW xyz_latest AS SELECT * FROM xyz WHERE ds = '{{ ds }}'&lt;/code&gt;. This way analysts can usually just query the &lt;code&gt;_latest&lt;/code&gt; and not think about it.&lt;/p&gt;
&lt;p&gt;There are many upsides to this scheme: (i) if today's ETL fails, people can continue working with yesterday's data and (ii) since partitions are write-once, data never changes, so it's always easy to reproduce a number sent out 6 months ago (just run the query on that &lt;code&gt;ds&lt;/code&gt; partition, like &lt;code&gt;WHERE ds='2018-02-08'&lt;/code&gt;). The downside is that it's a lot of duplicate data, but with S3 being super-cheap this is a non-issue.&lt;/p&gt;
&lt;p&gt;Okay, but still, this is wasteful and slow in terms of ETL time? After all, we just imported all these tables last night, do we need to re-import the whole dataset again? As the company and our data volume grew, the nightly import actually started taking too long, so we were forced to optimize this: for our big tables, now we import the historic tail once a week on weekends, and on a daily basis we only import data on orders that we received in the last ~3 months. This ensures our ETL finishes on time every night.&lt;/p&gt;
&lt;h2&gt;Presto&lt;/h2&gt;
&lt;p&gt;We currently run two EMR+Presto clusters, each 5 nodes. As before we don't run any ETL or queries on Hive/MapReduce, we exclusively use Presto for compute, since our queries never touch more than 10-100M rows.&lt;/p&gt;
&lt;p&gt;We introduced a secondary cluster for ad-hoc analytics queries because, in cases when our ETL is slow and running during the day, or our regular hourly ETLs are running during the day, it kept blocking us from getting our work done.&lt;/p&gt;
&lt;p&gt;Since all our data lives in S3, having two clusters see (read and write) the same data is not very hard. All we had to do is make sure our schemas are in sync on the two clusters. Since 99% of our schema operations are managed through Airflow jobs (&lt;code&gt;CREATE TABLE IF NOT EXISTS ...&lt;/code&gt;), we just had to modify our Airflow framework to also execute these on the secondary cluster. Additionally, when we manually make changes to an existing table (&lt;code&gt;ALTER TABLE ...&lt;/code&gt;), we have to execute this on both clusters, which is an inconvenience, but a minor one, and quite managable at this scale.&lt;/p&gt;
&lt;h2&gt;Airflow&lt;/h2&gt;
&lt;p&gt;We continue to use Airflow to be the backbone of our data pipelines with great success. We have two nodes for production: one running the scheduler and webserver, and one running the worker processes. Since these nodes don't do any compute themselves (they just launch Presto &lt;code&gt;INSERT INTO ... SELECT&lt;/code&gt; jobs), we did not need to scale out here so far, nor do we expect this to happen in the next year.&lt;/p&gt;
&lt;p&gt;One the other hand, we have deepened our investment into Airflow as our standard ETL system wrt code. We have identified the 4-5 common Airflow use-cases we have (import from Postgres to S3, run an ETL job on Presto, export data to the BI team's Redshift DWH, create dashbord screenshots and send in email, run Growth Accounting) and we have created helper functions to encapsulate them. As a result, the vast majority of our Airflow DAGs don't create Airflow operators directly, instead they call these library functions, which construct and return the DAGs. As a result our Airflow jobs look very clean, with all of the messy complexity hidden away:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/airflow_code_example.png" alt="A table on S3." style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This had a big pay-off when we introduced our secondary Presto cluster a few months ago, and we needed to automtically create all our schemas there. We just added extra operators to our library functions to run schema operations on the secondary, and the next night when the ETL ran, all our table schemas were created on the secondary Presto cluster, pointing to the backing S3 files, ready to go. We were running analytics queries on the secondary cluster the day after we spun it up!&lt;/p&gt;
&lt;p&gt;Currently we have 76 DAGs in Airflow, importing and exporting from 5-10 data sources (3 production databases, S3, 2 Presto clusters, Redshift, DynamoDB, various custom extracts sent to clients and the ML cluster).&lt;/p&gt;
&lt;h2&gt;Superset&lt;/h2&gt;
&lt;p&gt;Superset is both a dashboarding system and has an SQL IDE (called SQL Lab), which our Data Scientists use as their primary tool for accessing data. We continue to use Superset for dashboarding and have spun up a secondary Superset instance just for queries.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/sql-lab.png" alt="Fetchr Data Science Infra" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;p&gt;We have hit a limitation with Superset, where Superset will launch a new gunicorn process for each page request, and if the page happens to be a dashboard, for each chart on the dashboard. The chart processes will launch a Presto query each, which could take 10-60 seconds to return. Some of our dashboards have a lot of charts on them (20+), plus we have many concurrent users accessing dashboards (or running queries). In cases like this, Superset runs out of worker processes, and it becomes totally unresponsive. Each worker process eats up 1-2G of RAM, so the number of processes it can run are limited.&lt;/p&gt;
&lt;p&gt;As an initial workaround, we split the dashboarding and querying use-case into two Superset instances, so analysts are not blocked by the dashboards. Additionally, we broke large dashboards into smaller pieces (which is unfortunate). This way, running 32 worker processes each, both instances are good for every-day work at current loads.&lt;/p&gt;
&lt;p&gt;Both Airflow and Superset are still rough around the edges, but since Superset is user-facing, it can create more problems. It still happens that we want to look at a dashboard but the webserver times out because Superset ran out worker processes (maybe because the Presto queries are slow, because they're running on the same cluster as the ETL, and the ETL is slow, because something changed in production). Right now we get by with work-arounds (for example, in the previous case, we re-direct the Superset dashboarding traffic to the analytics cluster temporarily by changing a connection string, until ETL finishes). So far Superset is good enough for internal Data Science / Understand dashboards, and we do have a fair number of colleagues using it on a daily basis for basic reporting. But admittedly we will need to invest more time into understanding how to tune if we want to deploy it to a company-wide 1000+ person audience and feel good about it.&lt;/p&gt;
&lt;p&gt;Currently we have 26 dashboards in Superset, many of them viewed by CxOs and country General Managers every day.&lt;/p&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;In the last 3 months we have rolled out a prediction model in production. We perform the Data Science work to arrive at the models on our laptops and/or on a dedicated Jupyter host, in ipython notebooks. Once we're happy with the result, we deploy the model to our ML prediction cluster. We don't yet have CI/CD set up for it, deployment is manual and requires domain knowledge.&lt;/p&gt;
&lt;p&gt;The model is already running in production and is being used for on-the-ground delivery operations, so downtime is not acceptable. We quickly switched to a 2 node blue/green model: both nodes are running identical code/data, we deploy to green first, if it goes well, then to blue. Both are behind an Elastic Load Balancer (&lt;code&gt;predict&lt;/code&gt; happens over a HTTP API call), so things keep going if one of them is down, even if down for a long time.&lt;/p&gt;
&lt;p&gt;Fresh data is loaded every night by an Airflow process: first it creates a daily dump for the ML models, uploads it to S3, and then triggers a special &lt;code&gt;load&lt;/code&gt; API call, first on the green, then the blue host. The Airflow job for blue depends on green, so if green fails, it won't touch blue, so production will not be impacted.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/model_dag.png" alt="A table on S3." style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;At this scale/complexity, this simple model works quite well and we have excellent uptime; we have more problems coming from software bugs than availability.&lt;/p&gt;
&lt;h2&gt;Impact and Conclusion&lt;/h2&gt;
&lt;p&gt;This post focused on what is usually called Data Infrastructure and Data Engineering, but actually to build and run this platform only took about 0.5-1 FTE average effort over time (though very senior FTEs). The rest of our time directly focused on operational and business impact by building metrics and dashboards, running ad-hoc analytics, building operational models for forecasting and sizing, building ML models, and most important of all, explaining it all to our colleagues so our work is adopted and has impact on the ground. It's interesting to note that just 5 years ago, in a similar scenario, good-enough open source tools like Airflow and Superset were not available, so we had to roll our own and ended up spending an order of magnitude more time on DI/DE work.&lt;/p&gt;
&lt;p&gt;Overall, in the last year, our small Data Science team was able to have dollar-measurable outsized impact on Fetchr by using data to understand and optimize operations and business processes. Today, the most important operational decisions are based on data and driven by Data Scientists, including sizing our fleets and warehouses, understanding their performance, and ML models optimizing and/or automating human labor.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="airflow"></category><category term="fetchr"></category><category term="model"></category><category term="ml"></category></entry><entry><title>What not to spend time on</title><link href="/what-not-to-spend-time-on.html" rel="alternate"></link><published>2018-07-23T00:00:00+02:00</published><updated>2018-07-23T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-07-23:/what-not-to-spend-time-on.html</id><summary type="html">&lt;p&gt;Warren Buffett says deciding what &lt;strong&gt;not&lt;/strong&gt; to spend time on is just as important as deciding what to spend time on.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A couple of years ago I read &lt;a href="http://www.euclidean.com/the-essays-of-warren-buffett-review-lessons-quotes/"&gt;Warren Buffett's books&lt;/a&gt; and some stories about him. One of the lessons stuck with me, it's something I think about regularly when deciding what to spend time on. Below is an excerpt from &lt;a href="https://jamesclear.com/buffett-focus"&gt;James Clear's website&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;The story of Mike Flint&lt;/h2&gt;
&lt;p&gt;Mike Flint was Buffett's personal airplane pilot for 10 years. (Flint has also flown four US Presidents, so I think we can safely say he is good at his job.) According to Flint, he was talking about his career priorities with Buffett when his boss asked the pilot to go through a 3-step exercise. Here's how it works…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;STEP 1:&lt;/strong&gt; Buffett started by asking Flint to write down his top 25 career goals. So, Flint took some time and wrote them down. (Note: you could also complete this exercise with goals for a shorter timeline. For example, write down the top 25 things you want to accomplish this week.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;STEP 2:&lt;/strong&gt; Then, Buffett asked Flint to review his list and circle his top 5 goals. Again, Flint took some time, made his way through the list, and eventually decided on his 5 most important goals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;STEP 3:&lt;/strong&gt; At this point, Flint had two lists. The 5 items he had circled were &lt;strong&gt;List A&lt;/strong&gt; and the 20 items he had not circled were &lt;strong&gt;List B&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Flint confirmed that he would start working on his top 5 goals right away. And that's when Buffett asked him about the second list, &lt;em&gt;“And what about the ones you didn't circle?”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/warren-buffett.jpg" alt="Warren Buffett" style="width: 600px;"/&gt;&lt;/p&gt;
&lt;p&gt;Flint replied, &lt;em&gt;“Well, the top 5 are my primary focus, but the other 20 come in a close second. They are still important so I’ll work on those intermittently as I see fit. They are not as urgent, but I still plan to give them a dedicated effort.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To which Buffett replied, &lt;em&gt;“No. You’ve got it wrong, Mike. Everything you didn’t circle just became your Avoid-At-All-Cost list. No matter what, these things get no attention from you until you’ve succeeded with your top 5.”&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;What I don't spend time on&lt;/h2&gt;
&lt;p&gt;I think this is great advice, it's something I think about regularly. I will list 2 things here that I decided not to spend time on after reading this Warren Buffett bit a few yeas ago.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quantum Field Theory&lt;/strong&gt; (and research-level physics in general). I have a degree in Physics, it's a second degree I got mostly for fun and curiousity. Getting it has been one of the greatest decisions of my life, learning and doing Physics has been continually paying off ever since than, both personally and for work (Computer Science + Physics = Data Science). As a physicist you learn the deep insights of your predecessors, but also that with certainty they are wrong in some sense, and over time their achievements will be "just" an approximation or a special case. So physicists are always learning and adjusting their perspective. Physicists also have a keen sense for measurements, statistics, errors, which is very useful when dealing with numbers in the real world.&lt;/p&gt;
&lt;p&gt;So after I got the Msc degree in Physics back in 2008, I started a Phd that I never finished because I went off to do a startup. Maybe it's because I never finished it, but for many years afterwards, I had this romantic notion and kept going back to Physics in my free time, reading papers, books. I once even had a yearly goal of writing a paper and submitting it to a journal. I actually did it, I wrote a short paper titled &lt;a href="https://arxiv.org/abs/1208.4990"&gt;Pure Lattice Gauge Theory in the Expanding Universe&lt;/a&gt;, and submitted it to the Physical Review Letters (PRL), but got rejected (most papers get rejected at first, &lt;em&gt;real&lt;/em&gt; scientists keep improving and submitting). Since I don't &lt;em&gt;really&lt;/em&gt; care about getting into a journal, I didn't spend any more time on it, now it just sits on Arxiv [1].&lt;/p&gt;
&lt;p&gt;So, when I decided not to spend time on Physics anymore, it was not a trivial thing to do. I really like to do Physics, even in my spare time. But at the end of the day, I'm not in Academia, so I don't actually have time to follow topic(s) of research like real scientists do and figure out how to contribute. Also, academia is pretty crowded, and research topics have become very specialized. It's not really a good investment of time for me; investing my time into things related to software/startups has much higher potential impact and payoff. I also stopped reading Physics blogs, all the drama around string theory is fun but ultimately just a distraction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Haskell.&lt;/strong&gt; I first came into contact with Haskell in 2013 when working at Prezi. There are a lot of things that are intesting about Haskell: it's a strongly typed, purely functional language with type classes, type inference, etc. You can do really cool things with Haskell, or so it seems at first. In 2013, before systems like &lt;a href="http://bytepawn.com/fetchr-airflow.html#fetchr-airflow"&gt;Airflow&lt;/a&gt; were opensourced by Airbnb, there was no good standard open source ETL system; but we needed one at Prezi, to replace the bash hairball we had. We used this project as a testing ground for Haskell, and wrote an ETL framework in Haskell (I think we called it Datapipe). We spent about 3-6 months on it and it was a big disappointment. Although we put it into production at one point, it was quickly replaced by a re-write (this time in Go, which I believe is still in production). At a later point I tried using Haskell for a personal project for representing Physics equations and quantities, and also ran into major/deep problems (oddly, I found it easier to model what I wanted with C++ templates of all things).&lt;/p&gt;
&lt;p&gt;Despite all these failures, I continued to be interested in Haskell. Although I adopted a critical stance, and usually argued against it in conversations, I assumed &lt;em&gt;"it's just me, I don't "get it"&lt;/em&gt;. But I never found or really saw good reasons and generalizable examples where using Haskell in production really made sense. So when I read the Warren Buffett bit, I knew that Haskell is one of those things that I have to stop spending time on, it's just not a practical thing for me to use. I share &lt;a href="http://functionaltalks.org/2013/08/26/john-carmack-thoughts-on-haskell/"&gt;John Carmack's stance&lt;/a&gt;, he views Haskell as a good source for ideas to use in imperative languages like C++.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I don't think spending time reading/writing Physics papers or Haskell has been a waste for me. Far from it. I learned a lot doing these things. But I've also determined that investing more time into them is not worth it right now. Remembering Warren Buffett and his pilot is a great way to remind myself to stick to it, and spend time on other, potentially more impactful things [2].&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;p&gt;[1] A &lt;a href="https://arxiv.org/abs/1405.6665"&gt;PRD paper&lt;/a&gt; later referenced it and called it "pioneering work", but I'm pretty sure it's not.&lt;/p&gt;
&lt;p&gt;[2] Having said that, if I were to go on a sabbatical, doing Physics would be fair game. The point of sabbatical is to take time off from the normal pursuit of things and spend time on more risky/fun projects, in a timeboxed way.&lt;/p&gt;</content><category term="warren"></category><category term="buffett"></category><category term="self"></category><category term="help"></category><category term="physics"></category><category term="haskell"></category></entry><entry><title>Beat the averages</title><link href="/beat-the-averages.html" rel="alternate"></link><published>2018-07-07T00:00:00+02:00</published><updated>2018-07-07T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-07-07:/beat-the-averages.html</id><summary type="html">&lt;p&gt;When working with averages, we have to be careful. There are pitfalls lurking to pollute our statistics and results reported.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;The numbers in this article are made up, but the lessons come from real life.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When doing Data Science, we almost always report averages. This is natural, because it stands for a simple model that everybody can understand. For example, in the delivery business, a topline metric is Deliveries per Driver (DPD), the average deliveries made per driver per day. This is a simple model the CEO can also remember: if our fleet is performing at DPD = 40, and we have 1,000 drivers, we make 40,000 deliveries per day. Being able to multiply two topline numbers and get a third one is a good thing.&lt;/p&gt;
&lt;p&gt;When working with averages, we have to be careful though: there are pitfalls lurking to pollute our statistics and results reported. It is important to note that &lt;strong&gt;there is nothing wrong with averages themselves, we just have to be careful with them&lt;/strong&gt;. I don’t believe that for most reporting purposes averages should or can be replaced (eg. by reporting the median), it is simply the job of the Data Science team to make sure the metrics make sense.&lt;/p&gt;
&lt;h2&gt;Outliers&lt;/h2&gt;
&lt;p&gt;When we say that our DPD is 40 and we have 1,000 drivers, the natural inclination (even for data people) is to &lt;em&gt;imagine&lt;/em&gt; 1,000 equivalent drivers, each performing exactly 40 deliveries every day. But we know that the world isn’t this simple. Things like driver performance tend to follow some more interesting distribution. The simplest thing we can imagine is that it follows a normal distribution. The plot below shows a normal distribution, the mean (green) and median (red) coincide. Gauss is happy.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-1.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;But almost always &lt;strong&gt;there are outliers&lt;/strong&gt;. In the case of drivers, there are various special circumstances which can cause a driver to have very low or very high DPD. For example, maybe the driver got sick, interrupted his route and went home early. Below is a the same distribution as above, with some stragglers introduced. We can see that this shifts the mean (green) down.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-2.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The shift in the mean is important, because it signals that something is going on: a bunch of our drivers got sick and went home early. Maybe tomorrow they are not coming to work. So monitoring both the average and median is important to detect and understand deviations.&lt;/p&gt;
&lt;p&gt;Apart from the median, which is also called the 50th &lt;a href="https://en.wikipedia.org/wiki/Percentile"&gt;percentile&lt;/a&gt;, &lt;strong&gt;checking out the bottom and top percentiles&lt;/strong&gt; is also very helpful. Below is the same two plots, with p10 and p90 also showing in red.&lt;/p&gt;
&lt;div&gt;
&lt;img src="/images/averages-3.png" alt="Probability distribution" style="width: 325px;"/&gt;

&lt;img src="/images/averages-4.png" alt="Probability distribution" style="width: 325px;"/&gt;
&lt;/div&gt;

&lt;p&gt;Something really useful happened! After we introduced the stragglers, the p10 dropped from about 27 to about 8!&lt;/p&gt;
&lt;p&gt;In general, &lt;strong&gt;showing percentiles is a useful technique, because as the example above shows, they can dramatically speed up detection of anomalies&lt;/strong&gt;. In real life work, looking at distribution doesn’t happen on a daily basis, but a timeseries showing the historic DPD can also show p10, median and p90, and can show such anomalies. The chart below shows such a made-up example, showing p10, p50 and p90 in red, the average in green, for the last 30 days for the fleet. On the 25th day a flu started spreading between our drivers, introducing the stragglers as shown in the distribution above. The mean and the median separate somewhat, but &lt;strong&gt;the p10 gives it away&lt;/strong&gt;.
It’s worth showing all four lines, at least on internal, debug dashboards.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-5.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;It’s worth noting the outliers can come from another source too: data bugs. Another good trick is to periodically examine low and high performers in a table, attached to the bottom of the internal report/dashboard.&lt;/p&gt;
&lt;p&gt;Finally, outlier/anomaly detection can also be automated, for example Facebook does this internally for various metrics. It’s important to automate at least the visualization of anomalies/distributions/stragglers in a debug dasboard, because in the long-run Data Scientists will forget to check manually (export and plot in ipython takes time).&lt;/p&gt;
&lt;h2&gt;Several populations&lt;/h2&gt;
&lt;p&gt;Another reason averages can be polluted is because of multiple populations (outliers can also be thought of as a population). In the delivery business, it is not uncommon to have many separate fleets of drivers, for different purposes. For example, we may have a B2C and a C2C fleet. Another distinction is cars vs bikes. Uber could have a fleet for passengers and a totally separate fleet for UberEats. Below is a (made-up) distribution that’s actually two fleets, a C2C fleet performing at DPD=20 and a B2C fleet performing at DPD=40.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-6.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;In cases like this, &lt;strong&gt;reporting on the blended mean may be misleading&lt;/strong&gt;. For example, if country X has a B2C and a C2C fleet, while country Y only has a B2C fleet, then reporting just on country-wise DPD will be misleading. For country X the C2C fleet will pull the DPD down, but this doesn’t mean that the Ops team in country X is performing worse, in fact it’s possible their B2C fleet is outperforming country Y’s. Report the per-fleet mean instead.&lt;/p&gt;
&lt;h2&gt;Skewed distributions&lt;/h2&gt;
&lt;p&gt;Sometimes distributions are not symmetric, they can be lopsided. In this case the median, mode (the most frequent outcome, the maximum of the distribution) and mean can be at different locations, which is often unintuitive for people. This isn’t a problem wrt the mean, but it’s good to know. The &lt;a href="https://en.wikipedia.org/wiki/Log-normal_distribution"&gt;Log-normal distribution&lt;/a&gt; is one such example:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/averages-7.png" alt="Probability distribution" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Percentage instead of mean&lt;/h2&gt;
&lt;p&gt;Sometimes, when building a metric, the mean is not a good choice. Let’s take pageload times an an example. Suppose we measure the average pageload time in miliseconds, and we see that it is 4,200ms; too high. After rolling out changes, it goes down to 3,700ms; but, 3,700 is still too high. Does that mean the rollout wasn't successful?&lt;/p&gt;
&lt;p&gt;In situations like this, it makes sense to &lt;strong&gt;bake the goal into the metric&lt;/strong&gt;. Suppose our goal is 2,000ms, which we deem pleasant from a UX perspective. Then a better way to define the metric is "% of pageloads that are within 2,000ms". If it was 57% before, and 62% after the rollout, it’s &lt;strong&gt;more natural to understand what happened&lt;/strong&gt;: an additional 5% of people now have a pleasant experience when loading our page. If there are 1,000,000 users per month, we impacted 50,000 users per month with the rollout. Not bad! A metric like this is also &lt;strong&gt;more motivating for product teams to work on&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Another big advantage of using percentages is &lt;strong&gt;increased resiliency to outliers&lt;/strong&gt;. While the mean could be polluted by outliers (users on slow connections, bots, data bugs), in the % it will be “just” a constant additive factor.&lt;/p&gt;
&lt;h2&gt;Ratios instead of means&lt;/h2&gt;
&lt;p&gt;Our delivery business also has C2C, ie. people can call a driver and send a package to another person, on demand. For example, if my partner is at the airport, but they forgot their passport at home, I can use the C2C app to fetch a car and send her the passport in an envelope. As such, the C2C app has standard metrics such as Daily Active Users (DAU) and Monthly Active Users (MAU). These are topline metrics, but we also need a metric which expresses how often people use the product. One way to do it using means would be to count, for each user, how many days they were DAU of the last 28 days. Suppose we call this Average DAU, and it’s 5.2. This is not that hard to understand, but could still be confusing. For example, people always forget the definition of a metric, in this case they would forget if the metric is 28 or 30 or 7 day based. Also, increments like this don’t feel natural: a +1% increment corresponds to +0.28 active days or 6.72 hours.&lt;/p&gt;
&lt;p&gt;A better metric is simply to divide DAU per MAU. This is a common metric also used inside Facebook. This feels more natural: if we are honest with ourselves, a user is essentially a MAU, because somebody who hasn’t used the product for 28 days is probably not coming back (For products with more sporadic usage, the base could be a 3*28 days). Thinking like this DAU/MAU is a very natural metric: it is the % of "users" who use the product daily.&lt;/p&gt;
&lt;h2&gt;Daily variations&lt;/h2&gt;
&lt;p&gt;Suppose our fleet’s average DPD is 40. Looking at driver X, his DPD yesterday was 29. Is he a low performer? Our first intuition might be to ask what the standard deviation of the fleet is (suppose it is 10), and then argue that this value is not “significantly” off. But from a business perspective, variance is irrelevant: if the COO wants to improve DPD and is looking for low performing drivers to cut, "cutting" at mean minus one sigma is a valid approach.&lt;/p&gt;
&lt;p&gt;However, &lt;strong&gt;it’s possible that our drivers have significant daily variation in their performance&lt;/strong&gt;. It’s possible that this driver had a DPD of 29 yesterday, but the previous day it was 47, and their historic average is actually 42. Always &lt;strong&gt;compare averages to averages&lt;/strong&gt;. In this case, compare the fleet’s average DPD over a long enough timeframe (probably at least 28 days) to the driver’s average DPD in the same 28 days. That is a more fair comparison to make, because it smooths daily variation. Of course, remember what was said here, for example don’t count days when the driver was sick, and compare him to his own fleet. &lt;/p&gt;
&lt;h2&gt;In summary&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Using averages is okay most of the time.&lt;ul&gt;
&lt;li&gt;Reporting on medians is probably not feasible in a business/product setting.&lt;/li&gt;
&lt;li&gt;Instead, make sure the average is meaningful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Watch out for outliers.&lt;ul&gt;
&lt;li&gt;Check median/p10/p90 and distributions regularly.&lt;/li&gt;
&lt;li&gt;Prune/separate outliers.&lt;/li&gt;
&lt;li&gt;Split up populations (B2C/C2C, car/bike), etc. to make sure the reported average (or median) is a meaningful number.&lt;/li&gt;
&lt;li&gt;Outliers can be real outliers, or issues in the data (eg. Self Pickup as a driver)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sometimes the population is homogeneous, but the distribution is skewed to one side or bimodal, in this case the average may be intuitively misleading.&lt;/li&gt;
&lt;li&gt;Sometimes, using %s instead of averages makes a better metric (Pageloads within 2000ms vs Average Pageload time).&lt;/li&gt;
&lt;li&gt;Sometimes, using a ratio instead of averages makes a better metric (example: DAU/MAU vs average number of DAUs in the last 28 days).&lt;/li&gt;
&lt;li&gt;Be careful when comparing daily snapshots and averages, there may be significant daily variation in performance.&lt;/li&gt;
&lt;/ol&gt;</content><category term="data"></category><category term="statistics"></category></entry><entry><title>Building the Fetchr Data Science Infra on AWS with Presto and Airflow</title><link href="/fetchr-airflow.html" rel="alternate"></link><published>2018-03-14T00:00:00+01:00</published><updated>2018-03-14T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2018-03-14:/fetchr-airflow.html</id><summary type="html">&lt;p&gt;We used Hive/Presto on AWS together with Airflow to rapidly build out the Data Science Infrastructure at Fetchr in less than 6 months.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Our goal at &lt;a href="https://fetchr.us"&gt;Fetchr&lt;/a&gt; is to build a world-class Data Science team. To do this, we need a world-class Data Science platform. I was fortunate enough to work at Facebook previously, which over the years arrived at a very efficient way of doing Data Science. So, when it came to building the platform I decided to follow the basic design patterns that I saw at Facebook.&lt;/p&gt;
&lt;p&gt;Based on the last 6 months, building a platform (including computation jobs, dashboarding) that is simple but allows us to move fast is feasible in just a 3-6 month period. So what does our platform look like? Like most things at Fetchr, we run on AWS. Our infra consists of 5-10 nodes right now (5 EMR, 2 Airflow, a few more for Supersets and others).&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/fetchr-ds-arch.png" alt="Fetchr Data Science Infra" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;We use &lt;a href="https://aws.amazon.com/emr/"&gt;EMR&lt;/a&gt; to get a Hadoop instance, with S3 as the backing storage. We actually don’t use the Hive query engine or MapReduce. We just use &lt;a href="http://hadoop.apache.org/"&gt;Hadoop&lt;/a&gt; as a metadata store (table definitions) for &lt;a href="https://prestodb.io/"&gt;Presto&lt;/a&gt;. Each EMR node also runs a Presto worker. Right now we use 1+4 nodes, with plans to scale it out to ~10.&lt;/p&gt;
&lt;p&gt;The data warehouse (DWH) philosophy is again based on the Facebook design pattern. We use flat tables, no fact/dimension tables; usually you can look at a table and see a complete picture. This makes the tables very usable and allows us to move fast, for example writing quick queries against tables is easy because it doesn’t require a lot of JOINs to get readable strings.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/flat-table.png" alt="Flat DWH table" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The other major design pattern from Facebook is the idea of daily partitioned tables. This is a feature available on Hive, and not really practical on eg. &lt;a href="https://aws.amazon.com/redshift/"&gt;Redshift&lt;/a&gt;. Essentially we store (complete) daily, write-once slices of each table, which are generated by daily jobs. The partitions are called &lt;code&gt;ds&lt;/code&gt; at Facebook and logically show up as a column of the table, and you’ll find plenty of references to it if you read the &lt;a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual"&gt;Hive docs&lt;/a&gt; (because Hive was written at Facebook). Physically, these are essentially directories, each one holding the data files for that day’s data. We use S3, so in our case it looks something like &lt;code&gt;s3://dwh-bucket/&amp;lt;table&amp;gt;/&amp;lt;ds&amp;gt;/&amp;lt;data_files&amp;gt;&lt;/code&gt;. For example, &lt;code&gt;s3://dwh-bucket/company_metrics/2018-03-01/datafile&lt;/code&gt;. For technical reasons, when importing data from our production (Postgresql) database, we use .csv, for later computed warehouse tables we use &lt;a href="https://orc.apache.org/"&gt;ORC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The advantage of this is that we have a complete history of the data warehouse going back as far as we’d like (old partitions can be deleted from a script after the desired retention period expires). There’s two ways to use &lt;code&gt;ds&lt;/code&gt; partitions, cumulative and events: each partition can store a complete copy of its data up to that day (cumulative), or each partition just stores that day’s worth of (event) data. For aggregate tables, it’s usually the first, for raw event tables, it’s usually the second. For example, our &lt;code&gt;company_metrics&lt;/code&gt; has complete cumulative data in each &lt;code&gt;ds&lt;/code&gt;, while our &lt;code&gt;driver_telemetry&lt;/code&gt; table has just that day’s worth of telemetry events. The advantage of this is that if something breaks, there’s almost never a big problem; we can always refer to yesterday’s data, and get away with it. Data will never be unavailable, it may just be late. Also, if there’s ever a question why a number changed, it’s easy to see what the reported number was a month ago (by examining that day’s &lt;code&gt;ds&lt;/code&gt; partition).&lt;/p&gt;
&lt;p&gt;We use &lt;a href="https://airflow.apache.org/"&gt;Airflow&lt;/a&gt; for data piping, which is loosely based on &lt;a href="http://www.asiliconvalleyinsider.com/asiliconvalleyinsider/Blog_A_Silicon_Valley_Insider/Entries/2016/5/1_Data_Engineering_%40_Facebook.html"&gt;Facebook’s Dataswarm system&lt;/a&gt;. Airflow allows us to write jobs as Directed Acyclic Graphs (DAGs) of tasks, with each task getting something useful done, like a database &lt;code&gt;INSERT&lt;/code&gt;. In Airflow, each DAG has a schedule, which uses the &lt;a href="https://airflow.apache.org/scheduler.html"&gt;cron format&lt;/a&gt;, so it can be daily, hourly, or just run every Wednesday at 3:15PM. On each of these runs, Airflow creates an instance of the DAG (identified by the timestamp), and executes the tasks, taking into account the dependencies between them. We have 2 types of DAGs: imports, for importing tables from the production database to the DWH, and compute jobs, which take existing (imported or computed) tables and make a new, more useful table. Fundamentally, each table is its own DAG.&lt;/p&gt;
&lt;p&gt;This poses a question: how do we make sure that a table’s DAG only runs once another table that is required (eg. it’s used in the &lt;code&gt;FROM&lt;/code&gt; part) is available (the latest &lt;code&gt;ds&lt;/code&gt; is available). This is accomplished with having special sensor tasks, which continuously check something (in this case whether a table’s partition is there), and only succeed if the check succeed; until then these “wait” tasks block the DAG from executing. For example, this is what a typical DAG looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/warehouse-dag.png" alt="Warehouse DAG" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;There are two waits (one for a table called &lt;code&gt;deliveries&lt;/code&gt;, one for this table but yesterday’s &lt;code&gt;ds&lt;/code&gt; partition, which is a kind of self-dependency), there is a &lt;code&gt;create&lt;/code&gt; task which creates the table in case it doesn’t exist, the &lt;code&gt;drop_partition&lt;/code&gt; drops the partition in case it already exists (in case we’re re-running the job), the &lt;code&gt;insert&lt;/code&gt; does the actual &lt;code&gt;INSERT INTO … SELECT ... FROM ...&lt;/code&gt;, and then some useful views are created (eg. for a table called &lt;code&gt;company_metrics&lt;/code&gt;, the view task creates a view called &lt;code&gt;company_metrics_latest&lt;/code&gt;, which points to the latest &lt;code&gt;ds&lt;/code&gt; partition).&lt;/p&gt;
&lt;p&gt;DAGs for import jobs are simpler:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/import-dag.png" alt="Import DAG" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;s3copy&lt;/code&gt; is the task which dumps the table from the production Postgresql into a local file and then copies it to S3, to the appropriate path. The &lt;code&gt;notice&lt;/code&gt; lets Hive now that we “manually” created a new partition on the backing storage, and triggers the metadata store to re-scan for new partitions by issuing &lt;code&gt;MSCK REPAIR TABLE &amp;lt;table&amp;gt;&lt;/code&gt;. (The &lt;code&gt;notice&lt;/code&gt; in the upper DAG is actually not required, since it’s a Presto job.)&lt;/p&gt;
&lt;p&gt;Airflow creates daily instances (for daily jobs) of these DAGs, and has a very helpful view to show progress/completion.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/dag-runs.png" alt="Warehouse DAG" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The UI also allows for tasks to be cleared, re-run, etc.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/task-actions.png" alt="Task actions" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Each DAG is implemented as Python code, in our case one &lt;code&gt;.py&lt;/code&gt; file per DAG. Most of these DAGs are highly repetitive, so we wrote a small library to save us time. For example, since we’re importing from a Postresql database, which is itself a relational database, it’s enough to say which table we want to import, our scripts figure out what the source table’s schema is, it knows how to map Postgresql types to Hive types, handle column names which are not allowed on Hive, etc. This makes importing a table as easy as:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/import-code.png" alt="Import code" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;All the logic is contained in the &lt;code&gt;dag_import_erp_table()&lt;/code&gt; function, which is re-used for all imports.&lt;/p&gt;
&lt;p&gt;We wrote similar helper functions for our common warehouse jobs, which take existing tables to build a new, more useful one. We specify the name of the output table, the &lt;code&gt;schedule_interval&lt;/code&gt;, the Hive columns (which is used to generate the &lt;code&gt;CREATE TABLE&lt;/code&gt; task), and the Presto &lt;code&gt;SELECT&lt;/code&gt; query, which will be placed after the &lt;code&gt;INSERT&lt;/code&gt; part in the insert task. Note the use of the &lt;code&gt;wait::&lt;/code&gt; prefix in the &lt;code&gt;FROM&lt;/code&gt; part. The helper functions automatically parses out these and generates wait tasks for these tables. A number of other such features were added to make it easy, fast and convenient to write jobs, without having to go outside the use of these helper functions. The &lt;code&gt;{{ ds }}&lt;/code&gt; macro will be replaced by the Airflow runtime with the proper ds, like &lt;code&gt;2018-02-20&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/warehouse-code.png" alt="Warehouse code" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Right how we have around 50 jobs, about half are “real” computations, the rest are imports. At this point we are able to move really fast: writing a new job and deploying it to production takes about an hour, and new joiners can ramp up quickly. Because we use Presto/Hive on top of S3 (versus &lt;a href="https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c"&gt;Airbnb runs their own Hadoop cluster&lt;/a&gt;) this introduced some low-level difficulties, so we had to write our own Operators, for example a PrestoOperator. Overall this code, plus the helper code is about 1-2k LOC, so it wasn’t too much work. To be fair, we never hit any data size problems, since compared to the capabilities of these tools, we have "small data". Our biggest tables are ~100M rows (these are part of 10-way &lt;code&gt;JOINs&lt;/code&gt;), but Hive/Presto can easily handle this with zero tuning. We expect to grow 10x within a year, but we expect that naive linear scaling will suffice.&lt;/p&gt;
&lt;p&gt;Maintaining a staging data warehouse is not practical in our experience, but maintaining a staging Airflow instance is practical and useful. This is because of Airflow’s brittle execution model: DAG’s &lt;code&gt;.py&lt;/code&gt; files are executed by the main webserver/scheduler process, and if there’s a syntax error then bad things happen, for example certain webserver pages don’t load. So it’s best to make sure that scripts deployed to the production Airflow instance are already working. So we set up a second, staging Airflow instance, which writes to the same data warehouse, (we have only one) but has its own internal state. Our production Airflow instance runs on two EC2 nodes. One for the webserver and the scheduler, one for the workers. The staging runs on a third, all 3 components on the same host.&lt;/p&gt;
&lt;p&gt;Overall, getting here was fast, mostly because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the database components (Hive, Presto) were open sourced by Facebook&lt;/li&gt;
&lt;li&gt;Amazon runs them for us as part of EMR&lt;/li&gt;
&lt;li&gt;we don't have to manage storage because of S3&lt;/li&gt;
&lt;li&gt;other former Facebook engineers built Airflow and Airbnb open sourced it&lt;/li&gt;
&lt;li&gt;because of the common background (Facebook) everything made sense.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Having said that, Airflow still feels very “beta”. It’s not hard to “confuse” it, where it behaves in weird ways, pages don’t load, etc. For example, if a DAG’s structure changes too much, Airflow seems to get confused and exceptions are thrown; for cases like this we wrote a custom scripts which wipes Airflow’s memory of this DAG completely (we didn’t find a way to do this with the provided CLI or UI). But, once we understood how it works and learned its quirks, we found a way to use it for our use-case. This process took about 1-2 months. We now rarely run into Airflow issues, perhaps once a month.&lt;/p&gt;
&lt;p&gt;The limits of this architecture is that it's very batch-y. For "real-time" jobs, we use hourly or 15-minute jobs to get frequent updates, but we apply manual filters on data size to make these run fast(er). Overall, this is inconvenient, and won't scale very well, eventually we'll have to look at other technologies for this use-case. Overall, we feel this is inconveniance/limitation/techdebt is a small price to pay for all the high-level product and business impact that we were able to deliver with this architecture.&lt;/p&gt;
&lt;p&gt;Airflow is now under Apache incubation, with lots of development activity, so it will surely get even better in the coming years. Going with Airflow was a bet that payed off, and we expect that Airflow will become the defacto open source ETL tool, if it’s not already that.&lt;/p&gt;
&lt;p&gt;In the next part about Fetchr's Data Science Infra, I’ll talk about how we use Superset for dashboarding and SQL.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="airflow"></category><category term="fetchr"></category></entry><entry><title>Don’t build cockpits, become a coach</title><link href="/data-science-coaching.html" rel="alternate"></link><published>2016-11-09T00:00:00+01:00</published><updated>2016-11-09T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-11-09:/data-science-coaching.html</id><summary type="html">&lt;p&gt;I used to think that a good analogy for using data is the instrumentation of a cockpit in an airliner. Lots of instruments, and if they fail, the pilot can’t fly the plane and bad things happen. There’s no autopilot for companies. The problem with this analogy is that planes aren’t built in mid-air. Product teams and companies constantly need to build and ship new products.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I used to think that a good analogy for using data is the instrumentation of a cockpit in an airliner. Lots of instruments, and if they fail, the pilot can’t fly the plane and bad things happen. There’s no autopilot for companies.&lt;/p&gt;
&lt;p&gt;The problem with this analogy is that planes aren’t built in mid-air. Product teams and companies constantly need to build and ship new products. Facebook is very good at this.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/cockpit.jpg" alt="A big complicated cockpit" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;A better model is a person, let’s say Petie, who is overweight and wants to lose weight. We’re going to need a scale, to see how much Petie actually weighs. If there’s no scales around, we’ll need to build one. While we’re at it, we can also collect other numbers, like body fat, circumference, etc. Then, we need to get Petie to actually look at the numbers. So we send these numbers to Petie every day in an email, or maybe we build a dashboard for him. Sounds good! But at this point all we did is make Petie know precisely how overweight he is. We probably successfully made Petie depressed about himself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What Petie really needs is a coach who helps him get in shape. He needs a workout plan and somebody to work with him on reaching his goals.&lt;/strong&gt; He needs somebody to help figure out what the workout should be, and set goals for the workout sessions. Once he’s reached his goals, he needs help figure out the next phase, what other exercises to do. Less running, more lifting, maybe do an experiment to see what diet works better for him. Knowing numbers is part of it, but the point it to somehow get him to go and do the things which will make him lose weight, keep track of how he’s doing, make sure he’s on track, help him make changes on the way.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/hosszu-shane.jpg" alt="A big complicated cockpit" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Many professional sports teams already use data heavily. The most famous example I know is the british cycling team, first described in this &lt;a href="http://www.bbc.co.uk/sport/olympics/19174302"&gt;2012 BBC article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I think being good coach is really hard. A good coach needs to know the sport, the athlete, the equipment, how training works, how the season and races work, everything. A good coach is on excellent terms with the athlete and is a great communicator. The coach needs to be able to convince the athlete to perform deliberate practice, which is hard and painful stuff. And to make it harder, painstakingly take precise measurements while he’s doing it. A good coach is not made overnight. A good idea is to learn from other, more experienced coaches, who have successfully helped athletes reach their goals. Facebook has good coaches (data scientists).&lt;/p&gt;
&lt;p&gt;Whenever I use an analogy I set off the Elon-alarm in my head. &lt;a href="http://jamesclear.com/first-principles"&gt;Elon Musk famously said&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I think it is important to reason from first principles rather than by analogy. The normal way we conduct our lives is we reason by analogy. [When reasoning by analogy] we are doing this because it’s like something else that was done or it is like what other people are doing — slight iterations on a theme.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But, this one seems useful enough to ignore Elon.&lt;/p&gt;</content><category term="data"></category><category term="science"></category><category term="product"></category><category term="analytics"></category></entry><entry><title>Beautiful A/B testing</title><link href="/beautiful-ab-testing.html" rel="alternate"></link><published>2016-06-05T00:00:00+02:00</published><updated>2016-06-05T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-06-05:/beautiful-ab-testing.html</id><summary type="html">&lt;p&gt;I gave this talk at the O’Reilly Strata Conference London in 2016 June, mostly based on what  I learned at Prezi from 2012-2016.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I gave this talk at the &lt;a href="http://conferences.oreilly.com/strata/hadoop-big-data-eu/public/schedule/detail/49583"&gt; O’Reilly Strata Conference London&lt;/a&gt; in 2016 June.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; this is not about Facebook A/B testing. If you want to hear about that, you will be disappointed, because nothing here is about Facebook. This is based on my experiences at previous jobs.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/1.png" alt="1. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;5 years ago I was working on my own startup, it was called Scalien, a combination of the words Scale + Alien. It was a cool name, and we had a cool product called &lt;a href="https://github.com/scalien/scaliendb"&gt;ScalienDB&lt;/a&gt; that nobody wanted, a NoSQL database that unlike other NoSQL databases used an algorithm called &lt;a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)"&gt;Paxos&lt;/a&gt; for consistent replication, whereas the competing products used Eventual Consistency. Scalien didn't work out and after we shut down the company I took a job with &lt;a href="https://prezi.com"&gt;Prezi&lt;/a&gt; in Budapest, Hungary, where I'm from. I was hired at Prezi as a data engineer, and eventually became the head data guy. My job was to build out the data team and the data infrastructure, and one of the projects I worked on was A/B testing, figuring out how the company should do A/B testing.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/2.png" alt="2. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;When I first started to think about how to improve the A/B testing culture and saw the problems we were having, I was a bit surprised. I thought, isn't it as simple as taking four numbers (the total impressions for A and B, and conversions for A and B), plugging it into some math equations, and reading off the results? I guess the naivete I had back then is a testament to the fact that even an education in Physics, the cradle of the scientific method, doesn't prepare you for how much more there is to it (=the scientific method), and sort of explains &lt;a href="http://blog.minitab.com/blog/understanding-statistics/what-can-you-say-when-your-p-value-is-greater-than-005"&gt;why so many academic studies are flawed&lt;/a&gt;. So the title is beautiful because (other than stealing this trendy phrase from O'Reilly) it turns out that A/B testing is a much more complex topic, and the actual complexities that you find when you look into it I personally find beautiful.&lt;/p&gt;
&lt;p&gt;I cannot claim I have found all pitfalls or know all the answers to the issues around A/B testing; here I will tell you what I learned in the last 3 years, what fallacies I found, and how we tried to work around them.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/3.png" alt="3. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Let’s define what A/B testing is in this context here:&lt;/strong&gt; you have two variants of your SaaS product, and you want to see which one is better. Note that you don't need to perform an A/B test (=compare two variants) if you just want to measure metrics for the new version! Performance is always measured relative to some relevant business metric, and on your target audience, the users you care about. The standard simple example is checking whether a button should be green or blue, which converts better. Another example is reorganizing templates that you can use in an editor, and seeing whether it affects user behaviour, like spending more time in the editor or creating more presentations. A third example is trying out different variants of a pricing page, and seeing how it influences people's choices between free and various paying options of a freemium product.&lt;/p&gt;
&lt;p&gt;No talk about A/B testing is complete without mentioning HIPPOs. &lt;strong&gt;HIPPO stands for Highest Paid Person's Opinion&lt;/strong&gt;, and is the antithesis of A/B testing and scientific thought: it's when the big guy decides what happens with the software based on her own personal preferences, and in general it's a bad idea.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/4.png" alt="4. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The talk is divided into 9 topics, organized in a nice staircase, as they are based on each other:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 are things you should think about before doing an A/B test&lt;/li&gt;
&lt;li&gt;3 are relevant while the test is running&lt;/li&gt;
&lt;li&gt;2 are things you should do after the A/B test is finished&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/5.png" alt="5. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The first example is about pricing pages. In a freemium business, users can go free or choose one of the paying options to get extra features. One thing we learned is that, if and as we make the free option on the pricing page smaller and the color less distinguished, people chose the paying option more (paying after a free trial period). After a few such experiments, we didn’t need to do an A/B test just to learn this, we already knew.&lt;/p&gt;
&lt;p&gt;The other example is about sign up emails. After a large number of such experiments, we learned that green buttons work better with our blue color scheme. (We also learned that designers are unhappy with green buttons.)&lt;/p&gt;
&lt;p&gt;So the lesson is, &lt;strong&gt;don't run an A/B test---which is an experiment to tell whether A or B is better---when you already know from past experiments which one will be better.&lt;/strong&gt; Note that not doing an experiment doesn’t mean you should not measure the metrics associated with a new version, that’s not the same thing.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/6.png" alt="6. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When you run an A/B test, you have to decide what metric to look at, and that should be the right metric.&lt;/strong&gt; A mistake many people make is they focus on a more immediate metric like conversion. But usually your desired business outcome is 3 more steps down the line, so you should focus on that. Usually this metric you should look at is related to something important to your business, like revenue, or how many pieces of content people make, or how much time people spend viewing content, and so on; or to give an example from social networking: do people post more, comment more, do they spend more time with the product, and so on.&lt;/p&gt;
&lt;p&gt;This is very tricky, and even when you think you're doing the right think, you may not. One story here is series of experiments that was performed on a pricing page. And we were smart, we didn't just look at clicks, we looked at the revenue generated. And we saw it was better, and we were super happy, we changed the HTML of the pricing page and now we're making more money! But we didn't have a good understanding of payment flows, so we didn't take into account refunds and chargebacks. A refund is when a customer asks you, the merchant for the money back. A chargeback is when the customer asks their credit card company for the money back. The way it works, if you're an online SaaS, essentially is a customer asks for money back, you want to give it back, to have good relations with customers and look good with the payment providers, because you're at their mercy. So these are 2 channels that we should have taken into account, and subtracted from the results of the A/B test. This is of course hard, because it requires sophisticated tracking and data collection, plus patience, because chargebacks are collected not by you, and only arrive at your doorstep 30-60 days after the payment event.&lt;/p&gt;
&lt;p&gt;A fallacy related to the question of which metric to look at is what I call &lt;strong&gt;data mining metrics&lt;/strong&gt;. I haven't yet talked much about technical concepts like statistical significance, and I won't because this is not intended to be a deeply technical talk. But most A/B tests are run using what's called frequentist statistics. With these statistics, after collecting samples for both A and B (exposures, conversions), you calculate something called the p-value. The p-value is usually expressed as a %, like 3%. The p-value has a very specific meaning, and it's the false positive rate. It says, if you were to repeat this experiment many-many times, given the current results, the math says that there is 3% chance of getting this outcome, assuming that A and B are the same. So the way frequentist A/B testing works is that, a company usually standardizes on a threshold like 5%, saying, let's run our A/B tests long enough so we collect enough samples so we only accept a false positives 1 out of 20 times.
So the fallacy here is that if you have many business metrics, let's say 20, and you run a test, and then you calculate A vs B for all 20 metrics, and you find one metric where B is better than A (p is less than 5%), and you accept it; then, if this is your methodology, then in the long run you're wrong in thinking that your false positive rate is only 5%. It will be much higher, because you're essentially giving yourself extra chances, like is B better wrt to Metric 1? How about Metric 2? and so on. So you shouldn't fool yourself like this.&lt;/p&gt;
&lt;p&gt;Of course you &lt;em&gt;should&lt;/em&gt; look at a bunch of metrics after the A/B test, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;before the test, you should select the target metric, and put it down in your A/B testing tool to keep yourself honest&lt;/li&gt;
&lt;li&gt;when you look at other metrics later, just remember that you're increasing the chance of seeing a false positive&lt;/li&gt;
&lt;li&gt;if you find a lift in another metric that seems like a reasonable effect that you didn't think off, repeat the A/B test with that target metric&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Large numbers =&amp;gt; significance.&lt;/strong&gt; There is one more thing to keep in mind. Large sites with a lot of traffic, they won't have these problems, because they can quickly collect so many samples that the p-value goes way down to like 1 in 10,000, and then you can look at many metric and still be reasonably sure that it's not a false positive. Essentially, if you collect a lot of samples, you're converting that bell curve estimate of the metrics, which has a certain precision error, into a point estimate as the bell curve becomes leaner and leaner, and you can compare those "freely". I'm a bit oversimplifying, but that's the gist of it.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/7.png" alt="7. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This is a simple lesson that I learned by observing PMs in action. The anti-pattern is that a product team will spend 2 months building out a cool new feature, and then they will run an A/B test on it "to prove that it works". Note how I said "to prove that it works". The problem is, after 2 months of invested time in the project, the team wants to see the thing succeed, they don't want their work to go to waste. This is when people can get really creative with p-values and explaining the results, if the results are negative or just inconclusive.&lt;/p&gt;
&lt;p&gt;I will cite an example that I heard from a guy at Etsy some time back because it's a great example. They were developing infinite scroll, and they invested a lot of work, but the A/B test showed it's performing much worse then the original. It was hard to let go. In the end they performed a sort of back-test to verify the negative result, and they simply increased one parameter in the original version, instead of showing 40 items they showed 80. And surprisingly, that already showed that users don't like that, so they could have saved a lot of development time by performing that simple A/B test first.&lt;/p&gt;
&lt;p&gt;In a SaaS environment, where you can release anytime and hopefully you have a lot of users, always try to test mock versions early. It's really the same idea as the Build-Measure-Learn loop from the Lean Startup book, it's just that people forget that the whole point of BML is that you want to get to a place where you're moving through BML loops very quickly. So I think if you spend more than 2 weeks on a feature before you start collecting data on it, you are in danger of getting attached to it.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/8.png" alt="8. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;The message around logging is pretty simple. Make sure that your product has the logging in place to tell which user was exposed to which treatment, and that you can connect your target metric. When I say you can connect your target metric, I mean that you can attribute the bottom of the funnel with what's happening in your A/B test. So if your A/B test is about which pricing page variant you show, and your target metric is revenue, you have to be able to connect the users to the purchases (and refunds, and chargebacks, etc). Sounds easy, but there are lots of caveats. For example, you have to make sure they always see the same pricing page. Or maybe there are 10 other ways to make a purchase. And so on. Or maybe your target metric is time spent, in that case you want to be able to slice that. Think it through, and make sure you have enough logging in place that you can compute your results at the end. It sounds easy and obvious, but it's actually pretty hard to do, because in a reasonably complex application, events come from lots of different sources on the site, and you have to make sure the logging is good and uniform everywhere.&lt;/p&gt;
&lt;p&gt;Also, make sure you have a dashboard that shows, &lt;strong&gt;based on the logs, what % of your users are seeing which variant&lt;/strong&gt;. I've seen a situation where the A/B test was essentially misconfigured due to a bug, and the actual exposure %s were different than what we thought it is. It would have been easy to catch if you just show this simple split in users, but we didn't. If one of the variants is performing very poorly, this can be very painful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logging is an investment you want to make early on in the game.&lt;/strong&gt; At that point it's pretty cheap, because the code for this is not terribly complex even for a moderately site. But if you don't do it, then it will be very hard and painful later, because you have to go through your entire product and change the logging. That is so painful that it pretty much requires executive buy-in, because product teams need to stop what they're doing and fix the logging.&lt;/p&gt;
&lt;p&gt;A related issue is hashing. Suppose you have an A/B test that you want to run on 10% of your users. So you want to run A for 5% and B for 5%. So who should those 10% be? A deceptively simple answer is, let's take the user's user_id, and take the modulo 100. Modulo 100 is just the fancy term for taking the last 2 digits of the number, like 11 or 39. There happens to be 100 two digit numbers (00 to 99), you then you can easily think in terms of %s. So you give treatment A to 00-04 and B to 05-09. We ended up assigning these ranges to product teams to use. Team one uses 00-09, team two uses 10-19, and so on.&lt;/p&gt;
&lt;p&gt;So what's the problem with this? It turns out there are a number of problems here. Let me point out a couple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uneven distribution: because servers assign user_ids in some systematic way, for example german users are more likely to end up with user_ids that end in 50-99.&lt;/li&gt;
&lt;li&gt;Memory: Suppose you run an A/B test the way I described above, in the first 10%, with an even split. Then it's over, and a week later, you run another one, with the same split. You risk contaminating the new test with the effects of the old tests, because users remember! Eg. if they got frustrated with the previous B, then you'll measure that frustration for your new B, too.&lt;/li&gt;
&lt;li&gt;What if you have more tests to run then you have ranges, or a product team wants to run more tests in its own range. It just becomes hard to do the accounting, and it becomes error-prone. The problem is, if you start to overlap A/B tests, and different A/B tests overlap in uneven ways, then you risk measuring the effects of other A/B tests, and not your own.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Hashing:&lt;/strong&gt; Fortunately there's an easy way out of this, and that's to use hashing, and I hope everybody is already doing that and nobody is learning anything here. Hashing just means that for each test, you randomize your users, and divide them into buckets after randomization. It's simple, you can take the name of the A/B test and combine it with a user_id or session_id, and use a standard hashing function like MD5(). This will generate a number that's unique for each test+user combination. Then you can actually use the modulo rule above, and take 10%, and so on. Because the randomization is different for each test, the tests will overlap (whether they're running at the same time or one after the other) in random ways, so other tests will "contaminate" your A and B to the same degree, so you can still measure the lift. This also shows that when you're doing a large number of A/B tests, you're never really measuring the “true” value of metrics, your measurement are always affected by other tests; but if you’re using md5(test_name+user_id) hashing both your variants are equally affected, by a constant background lift, so your measured lift is still the true difference between your A and B.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/9.png" alt="9. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Now we move on to things that are relevant while the experiment is running. The first one says, "Don't change the experiment while it's running".&lt;/p&gt;
&lt;p&gt;Let’s take this example: a product team is churning out new versions of a feature every week, and they're A/B testing it. They are using frequentist A/B testing, and they want to prove that B is better than A, so they're trying to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;build a better product (variant B)&lt;/li&gt;
&lt;li&gt;collect enough samples so that the p-value goes below 5%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The fallacy is, when they release a new version, they don't restart the A/B test, because then they'd have to restart counting the exposures and everything from 0. They just release into a running A/B test, to “keep the existing samples”. But you shouldn't do that, because you're cheating. Of course it's fine to release minor bugfixes and such, but if you want to run an A/B test where you take the p-value seriously at the end, then the "treatment" that you expose your users to has to be constant and fixed. Otherwise it's possible that the initial version was good, and that got some good conversions, and then subsequents were not good, but then statistical significance was reached, so now you think your final version is good. Imagine doing coin flips, and you’re trying to tell whether the coins are different, but you keep changing one of the coins, but not resetting the counts. What does the final result say about the final coin? Not much.&lt;/p&gt;
&lt;p&gt;Of course, if you're very quickly releasing new versions, that's a good thing. There are three things you can do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keep the old ones running "in the background" for long enough to get enough samples. &lt;/li&gt;
&lt;li&gt;Restart the experiment each time, and since you have lower counts, accept lower p-value thresholds, and hence accept more false positives. Remember, there's nothing magic or special about 5%.&lt;/li&gt;
&lt;li&gt;Do release the new version into the running test, but in that case you shouldn't calculate the p-value, just look at the time series for A and B, and if you have enough users, maybe you can see the difference between A and B, and between the versions of B. Or maybe you won't be able to.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/10.png" alt="10. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This one is also pretty simple, but it's hard to be disciplined about. So suppose you have a freemium product and you want to make more money. But only a fraction of your users is a paying user, so you test on free users, because you have plenty of that. That's great, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You won't be able to measure revenue related metrics.&lt;/li&gt;
&lt;li&gt;Users who don't want to pay for your product will be fundamentally different from users who do, so you're not really measuring on your target demographic.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other fallacy here is to run a test for 12 hours on Wednesday. The problem is that if your German users are up, your Chinese users are asleep, and so on. So depending on your user base, it's possible that the sample was not representative. Weekly seasonality is a similar story, don’t run a test from Friday to Tuesday. I found in most cases it's a good idea to run tests for multiples of 7 days. Even tougher is the yearly seasonality. For example, during the summer students and teachers are on vacation, so if your product has a mix of students and teachers, that mix will change sharply between June and September. Fortunately these are all things you can be cautious about and take into account, even correct.&lt;/p&gt;
&lt;p&gt;The keyword is &lt;strong&gt;representative sample&lt;/strong&gt;, you have to run your test on a representative sample.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mechanical Turk.&lt;/strong&gt; Sometimes at A/B testing talks the question comes up, what if you have a small site, not a lot of visitors, and you want to do A/B testing, and the speaker will say you can use Amazon Mechanical Turk. I think this is dangerous. For many SaaS sites your measurements will be meaningless, because people getting paid to take your test will behave differently from your real users. So what can you do?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From a math perspective, if you don’t have enough users, you’ll have to accept higher false positive/negative rates.&lt;/li&gt;
&lt;li&gt;Perhaps you should (take exception to one of the fallacies I have here, and) concentrate on an intermediate metric, like how much time visitors spend on your site, did that increase significantly? Or look at heatmaps.&lt;/li&gt;
&lt;li&gt;If you don’t have enough traffic, you could concentrate on getting more traffic, by eg. purchasing email lists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/11.png" alt="11. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This presentation is not specifically about the mathematics of A/B testing, so I decided not to talk too much about it. What is perhaps interesting is that there's 2 distinct schools of thought about A/B testing: &lt;strong&gt;frequentist and bayesian&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;frequentist view&lt;/strong&gt; of the world is an objective view. They say that there is a true conversion rate of A and B, and you could in theory repeat your finite A/B test an infinite number of times, each time collect only a finite number N of samples, and they say you should worry about your false positive and false negative rate. The concept of statistical significance is about false positive rates, the concept of statistical power is related to false negative rates. False positive is when B isn't better than A, but you measure B to be better and keep it. False negative is when B is better than A, but you measure it to be worse or your experiment does not reach significance, so you reject it. So in the frequentist view, you, the observer is not a part of the picture, you're just controlling statistical fluctuations.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;bayesian approach&lt;/strong&gt; is one where the observer is part of the picture. Essentially, it starts by asking what your belief is about A and B's conversion rate, this is called the prior, and then updates this belief after each and every observation. The end-result is something called the posterior, which is the current best-guess about A and B, based on what we've seen.&lt;/p&gt;
&lt;p&gt;Personally, I like the bayesian approach better. The reason is that the frequentist approach is very hard to use and the quantities it deals with are not intuitive. The way to conduct a frequentist approach requires the product manager, or somebody:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to guess how big the lift will be; this is necessary to figure out how many samples N to collect&lt;/li&gt;
&lt;li&gt;understand the concepts of statistical significance (~false positive rate) and statistical power (~false negative rate); based on my experience, many non-technical people like product managers are confused and easily misled by these&lt;/li&gt;
&lt;li&gt;not commit a number of mistakes, such as peeking and stopping early&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The bayesian approach, on the other hand, is more natural. It does not solve all these issues, but at least it operates with more easy to understand terms like "What is the probability that B is better than A? The probability---based on the observations so far---is 82%". Note that this is not the same thing as (one minus) statistical significance.&lt;/p&gt;
&lt;p&gt;Personally, I would use a mix of bayesian and frequentist today; frequentist to gauge how long to run a test, bayesian to evaluate (but also show frequentist results in small print). Note that large organizations which can afford to hire a team of smart data scientists can essentially go either way, because that team will make sure the decisions are statistically sound.&lt;/p&gt;
&lt;p&gt;There are other approaches like &lt;strong&gt;Multi Armed Bandit&lt;/strong&gt;, which is about regret minimalization. This says, if one of the variants is performing better, let's show that more often, so as not lose conversions. So it dynamically re-allocated traffic between A and B based on current conversion rates.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maserati problem.&lt;/strong&gt; A Maserati problem is a "nice to have problem", like "once I'm rich, should I buy a Maserati or a Ferrari". In my experience, chosing between these statistical engines is a nice problem to have, if this is your biggest concern, it means all your logging is good, you have good metric, you have enough traffic, you're not committing any of these fallacies. The flipside is, if you have problems with more basic things like logging, you probably shouldn't obsess too much about the math: as long as you're feeding wrong numbers to the equations, your number one priority should be to fix that, as your numerical results will be wrong or off.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/12.png" alt="12. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Now we move on to what to do after the test is finished running. The first question is, how to report the results, whatever they are. I found it's very important to have a &lt;strong&gt;standardized reporting&lt;/strong&gt; tool for A/B results, and not have PMs write their results in custom emails that get sent around. There are two reasons. One is, it helps to keep us honest. If there is standard tool where everybody can go and check out results, there’s less of a chance that people will cherry-pick, and just talk about the good results, or do metrics data mining, the fallacy I mentioned before. Also, it conditions stakeholders and executives to a standard format, so they’ll get used to a standard set of numbers, how they’re presented. They’ll get used to usual lift %s, so they can relate results to past results, have expectations.&lt;/p&gt;
&lt;p&gt;One story I want to share is related to &lt;strong&gt;raw vs projected&lt;/strong&gt; results. So in this case there was no standardized reporting, and the results were communicated in email, and the result was like a 40% lift. So everybody was happy, and they were right to be happy, and the thing got deployed to 100% of users. Then a month later the team got an email from the CEO, asking why the metric is flat, if the thing got us a 40% lift. So it turns out that the PM was talking about raw lift %, which is the lift you see among users who actually use that feature in the product. Think of the feature like a dialog in a big application. But let’s say only 1% of users ever use that. Then your overall or projected or top-line lift will only be 0.4%. So that’s why the CEO didn’t see a big lift across the company, because the result was miscommunicated and a false expectation was set.&lt;/p&gt;
&lt;p&gt;So the lesson, keep your raw and projected (to all users) lift separate, and report in a standard way to your stakeholders!&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/13.png" alt="13. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;This point just says remember, and it connects back nicely to the first point, where I said you should ask yourself, do you already know? The example I gave there was about the green button, which has been tested and always performed better in emails than the blue ones. So the way to make sure your organization learns and doesn’t repeat experiments if you record them in a tool. That way you look them up and reference them later. Referencing them is important, because people’s memory gets worse over time, so it’s important to have raw results that you can point to to make arguments. This also ties in with the previous point about standardized reports, once you have standardized the reporting then it’s probably easy to keep them around. If you don’t have a fancy tool, you can just use a standard Google Docs template and put it into a shared folder!&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/14.png" alt="14. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;There are a number of other topics around A/B testing that would be interesting to talk about, these were the ones that were the most important lessons for me over the last few years.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/strata-2016/15.png" alt="15. slide" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Based on these lessons I constructed this simple flowchart, what a good A/B testing procedure looks like. There’s the Build-Measure-Learn cycle from Lean Startups, this is sort of an exploded view of that. Speed is very important, at large sophisticated websites only 1 in 10 experiments yields a clear lift, so you have to be fast to get to the wins.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://prezi.com/sqzccuudw5hz/strata-2016/"&gt;Prezi of this talk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://greenteapress.com/wp/think-bayes/"&gt;Think Bayes&lt;/a&gt;: free book&lt;/li&gt;
&lt;li&gt;&lt;a href="http://greenteapress.com/thinkstats/"&gt;Think Stats&lt;/a&gt;: free book&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/1118539273"&gt;How to Measure Anything&lt;/a&gt; - good book, though not specifically about A/B testing&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.evanmiller.org/"&gt;Evan Miller’s site&lt;/a&gt;: articles and tools about A/B testing&lt;/li&gt;
&lt;li&gt;&lt;a href="http://varianceexplained.org/"&gt;Variance Explained&lt;/a&gt;: articles about A/B testing&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ai.stanford.edu/~ronnyk/ronnyk-bib.html"&gt;Ron Kohavi’s articles&lt;/a&gt;: he worked A/B testing at Bing&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/Group-Sequential_Tests_for_Two_Means.pdf"&gt;Group Sequential Testing&lt;/a&gt;: how to take into account peeking in frequentist experiment design&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Eh00PoR76NY"&gt;Youtube: talk from the CEO of Optimizely on A/B testing lessons learned after 100,000+ experiments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=0dVIjWTI_A0"&gt;Youtube: A/B testing war stories from Etsy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=Auu9AnCozWQ"&gt;Youtube: A/B testing math from a Groupon statistician&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=8DMU9h4dwi8"&gt;Youtube: introductory talk on Bayesian A/B testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/A/B_testing"&gt;Wikipedia: starting point on A/B testing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="ab-testing"></category><category term="strata"></category></entry><entry><title>Hack, HHVM and avoiding the Second-system effect</title><link href="/hack-hhvm-second-system-effect.html" rel="alternate"></link><published>2016-05-14T00:00:00+02:00</published><updated>2016-05-14T00:00:00+02:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-05-14:/hack-hhvm-second-system-effect.html</id><summary type="html">&lt;p&gt;I read this book on my first vacation after I started working at Facebook and thus became a semi-regular &lt;a href="http://hhvm.com/"&gt;Hack/HHVM&lt;/a&gt; user. I highly recommend reading (parts of) it. But not to learn Hack/PHP, which is irrelevant to most people. Instead, it’s to learn about how Facebook improved it’s www codebase and performance without rewriting the old PHP code in one big effort, and thus avoided the famous Second-system effect.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I read &lt;a href="http://www.amazon.com/Hack-HHVM-Programming-Productivity-Breaking/dp/1491920874"&gt;Hack &amp;amp; HHVM—Programming Productivity without Breaking Things&lt;/a&gt; on my first vacation after I started working at Facebook and thus became a semi-regular Hack/HHVM user. I highly recommend reading (parts of) it. But not to learn Hack/PHP, which is irrelevant to most people. Instead, it’s to learn about how Facebook improved it’s www codebase and performance without rewriting the old PHP code in one big effort, and thus avoided the famous &lt;em&gt;Second-system effect&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hack &amp;amp; HHVM book" src="http://akamaicovers.oreilly.com/images/0636920037194/cat.gif" title="Hack &amp;amp; HHVM book"&gt;&lt;/p&gt;
&lt;h2&gt;Second-system effect&lt;/h2&gt;
&lt;p&gt;The second system effect was first described by Fred Brooks in &lt;a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month"&gt;The Mythical Man Month&lt;/a&gt;, based on his experiences managing operating system software development at IBM in the early 1960s:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The second-system effect proposes that, when an architect designs a second system, it is the most dangerous system they will ever design, because they will tend to incorporate all of the additions they originally did not add to the first system due to inherent time constraints. Thus, when embarking on a second system, an engineer should be mindful that they are susceptible to over-engineering it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let me offer a more modern description: version 1.0 of the product/app/software is successful. Over time the programmers realize that, knowing what they know now, they could do a much better job. Meanwhile, the technology landscape changes, it'd be nice to take advantage of the shiny new architectures, languages, frameworks available. So the team embarks on the quest to ship 2.0―a rewrite. Inevitably, even good teams will over-engineer, and the result will be a technological and project management mess. 2.0 projects like this miss their original ship dates by several years. Once it does ship, it’s buggy and slow, because unlike 1.0 it has no fine-tuning, since it hasn't seen the light of real-world usage yet. So several more years go by until 2.0 is also fine-tuned. At this point the new set of programmers―the cohort who joined after 1.0―can repeat the Second-system effect with 3.0, which for them will be the new 2.0. Rinse, repeat.&lt;/p&gt;
&lt;p&gt;I think within this book is a nice little lesson about how to avoid the Second-system effect. The book doesn't actually mention the Second-system effect, and I'm not implying anything about the history of the main www codebase at Facebook. I'm not saying that Facebook specifically did this to avoid the Second-system effect. It's just a lesson that I think can be extracted from the design decisions explained in the book.&lt;/p&gt;
&lt;h2&gt;PHP, Hack, HHVM&lt;/h2&gt;
&lt;p&gt;The story here is that Facebook started out as a PHP codebase. Over time the product became very successful, which meant that it was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;very large (1M+ LOC)&lt;/li&gt;
&lt;li&gt;serving a large number of users&lt;/li&gt;
&lt;li&gt;being worked on by a large number of engineers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So there was a desire to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;speed it up so it can serve more users per node&lt;/li&gt;
&lt;li&gt;make it easier for engineers to work on the code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think that for many programmers (including yours truly) the instinctive reaction would have been to say "PHP sucks, it's slow and unsafe, let's rewrite the www codebase in a real programming language like Java and run on the JVM". What's interesting is that Facebook did not do this; Facebook did not discard PHP.&lt;/p&gt;
&lt;p&gt;Instead, Facebook decided to improve the layer below the application code to improve overall performance, and write new code in a way which takes advantage of the features of the improved layer (and very slowly deprecate old code). The "layer" here is actually many things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hack, a language like PHP, but much nicer&lt;/li&gt;
&lt;li&gt;a static type-checker for Hack&lt;/li&gt;
&lt;li&gt;HHVM, a runtime for Hack (and also regular PHP)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Two notes are in order here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Historically, there was something called &lt;a href="https://en.wikipedia.org/wiki/HipHop_for_PHP"&gt;HPHPc&lt;/a&gt; before HHVM. It was a PHP-to-C++ compiler, but it’s no longer being used at Facebook.&lt;/li&gt;
&lt;li&gt;Hack and HHVM did not come about as a result of a committee sitting down, identifying the problem, scoping out the solutions, and picking one. They originated (both HPHPc and the Hack language) from &lt;a href="https://www.facebook.com/hackathon"&gt;Hackathons&lt;/a&gt;, an integral part of Facebook engineering culture, where individual engineers were attacking problems they thought are promising and important.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My favorite features of Hack/HHVM:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;very fast&lt;/li&gt;
&lt;li&gt;100% interoperability with regular PHP code (eg. existing code)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.hhvm.com/hack/types/introduction"&gt;types&lt;/a&gt; (can also run regular PHP code in untyped mode)&lt;/li&gt;
&lt;li&gt;the &lt;a href="https://docs.hhvm.com/hack/typechecker/introduction"&gt;type-checker&lt;/a&gt; is very fast, millisecond response time even is very large codebases since it maintains state&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.hhvm.com/hack/generics/introduction"&gt;generics&lt;/a&gt;, &lt;a href="https://docs.hhvm.com/hack/lambdas/introduction"&gt;lambdas&lt;/a&gt;, etc.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.hhvm.com/hack/async/introduction"&gt;async/await keyword for cooperative multitasking&lt;/a&gt;: this is very cool and worth reading up on. Essentially it's language/runtime level support for (single-threaded) event-driven architecture (epoll, kqueue, Completion Ports), so you don't have to explicitly manage the state like we did in the plain old C++ &lt;a href="https://github.com/scalien/scaliendb"&gt;ScalienDB codebase&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.hhvm.com/hack/XHP/introduction"&gt;XHP&lt;/a&gt;: the way to do www rendering safely (in the xss sense) in Hack, with language level support for XHTML and custom modules (eg. a Comments box)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Lesson learned&lt;/h2&gt;
&lt;p&gt;So the interesting lesson here is that a possible way out of the Second-system effect is to start improving the environment (language, runtime, frameworks, etc.) of the main codebase instead of rewriting the main codebase. I certainly don't think this is the solution, in many cases it cannot be applied, but it's something to keep in mind as a design pattern. Some of the challenges of this approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You need a couple of really smart people who can design and implement a new language that's backwards compatible with existing code.&lt;/li&gt;
&lt;li&gt;You need to put sustained effort into it afterwards, keeping it mostly compatible with the standard version of the language.&lt;/li&gt;
&lt;li&gt;Every new engineer will need time to ramp up using the new language.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another interesting aspect of this is the investment needed. Rewriting the whole application codebase is an all-in project, with all (or much) of the engineering team working on it. I assert that changing out the layers below and around it can be accomplished by a smaller, focused team, iteratively. It's a smaller bet. Writing HHVM was certainly a smaller effort than rewriting all of Facebook in Java would have been! Having said that, an organizational/management note: I do think you need a fairly large group of people to generate enough ideas (and Hackathon projects) so that some really good and impactful ones come out of it.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I will conclude this post with my personal impressions: working with Hack/HHVM is very pleasant. The type checker holds your hand all the way, so it feels much nicer/safer than eg. writing Python. The syntax is a bit unfortunate in places, but overall it’s a non-issue for me. I'd consider using Hack/HHVM for personal projects or a startup. It's completely open source, so anybody can use it for their projects.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/zsol"&gt;Zsolt Dollenstein&lt;/a&gt; for reviewing this blog post and giving valuable suggestions.&lt;/p&gt;
&lt;p&gt;Links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://hhvm.com/"&gt;HHVM main site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.facebook.com/hhvm/"&gt;HHVM Facebook page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebook/hhvm"&gt;HHVM on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.engineyard.com/2014/hhvm-hack"&gt;A series of posts about Hack/HHVM from 2014&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;If you find Hack interesting, check out &lt;a href="http://flowtype.org/"&gt;Flow, a static type checker for Javascript, by Facebook&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><category term="books"></category><category term="programming"></category><category term="hhvm"></category><category term="brooks"></category></entry><entry><title>Einstein's amazing theory</title><link href="/einsteins-amazing-theory.html" rel="alternate"></link><published>2016-02-16T00:00:00+01:00</published><updated>2016-02-16T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-02-16:/einsteins-amazing-theory.html</id><summary type="html">&lt;p&gt;This post is about the amazing success of Einstein's general theory of relativity. The theory predicts, among other things the accelerating Universe, black holes, gravitational lensing and gravitational waves. The real shocker is to remember that Einstein didn't invent general relativity to explain these. He didn’t know about these, they didn't exist at that time!&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently the &lt;a href="https://www.ligo.caltech.edu/"&gt;LIGO&lt;/a&gt; experiment &lt;a href="https://www.ligo.caltech.edu/news/ligo20160211"&gt;reported the measurement and  experimental verification of gravitational waves&lt;/a&gt;. This is a big deal and should get those involved in the experiment a &lt;a href="https://en.wikipedia.org/wiki/Nobel_Prize"&gt;Nobel prize&lt;/a&gt;. Great news for physics and our understanding of nature!&lt;/p&gt;
&lt;p&gt;From the press release:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Gravitational waves carry information about their dramatic origins and about the nature of gravity that cannot otherwise be obtained. Physicists have concluded that the detected gravitational waves were produced during the final fraction of a second of the merger of two black holes to produce a single, more massive spinning black hole. This collision of two black holes had been predicted but never observed.&lt;/p&gt;
&lt;p&gt;Based on the observed signals, LIGO scientists estimate that the black holes for this event were about 29 and 36 times the mass of the sun, and the event took place 1.3 billion years ago. About 3 times the mass of the sun was converted into gravitational waves in a fraction of a second—with a peak power output about 50 times that of the whole visible universe. By looking at the time of arrival of the signals—the detector in Livingston recorded the event 7 milliseconds before the detector in Hanford—scientists can say that the source was located in the Southern Hemisphere.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This post is about the amazing success of the &lt;a href="https://en.wikipedia.org/wiki/General_relativity"&gt;theory of general relativity&lt;/a&gt;, the theory &lt;a href="https://en.wikipedia.org/wiki/Albert_Einstein"&gt;Einstein&lt;/a&gt; developed between 1907 and 1916.&lt;/p&gt;
&lt;h2&gt;Historical context&lt;/h2&gt;
&lt;p&gt;Let's start with some history and context. The 100 years between 1850 and 1950 were a tremendously fruitful time for physics. &lt;a href="https://en.wikipedia.org/wiki/Maxwell%27s_equations"&gt;Maxwell wrote down his famous equations&lt;/a&gt;, Einstein developed the &lt;a href="https://en.wikipedia.org/wiki/Special_relativity"&gt;special&lt;/a&gt; and the &lt;a href="https://en.wikipedia.org/wiki/General_relativity"&gt;general theory of relativity&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Quantum_mechanics"&gt;quantum mechanics&lt;/a&gt; and then &lt;a href="https://en.wikipedia.org/wiki/Quantum_field_theory"&gt;quantum field theories&lt;/a&gt; were invented.&lt;/p&gt;
&lt;p&gt;It is often said that given the Maxwell equations and its invariant, the &lt;a href="https://en.wikipedia.org/wiki/Lorentz_transformation"&gt;Lorentz transformation&lt;/a&gt;, somebody would have eventually extracted special relativity from it. The invention of &lt;a href="https://en.wikipedia.org/wiki/Quantum_mechanics"&gt;quantum mechanics&lt;/a&gt; (QM) came after experimental observations that needed a new theory to explain them. Various frameworks for QM were devised in parallel by a number of physicists (Heisenberg, Schrodinger, Dirac, Pauli, Bohr, Sommerfeld, Einstein, etc.). Quantum mechanics isn’t compatible with special relativity, so a few years later &lt;a href="https://en.wikipedia.org/wiki/Quantum_field_theory"&gt;quantum field theories&lt;/a&gt; came along (mostly by the QM physicists, plus new kids on the block, like Feynman). QFTs are extensions of QM to take into account special relativity and the creation and destruction of particles and antiparticles. QFTs have predicted the existence of then-unobserved particles, but primarily have been constructed to model experimental observations, and have to be continuously patched and hacked to do so.&lt;/p&gt;
&lt;h2&gt;General relativity&lt;/h2&gt;
&lt;p&gt;Compared to the history of quantum theories, Einstein's invention of general relativity is very different and elevates Einstein into a class by himself. Einstein conducted &lt;a href="https://en.wikipedia.org/wiki/Thought_experiment"&gt;gedanken experiments&lt;/a&gt; and concluded &lt;em&gt;“this is how the Universe must work”&lt;/em&gt;. &lt;a href="http://www.astronomynotes.com/relativity/s3.htm"&gt;Here is a short description of his famous thought experiment involving elevators.&lt;/a&gt; His invention of general relativity was completely unexpected because from an experimental viewpoint it was "unnecessary": there were no experiments that needed to be explained. From &lt;a href="https://en.wikipedia.org/wiki/History_of_general_relativity"&gt;Wikipedia:&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As Einstein later said, the reason for the development of general relativity was the preference of inertial motion within special relativity, while a theory which from the outset prefers no state of motion (even accelerated ones) appeared more satisfactory to him. So, while still working at the patent office in 1907, Einstein had what he would call his "happiest thought". He realized that the principle of relativity could be extended to gravitational fields. Consequently, in 1907 (published 1908) he wrote an article on acceleration under special relativity. In that article, he argued that free fall is really inertial motion, and that for a free falling observer the rules of special relativity must apply. This argument is called the Equivalence principle. In the same article, Einstein also predicted the phenomenon of gravitational time dilation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Einstein's theory was successful of course, and in the next 100 years turned out to predict, among other things: (i) &lt;a href="https://en.wikipedia.org/wiki/Accelerating_expansion_of_the_universe"&gt;the accelerating Universe&lt;/a&gt;, (ii) &lt;a href="https://en.wikipedia.org/wiki/Black_hole"&gt;black holes&lt;/a&gt;, (iii) &lt;a href="https://en.wikipedia.org/wiki/Gravitational_lens"&gt;gravitational lensing&lt;/a&gt; and (iv) &lt;a href="https://en.wikipedia.org/wiki/Gravitational_wave"&gt;gravitational waves&lt;/a&gt;. The real shocker is to remember that Einstein didn't invent general relativity to explain these things. He didn’t know about these things, they didn't exist at that time!&lt;/p&gt;
&lt;p&gt;So how does thinking about inertial reference frames and accelerating observers lead one to come up with a theory that somehow features black holes and gravitational waves? The original thought experiments were “just” arguments about what would happen in an elevator in space, in an elevator in a gravitational field, and so on. Einstein needed a mathematical framework which can be extended with some physics, ie. equations, and a mapping from the mathematical quantities to measurable quantities, and hence a way to connect the math to an understanding of what is being calculated. He found this in &lt;a href="https://en.wikipedia.org/wiki/Manifold"&gt;manifolds&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Differential_geometry"&gt;differential geometry&lt;/a&gt;, put the famous &lt;a href="https://en.wikipedia.org/wiki/Einstein_field_equations"&gt;Einstein field equations&lt;/a&gt; on top, and connected the resulting theory to the real world (eg. in the mathematical framework, what corresponds to a real-world event, world line, how does an observer perceive time, distance, etc). It is this theory—that treats space and time as a combined entity called &lt;a href="https://en.wikipedia.org/wiki/Spacetime"&gt;spacetime&lt;/a&gt;, modeled as a manifold—that amazingly predicts (i)-(iv): (i) spacetime is expanding, at an accelerating rate (ii) spacetime can have singularities (iii) spacetime warps near a heavy object, photos follow spacetime, hence an object behind a heavy object appears lensed (iv) ripples in spacetime.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I’m not sure how successful I was in communicating the amazing fact that Einstein’s theory, developed to explain a very general but simple idea, predicts such a variety of mind boggling phenomena, which are one after the other found to exist in nature. I recommend you to read the &lt;a href="http://www.amazon.co.uk/Relativity-Special-General-Albert-Einstein/dp/1891396307/ref=la_B00BGN8B7O_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1455650031&amp;amp;sr=1-1"&gt;book Einstein wrote for the layman&lt;/a&gt; to explain the special and the general relativity with his original thought experiments. Read the book, and then think about how given that you could get to gravitational waves!&lt;/p&gt;
&lt;p&gt;It is often said that Einstein touched so many areas of physics, he could have received several Nobel prizes. An incomplete list of Einstein's more famous results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Brownian_motion"&gt;Brownian motion.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The special theory of relativity.&lt;/li&gt;
&lt;li&gt;The general theory of relativity.&lt;/li&gt;
&lt;li&gt;His contributions to quantum mechanics.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Einstein did get the Nobel prize in 1922 related to the last item on the list: &lt;em&gt;“for his services to Theoretical Physics, and especially for his discovery of the law of the &lt;a href="https://en.wikipedia.org/wiki/Photoelectric_effect"&gt;photoelectric effect&lt;/a&gt;”&lt;/em&gt;.&lt;/p&gt;</content><category term="physics"></category><category term="einstein"></category><category term="relativity"></category></entry><entry><title>Heisengames and the importance of patience in business</title><link href="/heisengames-business.html" rel="alternate"></link><published>2016-02-08T00:00:00+01:00</published><updated>2016-02-08T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-02-08:/heisengames-business.html</id><summary type="html">&lt;p&gt;Most bets businesses take, be it hiring, features, products or strategy don't  work out. Still, many businesses are successful despite setbacks. A negative attitude---even when the analysis of the situation is in fact correct---may be missing the bigger picture.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Most bets businesses take, be it hiring, features, products or strategy don't  work out. Only a few of them are ever successful. Analytical employees often notice this, and become negative: "We just hired executive E, she did X, Y and Z, and it's not working!", and similar sentiments.&lt;/p&gt;
&lt;p&gt;Still, many businesses are successful despite setbacks. A negative attitude---even when the analysis of the situation is in fact &lt;strong&gt;correct&lt;/strong&gt; and X, Y and Z were failures---may be missing the bigger picture. Putting aside the demotivating psychological aspect of negativity, &lt;em&gt;how can you be right and still miss the bigger picture?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Toy model&lt;/h2&gt;
&lt;p&gt;Let's model the business as a dice game you're playing at a casino. The game is simple: if you roll a six, you get &lt;code&gt;$X&lt;/code&gt;. The game costs &lt;code&gt;$1&lt;/code&gt; to play at the casino. Clearly, the breakeven point is &lt;code&gt;X=6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose the casino offers the game at &lt;code&gt;X=7&lt;/code&gt;. In this case, &lt;strong&gt;you still lose 5 times out of 6&lt;/strong&gt;. However, &lt;strong&gt;when you win, you offset your losses, and in the long run you will make a lot of money&lt;/strong&gt;. (Unless you hit a losing streak and lose all your money, in which case you don't have any more money to keep playing the game; but let's ignore this.) This toy model illustrates the point about business and negativity: most bets will be "failures", but in the long run, it can still be a (very) profitable game!&lt;/p&gt;
&lt;p&gt;Of course, it's also possible that &lt;code&gt;X=5&lt;/code&gt;. In this case the negativity is justified, because even when we win, it doesn't offset the cost of playing the game. Unless...&lt;/p&gt;
&lt;h2&gt;Heisengame&lt;/h2&gt;
&lt;p&gt;We can make the game even more interesting. Suppose &lt;code&gt;X=5&lt;/code&gt;, but every time you win, the casino increases the payout by a cent. In this case, most of the time you lose, and initially you also lose out on average, but if you keep it up long enough (eg. using venture capital), eventually the game will become (very) profitable! I call this Heisengame, because playing the game changes the game itself.&lt;/p&gt;
&lt;p&gt;We can also turn it around. Suppose &lt;code&gt;X=7&lt;/code&gt;, but every time you win, the casino decreases the payout by a cent. In this case, analysis will show that the game is currently profitable on average, but it won't be in the long run.&lt;/p&gt;
&lt;h2&gt;Patience&lt;/h2&gt;
&lt;p&gt;In real life, the payout itself is also not fixed. You can model it like this: the casino may pay you a random amount when you roll a six, say according to a normal distribution centered on &lt;code&gt;X&lt;/code&gt;. In this case, you need to play even longer to learn what the game actually is, because you need to collect many data points to understand the payout function. &lt;em&gt;You need to have patience to understand the game.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;These are just toy models, but they illustrate important points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Just because most bets are losers isn't a problem in itself. As long as the winners generate a big enough win, the game is worth playing. To know which game the business is playing, long term data and analysis is needed.&lt;/li&gt;
&lt;li&gt;Business is a Heisengame: playing the game changes the rules of the game. Long term data and analysis is needed to tell the direction of the change.&lt;/li&gt;
&lt;li&gt;In the real world, you don't know the parameters of the game. You need to be patient while you collect data and measure the parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So next time you see a bet not working out, consider that maybe the business just rolled a five. As long as you win in the long run, it's okay.&lt;/p&gt;</content><category term="heisengames"></category><category term="business"></category></entry><entry><title>Cloud9: Cloud coding that actually works</title><link href="/cloud9.html" rel="alternate"></link><published>2016-02-07T00:00:00+01:00</published><updated>2016-02-07T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-02-07:/cloud9.html</id><summary type="html">&lt;p&gt;For the past 2 months I've been using Cloud9 for writing code in the cloud, and I can wholeheartedly recommend it: it just works for me. It's basically Docker plus an IDE: you get a Docker container running Ubuntu that you can access over a web IDE.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've always wondered when it will be practical to write code in the cloud (and not locally).
Most of the apps I use have already moved to the cloud: email, calendar, docs, music. Writing code is one of the last blockers to a true thin client approach for me (the other notable one being photo management).&lt;/p&gt;
&lt;p&gt;Recently I've been using &lt;a href="https://c9.io"&gt;Cloud9&lt;/a&gt; for writing code in the cloud, and I can wholeheartedly recommend it: it just works for me. It's basically &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; plus an IDE: you get a Docker container running Ubuntu that you can access over a web IDE. Docker is a pretty standard thing, so there's not much to say there. The good news is that there's also not much to say about the IDE: &lt;strong&gt;it just works&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You can theme it, I have it set up to look and feel like Sublime. It has a nice treeview on the left, you can do simple file management there. Shortcuts like copy/paste, find, etc. work as expected, as does full-text search. It's quite impressive, I've been using it for about 2 months and I haven't run into a single instance where the webapp/javascripts feeling leaks through. Part of the IDE is the ability to open terminal consoles. Like the editing, the terminal also just works, including copy/paste, shortcuts, and so on. The one use-case I found where the IDE doesn't work is over mobile: trying to use it over a touch interface is horrible. They're working on it and this will get better over time.&lt;/p&gt;
&lt;p&gt;Some screenshots:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/images/c9-1.png"&gt;Editing a Markdown file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/images/c9-2.png"&gt;Terminal console&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The one problem I kept hitting is that I'm used to Cmd-Tab'ing to get to Sublime, but if it's running in Chrome it's a tab window. I couldn't get used to that. So I used &lt;a href="https://www.lessannoyingcrm.com/blog/2010/08/149/Create+application+shortcuts+in+Google+Chrome+on+a+Mac"&gt;this shell script&lt;/a&gt; to create a dedicated Cloud9 Chromium app.&lt;/p&gt;
&lt;p&gt;The container is accessible from the outside, so if you launch a webserver on localhost:8080, you can open it from your browser at https://&lt;project&gt;-&lt;username&gt;.c9users.io. At first I thought this is public so anybody can see it, but they check whether you're logged into c9, so actually only you can see it---nice!&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://c9.io/pricing/webide"&gt;business model&lt;/a&gt; is freemium. In the free tier can run 1 private docker container (with 1G space); you can launch more, but they will be public, meaning anybody can see your files. For $20/month you get unlimited private containers, and 10G disks (see &lt;a href="https://docs.c9.io/docs/reaching-resource-limits"&gt;reaching resource limits&lt;/a&gt;). I've been using the free tier, right now I'm at 50% disk usage. Most of it is stuff I install to get something working (&lt;code&gt;apt-get&lt;/code&gt;, &lt;code&gt;pip install&lt;/code&gt;, etc).&lt;/p&gt;
&lt;p&gt;This also enables a nice "separation of concerns": you can work on your personal projects on a computer (eg. work computer) without storing any files on it.&lt;/p&gt;</content><category term="coding"></category><category term="ide"></category><category term="c9"></category></entry><entry><title>Luigi vs Airflow vs Pinball</title><link href="/luigi-airflow-pinball.html" rel="alternate"></link><published>2016-02-06T00:00:00+01:00</published><updated>2016-02-06T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-02-06:/luigi-airflow-pinball.html</id><summary type="html">&lt;p&gt;A spreadsheet comparing the three opensource workflow tools for ETL.&lt;/p&gt;</summary><content type="html">&lt;p&gt;After reviewing these three ETL worflow frameworks, I compiled a table comparing them. Here's the original &lt;a href="https://docs.google.com/spreadsheets/d/1KCXtuht_wZPFROFwdeg7IXrNPUhFI277y4h-xnc8mgk/edit#gid=0"&gt;Gdoc spreadsheet&lt;/a&gt;. If I had to build a new ETL system today from scratch, &lt;strong&gt;I would use Airflow&lt;/strong&gt;. If you find any mistakes, please let me know at &lt;a href="mailto:mtrencseni@gmail.com"&gt;mtrencseni@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;style type="text/css"&gt;.ritz .waffle a { color: inherit; }.ritz .waffle .s1{border-bottom:1px SOLID #000000;text-align:center;font-weight:bold;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s4{text-align:center;color:#000000;background-color:#fce5cd;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s6{text-align:center;color:#000000;background-color:#d9ead3;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s9{text-align:left;color:#000000;background-color:#ffffff;font-family:'arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s0{border-bottom:1px SOLID #000000;border-right:1px SOLID #000000;text-align:left;font-weight:bold;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s3{text-align:center;text-decoration:underline;color:#1155cc;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s5{text-align:center;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s8{text-align:center;color:#000000;background-color:#d9ead3;font-family:'arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s2{border-right:1px SOLID #000000;text-align:left;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s7{border-right:1px SOLID #000000;text-align:left;font-weight:bold;text-decoration:underline;color:#000000;background-color:#ffffff;font-family:'Arial';font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}
.row-headers-background {display:none;}
.column-headers-background {display:none;}
.freezebar-cell {display:none;}
&lt;/style&gt;

&lt;div class="ritz grid-container" dir="ltr"&gt;&lt;table class="waffle" cellspacing="0" cellpadding="0"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th class="row-header freezebar-vertical-handle"&gt;&lt;/th&gt;&lt;th id="0C0" style="width:183px" class="column-headers-background"&gt;A&lt;/th&gt;&lt;th id="0C1" style="width:295px" class="column-headers-background"&gt;B&lt;/th&gt;&lt;th id="0C2" style="width:329px" class="column-headers-background"&gt;C&lt;/th&gt;&lt;th id="0C3" style="width:295px" class="column-headers-background"&gt;D&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R0" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;1&lt;/div&gt;&lt;/th&gt;&lt;td class="s0"&gt;&lt;/td&gt;&lt;td class="s1" dir="ltr"&gt;Luigi&lt;/td&gt;&lt;td class="s1" dir="ltr"&gt;Airflow&lt;/td&gt;&lt;td class="s1" dir="ltr"&gt;Pinball&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th style="height:3px" class="freezebar-cell freezebar-horizontal-handle"&gt;&lt;/th&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;td class="freezebar-cell"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R1" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;2&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;repo&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://github.com/spotify/luigi"&gt;https://github.com/spotify/luigi&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://github.com/airbnb/airflow"&gt;https://github.com/airbnb/airflow&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://github.com/pinterest/pinball"&gt;https://github.com/pinterest/pinball&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R2" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;3&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;docs&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://luigi.readthedocs.org"&gt;http://luigi.readthedocs.org&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="https://airflow.readthedocs.org"&gt;https://airflow.readthedocs.org&lt;/a&gt;&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;none&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R3" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;4&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;my review&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://bytepawn.com/luigi.html"&gt;http://bytepawn.com/luigi.html&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://bytepawn.com/airflow.html"&gt;http://bytepawn.com/airflow.html&lt;/a&gt;&lt;/td&gt;&lt;td class="s3" dir="ltr"&gt;&lt;a target="_blank" href="http://bytepawn.com/pinball.html"&gt;http://bytepawn.com/pinball.html&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R4" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;5&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;github forks&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;750&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;345&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;58&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R5" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;6&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;github stars&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;4029&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;1798&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;506&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R6" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;7&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;github watchers&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;319&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;166&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;47&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R7" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;8&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;commits in last 30 days&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots of commits&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots of commits&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;3 commits&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R8" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;9&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;architecture&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R9" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;10&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;web dashboard&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;not really, minimal&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;very nice&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R10" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;11&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;code/dsl&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;code&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;code&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python dict + python code&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R11" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;12&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;files/datasets&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, targets&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;not really, as special tasks&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R12" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;13&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;calendar scheduling&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no, use cron&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, LocalScheduler&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R13" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;14&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;datadoc&amp;#39;able [1]&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;maybe, doesn&amp;#39;t really fit&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;probably, by convention&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, dicts would be easy to parse&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R14" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;15&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;backfill jobs&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R15" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;16&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;persists state&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;kindof&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, to db&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, to db&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R16" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;17&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;tracks history&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, in db&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, in db&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R17" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;18&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;code shipping&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, pickle&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;workflow is shipped using pickle, jobs are not?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R18" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;19&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;priorities&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R19" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;20&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;parallelism&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, workers, threads per workers&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, workers&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R20" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;21&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;control parallelism&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, resources&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, pools&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R21" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;22&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;cross-dag deps&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, using targets&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, using sensors&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R22" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;23&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;finds new deployed tasks&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R23" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;24&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;executes dag&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no, have to create special sink task&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R24" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;25&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;multiple dags&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no, just one&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes, also several dag instances (dagruns)&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R25" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;26&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;scheduler/workers&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R26" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;27&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;starting workers&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;users start worker procceses&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;scheduler spawns workers processes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;users start worker procceses&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R27" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;28&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;comms&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;scheduler&amp;#39;s HTTP API&lt;/td&gt;&lt;td class="s8" dir="ltr"&gt;minimal, in state db&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;through master module using Swift&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R28" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;29&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;workers execute&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;worker can execute tasks that is has locally&lt;/td&gt;&lt;td class="s8" dir="ltr"&gt;worker reads pickled tasks from db&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;worker can execute tasks that is has locally?&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R29" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;30&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;contrib&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R30" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;31&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;hadoop&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R31" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;32&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;pig&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;doc mentions PigOperator, it&amp;#39;s not in the source&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R32" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;33&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;hive&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R33" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;34&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;pgsql&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R34" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;35&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;mysql&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R35" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;36&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;redshift&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R36" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;37&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;s3&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R37" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;38&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;source&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R38" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;39&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;written in&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;python&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R39" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;40&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;loc&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;18,000&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;21,000&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;18,000&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R40" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;41&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;tests&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;minimal&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;lots&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R41" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;42&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;maturity&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;fair&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;low&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;low&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R42" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;43&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;other serious users&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;not really&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R43" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;44&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;pip install&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;broken&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R44" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;45&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;niceties&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;-&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;sla, xcom, variables, trigger rules, celery, charts&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;pass data between jobs&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R45" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;46&lt;/div&gt;&lt;/th&gt;&lt;td class="s7" dir="ltr"&gt;does it for you&lt;/td&gt;&lt;td class="s5" dir="ltr"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;td class="s5"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R46" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;47&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;sync tasks to workers&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R47" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;48&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;scheduling&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R48" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;49&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;monitoring&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R49" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;50&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;alerting&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;slas, but probably not enough&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;sends emails&lt;/td&gt;&lt;/tr&gt;&lt;tr style='height:20px;'&gt;&lt;th id="0R50" style="height: 20px;" class="row-headers-background"&gt;&lt;div class="row-header-wrapper" style="line-height: 20px;"&gt;51&lt;/div&gt;&lt;/th&gt;&lt;td class="s2" dir="ltr"&gt;dashboards&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;no&lt;/td&gt;&lt;td class="s6" dir="ltr"&gt;yes&lt;/td&gt;&lt;td class="s4" dir="ltr"&gt;yes&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;

&lt;script type='text/javascript'&gt;
function posObj(sheet, id, row, col, x, y) {
  var rtl = false;
  var sheetElement = document.getElementById(sheet);
  if (!sheetElement) {
    sheetElement = document.getElementById(sheet + '-grid-container');
  }
  if (sheetElement) {
    rtl = sheetElement.getAttribute('dir') == 'rtl';
  }
  var r = document.getElementById(sheet+'R'+row);
  var c = document.getElementById(sheet+'C'+col);
  if (r &amp;&amp; c) {
    var objElement = document.getElementById(id);
    var s = objElement.style;
    var t = y;
    while (r) {
      t += r.offsetTop;
      r = r.offsetParent;
    }
    var offsetX = x;
    while (c) {
      offsetX += c.offsetLeft;
      c = c.offsetParent;
    }
    if (rtl) {
      offsetX -= objElement.offsetWidth;
    }
    s.left = offsetX + 'px';
    s.top = t + 'px';
    s.display = 'block';
    s.border = '1px solid #000000';
  }
};
function posObjs() {
};
posObjs();&lt;/script&gt;

&lt;p&gt;[1] By datadoc'able I mean: could you write a script which reads and parses the ETL jobs, and generates a nice documentation about your datasets and which ETL jobs read/write them. At Prezi we did this, we called it datadoc.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="luigi"></category><category term="airflow"></category><category term="pinball"></category></entry><entry><title>Pinball review</title><link href="/pinball.html" rel="alternate"></link><published>2016-02-06T00:00:00+01:00</published><updated>2016-02-06T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-02-06:/pinball.html</id><summary type="html">&lt;p&gt;Pinball is an ETL tool written by Pinterest. Like Airflow, it supports defining tasks and dependencies as Python code, executing and scheduling them, and distributing tasks across worker nodes. It supports calendar scheduling (hourly/daily jobs, also visualized on the web dashboard). Unfortunately, I found Pinball has very little documentation, very few recent commits in the Github repo and few meaningful answers to Github issues by maintainers, while it's architecture is complicated and undocumented.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/pinterest/pinball"&gt;Pinball&lt;/a&gt; is Pinterest’s open sourced workflow manager / ETL system. It supports defining several workflows (DAGs) consisting of jobs, and dependencies within jobs. Workflows are defined using a combination of declarative-style Python dictionary objects (like JSON) and Python code referenced in these objects. Pinball comes with a dashboard for checking currently running and past workflows.&lt;/p&gt;
&lt;p&gt;This review will be shorter than the previous &lt;a href="/luigi.html"&gt;Luigi&lt;/a&gt; and &lt;a href="/airflow.html"&gt;Airflow&lt;/a&gt; reviews, because Pinball turned out to be not very interesting to me for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Very little &lt;a href="https://github.com/pinterest/pinball#installation"&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Very &lt;a href="https://github.com/pinterest/pinball/commits/master"&gt;few recent commits&lt;/a&gt; in the Github repo&lt;/li&gt;
&lt;li&gt;Very &lt;a href="https://github.com/pinterest/pinball/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aissue"&gt;few meaningful answers&lt;/a&gt; to Github issues from the maintainers&lt;/li&gt;
&lt;li&gt;Complicated and undocumented architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately &lt;code&gt;pip install pinball&lt;/code&gt; doesn’t work and &lt;a href="https://github.com/pinterest/pinball/issues/9"&gt;the maintainers don’t care&lt;/a&gt;, so I didn't invest time in actually trying out Pinball, I just read the source code. Since this review is short and opinionated, I recommend also reading the Pinterest posts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://engineering.pinterest.com/blog/pinball-building-workflow-management"&gt;Pinball: Building workflow management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://engineering.pinterest.com/blog/open-sourcing-pinball"&gt;Open-sourcing Pinball&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Pinball has a modularized architecture. There are 5 modules:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Master (sits on the DB)&lt;/li&gt;
&lt;li&gt;Scheduler (also accessed DB)&lt;/li&gt;
&lt;li&gt;Worker (also accessed DB)&lt;/li&gt;
&lt;li&gt;UI web server (also accessed DB)&lt;/li&gt;
&lt;li&gt;Command-line&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The master module sits on top of a &lt;a href="https://www.mysql.com/"&gt;Mysql&lt;/a&gt; database (no others supported) and uses &lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; for ORM. The master exposes a synchronization token API using &lt;a href="https://thrift.apache.org/"&gt;Thrift&lt;/a&gt; to the other modules, and that’s all the master does. I think this is an unnecessary layer of abstraction; the Airflow design decision is better: everybody sees the DB and uses that to communicate, get &lt;a href="https://en.wikipedia.org/wiki/ACID"&gt;ACID&lt;/a&gt; for free; no need to define and maintain an API, no need for Thrift. In the blog post, they say &lt;em&gt;“component-wise design allows for easy alterations”&lt;/em&gt;, eg. you could write a different scheduler implementation. But:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Who’d ever want to write a different scheduler implementation? I'm using an opensource project to avoid writing my own ETL system.&lt;/li&gt;
&lt;li&gt;You can change the code in other architectures as well as long as it’s modularized.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moving on, the other daemon modules are the scheduler, the worker and the UI web server. The scheduler performs calendar scheduling of workflows. The workers actually execute individual jobs.&lt;/p&gt;
&lt;p&gt;An important piece of the Pinball architecture are tokens. Tokens are basically records, and the collection of all tokens is the system state. Unfortunately the different sort of tokens are not documented, and since Python is dynamic, there’s also no usable documentation in the code (eg. a header file in C++). Tokens have a &lt;code&gt;data&lt;/code&gt; member, and Python objects are pickled and stored there on the fly as the state.
At first when I read the blog posts and code, I saw &lt;a href="https://github.com/pinterest/pinball/blob/master/pinball_system.png"&gt;this diagram&lt;/a&gt; and &lt;a href="https://engineering.pinterest.com/sites/default/files/article/fields/field_image/tumblr_inline_mzxiegqh5c1s1gqll.png"&gt;then this&lt;/a&gt;, and I thought that only the master accesses the database, and the scheduler and workers don’t, everything goes through the master using tokens. But actually that’s not true, I think the architecture is  everybody accesses the database for reads (as an optimization), but only the master writes to the database. This seems like a leaky abstraction, and again it’s not clear why the modules can’t use the DB to communicate state, why the need for Thrift. Relevant parts from the &lt;a href="https://engineering.pinterest.com/blog/pinball-building-workflow-management"&gt;blog post&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Every state (token) change goes through the master and gets committed to the persistent store before the worker request returns… workers can read archived tokens directly from the persistent storage, bypassing the master, greatly improving system scalability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;An interesting design decision is separation of workflow description, which is given in Python dictionaries, and the actual job codes. &lt;a href="https://github.com/pinterest/pinball/tree/master/pinball_ext/examples"&gt;See example here.&lt;/a&gt; It’s a bit wierd that the workflow references the actual job using a string. I think this is because many modules load the workflow (eg. scheduler), but only the workers actually load the jobs.&lt;/p&gt;
&lt;h2&gt;Contrib stuff&lt;/h2&gt;
&lt;p&gt;Pinball has contrib stuff for the following job types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;S3 (also EMR)&lt;/li&gt;
&lt;li&gt;Hadoop, Hive&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.qubole.com/"&gt;Qubole&lt;/a&gt; (a data processing platform-as-a-service Pinterest uses)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are no connectors to Postgres, Mysql, Redshift, Presto or any SQL databases.&lt;/p&gt;
&lt;h2&gt;Source code and tests&lt;/h2&gt;
&lt;p&gt;The main codebase is ~18,000 LOC (python), plus about ~7,000 lines of unit test code. Other Python libraries used on the server side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://thrift.apache.org/"&gt;Thrift&lt;/a&gt; for RPC&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tox.readthedocs.org/"&gt;Tox&lt;/a&gt; for testing&lt;/li&gt;
&lt;li&gt;and a few more…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think it’s cool that Pinball doesn’t have many library dependencies; for a Python project, it barely has any.&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;If I had to build an ETL system from scratch today, I would not use Pinball. It’s not documented, not a lot of commits, can't find other users, and I'm suspicious of the architecture. I would use Airflow.&lt;/p&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="pinball"></category></entry><entry><title>How to make a blog like this</title><link href="/how-to-make-a-blog-like-this.html" rel="alternate"></link><published>2016-01-07T00:00:00+01:00</published><updated>2016-01-07T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-01-07:/how-to-make-a-blog-like-this.html</id><summary type="html">&lt;p&gt;Make a simple blog with Github Pages and Pelican.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Getting your own blog like this is really easy, no server hosting nedded. There are two ingredients:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Github Pages&lt;/li&gt;
&lt;li&gt;the Pelican static blog generator&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Github Pages&lt;/h2&gt;
&lt;p&gt;Suppose your username on github is mtrencseni. Create a repo called &lt;code&gt;mtrencseni.github.io&lt;/code&gt;. Here's mine: &lt;a href="https://github.com/mtrencseni/mtrencseni.github.io"&gt;https://github.com/mtrencseni/mtrencseni.github.io&lt;/a&gt;. Anything you put in there will be served up at &lt;a href="http://mtrencseni.github.io"&gt;http://mtrencseni.github.io&lt;/a&gt;. Try it out for youself, put in an index.html containing &lt;code&gt;Hello world&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Your own domain name with Github Pages&lt;/h2&gt;
&lt;p&gt;I wanted to use my existing domain name &lt;code&gt;bytepawn.com&lt;/code&gt;. Github is so nice, they support this. Put a file called &lt;code&gt;CNAME&lt;/code&gt; into your repo. Here's mine: &lt;a href="https://github.com/mtrencseni/mtrencseni.github.io/blob/master/CNAME"&gt;https://github.com/mtrencseni/mtrencseni.github.io/blob/master/CNAME&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tells Github to expect requests for this domain. What's left is to tell your DNS provider to use Github. (My registrar is Internet.bs, so much better than Godaddy.) Create an A record that points to &lt;code&gt;192.30.252.153&lt;/code&gt;, this is a Github IP address. That's it. For &lt;code&gt;bytepawn.com&lt;/code&gt; it looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ dig bytepawn.com

&lt;span class="p"&gt;;&lt;/span&gt; &amp;lt;&amp;lt;&amp;gt;&amp;gt; DiG &lt;span class="m"&gt;9&lt;/span&gt;.9.5-3ubuntu0.5-Ubuntu &amp;lt;&amp;lt;&amp;gt;&amp;gt; bytepawn.com
&lt;span class="p"&gt;;;&lt;/span&gt; global options: +cmd
&lt;span class="p"&gt;;;&lt;/span&gt; Got answer:
&lt;span class="p"&gt;;;&lt;/span&gt; -&amp;gt;&amp;gt;HEADER&lt;span class="s"&gt;&amp;lt;&amp;lt;- opco&lt;/span&gt;de: QUERY, status: NOERROR, id: &lt;span class="m"&gt;34353&lt;/span&gt;
&lt;span class="p"&gt;;;&lt;/span&gt; flags: qr rd ra&lt;span class="p"&gt;;&lt;/span&gt; QUERY: &lt;span class="m"&gt;1&lt;/span&gt;, ANSWER: &lt;span class="m"&gt;1&lt;/span&gt;, AUTHORITY: &lt;span class="m"&gt;0&lt;/span&gt;, ADDITIONAL: &lt;span class="m"&gt;1&lt;/span&gt;

&lt;span class="p"&gt;;;&lt;/span&gt; OPT PSEUDOSECTION:
&lt;span class="p"&gt;;&lt;/span&gt; EDNS: version: &lt;span class="m"&gt;0&lt;/span&gt;, flags:&lt;span class="p"&gt;;&lt;/span&gt; udp: &lt;span class="m"&gt;512&lt;/span&gt;
&lt;span class="p"&gt;;;&lt;/span&gt; QUESTION SECTION:
&lt;span class="p"&gt;;&lt;/span&gt;bytepawn.com.                  IN      A

&lt;span class="p"&gt;;;&lt;/span&gt; ANSWER SECTION:
bytepawn.com.           &lt;span class="m"&gt;600&lt;/span&gt;     IN      A       &lt;span class="m"&gt;192&lt;/span&gt;.30.252.153

&lt;span class="p"&gt;;;&lt;/span&gt; Query time: &lt;span class="m"&gt;19&lt;/span&gt; msec
&lt;span class="p"&gt;;;&lt;/span&gt; SERVER: &lt;span class="m"&gt;172&lt;/span&gt;.17.0.1#53&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;172&lt;/span&gt;.17.0.1&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;;;&lt;/span&gt; WHEN: Thu Jan &lt;span class="m"&gt;07&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;:36:09 UTC &lt;span class="m"&gt;2016&lt;/span&gt;
&lt;span class="p"&gt;;;&lt;/span&gt; MSG SIZE  rcvd: &lt;span class="m"&gt;57&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Generating a static blog&lt;/h2&gt;
&lt;p&gt;Github will serve static content from your repo, it doesn't run any scripts.
So you need to use a static site generator and serve up the generated pages.
Fortunately, there are many.
The two biggest ones are &lt;a href="https://jekyllrb.com/"&gt;Jekyll&lt;/a&gt; for Rubyists and &lt;a href="http://docs.getpelican.com"&gt;Pelican&lt;/a&gt; for Pythonistas.
I'm a Python guy, so I use Pelican.
The &lt;a href="http://docs.getpelican.com/en/3.6.3/quickstart.html"&gt;Pelican quickstart doc&lt;/a&gt; explains how to generate an empty blog. Basically:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install pelican markdown
$ &lt;span class="nb"&gt;cd&lt;/span&gt; blog
&lt;span class="c1"&gt;### this is https://github.com/mtrencseni/blog locally&lt;/span&gt;
$ pelican-quickstart
$ vi contents/my-first-article.md
&lt;span class="c1"&gt;### see the docs what an article should look like&lt;/span&gt;
$ pelican content
&lt;span class="c1"&gt;### generates static files in the `output` dir&lt;/span&gt;
$ cp -R ouput/* ../mtrencseni.github.io
$ &lt;span class="nb"&gt;cd&lt;/span&gt; ../mtrencseni.github.io
$ git add * &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; git commit -m &lt;span class="s2"&gt;&amp;quot;Working :)&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; git push
&lt;span class="c1"&gt;### it&amp;#39;s live!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Simply copy the contents of Pelican's &lt;code&gt;output&lt;/code&gt; directory into &lt;code&gt;mtrencseni.github.io&lt;/code&gt;, and that's it.&lt;/p&gt;
&lt;h2&gt;Getting a theme for Pelican&lt;/h2&gt;
&lt;p&gt;The default theme is kind of crappy looking.
Fortunately, there's a ton of free themes for Pelican at &lt;a href="http://www.pelicanthemes.com/"&gt;http://www.pelicanthemes.com/&lt;/a&gt;. Here's the github repo for all those themes: &lt;a href="https://github.com/getpelican/pelican-themes"&gt;https://github.com/getpelican/pelican-themes&lt;/a&gt;.
I picked &lt;a href="https://github.com/alexandrevicenzi/Flex"&gt;Flex&lt;/a&gt;, and did some very minor customization on it.
Getting Pelican to use a theme is simple: put a line like &lt;code&gt;THEME = 'flex'&lt;/code&gt; into your &lt;code&gt;pelicanconf.py&lt;/code&gt;, where &lt;code&gt;flex&lt;/code&gt; is the directory containing the theme you picked. Here is mine: &lt;a href="https://github.com/mtrencseni/blog/blob/master/pelicanconf.py"&gt;https://github.com/mtrencseni/blog/blob/master/pelicanconf.py&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;I have a repo &lt;a href="https://github.com/mtrencseni/blog"&gt;https://github.com/mtrencseni/blog&lt;/a&gt; which contains the source of the blog. It's a copy of Pelican, with my articles in the &lt;code&gt;content&lt;/code&gt; directory, and a (customized) copy of the flex theme in the &lt;code&gt;flex&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;I use Pelican to generate the static output, and then that gets pushed to &lt;a href="https://github.com/mtrencseni/mtrencseni.github.io"&gt;https://github.com/mtrencseni/mtrencseni.github.io&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I customized the Makefile to automate this. Here's mine: &lt;a href="https://github.com/mtrencseni/blog/blob/master/Makefile"&gt;https://github.com/mtrencseni/blog/blob/master/Makefile&lt;/a&gt;. It contains targets for &lt;code&gt;clean&lt;/code&gt;, &lt;code&gt;output&lt;/code&gt;, &lt;code&gt;preview&lt;/code&gt; and &lt;code&gt;publish&lt;/code&gt;. &lt;code&gt;preview&lt;/code&gt; uses the built in Python webserver to serve up the static site on localhost:8080 for testing. &lt;code&gt;publish&lt;/code&gt; git commits to the &lt;code&gt;mtrencseni.github.io&lt;/code&gt; repo. &lt;/p&gt;
&lt;p&gt;That's it. You can get up an running within a day.&lt;/p&gt;
&lt;h2&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pages.github.com/"&gt;Github Pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://help.github.com/articles/setting-up-a-custom-domain-with-github-pages/"&gt;Custom domain with Github pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/9082499/custom-domain-for-github-project-pages"&gt;Stackoverflow help on custom domain setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docs.getpelican.com"&gt;Pelican&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pelicanthemes.com/"&gt;Pelican themes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category><category term="pelican"></category></entry><entry><title>Airflow review</title><link href="/airflow.html" rel="alternate"></link><published>2016-01-06T00:00:00+01:00</published><updated>2016-01-06T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-01-06:/airflow.html</id><summary type="html">&lt;p&gt;Airflow is a workflow scheduler written by Airbnb. It supports defining tasks and dependencies as Python code, executing and scheduling them, and distributing tasks across worker nodes. It supports calendar scheduling (hourly/daily jobs, also visualized on the web dashboard), so it can be used as a starting point for traditional ETL. It has a nice web dashboard for seeing current and past task state, querying the history and making changes to metadata such as connection strings.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Airflow is a workflow scheduler written by Airbnb. It supports defining tasks and dependencies as Python code, executing and scheduling them, and distributing tasks across worker nodes. It supports calendar scheduling (hourly/daily jobs, also visualized on the web dashboard), so it can be used as a starting point for traditional ETL. It has a nice web dashboard for seeing current and past task state, querying the history and making changes to metadata such as connection strings.
I wrote this after my &lt;a href="/luigi.html"&gt;Luigi review&lt;/a&gt;, so I make comparisons to Luigi throughout the article.&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Airflow is designed to store and persist its state in a relational database such as Mysql or Postgresql. It uses &lt;a href="http://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt; for abstracting away the choice of and querying the database. As such much of the logic is implemented as database calls.
It would be fair to call the core of Airflow “an SQLAlchemy app”. This allows for very clean separation of high-level functionality, such as persisting the state itself (done by the database itself), and scheduling, web dashboard, etc.&lt;/p&gt;
&lt;p&gt;Similarly to Luigi, workflows are specified as a DAG of tasks in Python code. But there are many differences. Luigi knows that tasks operate on targets (datasets, files) and includes this abstraction; eg. it checks the existence of targets when deciding whether to run a task (if all output targets exists, there’s no need to run the task). This concept is missing from Airflow, it never checks for the existence of targets to decide whether to run a task. Like in Luigi, tasks depend on each other (and not on datasets). Unlike Luigi, Airflow supports the concept of calendar scheduling, ie. you can specify that a DAG should run every hour or every day, and the Airflow scheduler process will execute it. Unlike Luigi, Airflow supports shipping the task’s code around to different nodes using &lt;code&gt;pickle&lt;/code&gt;, ie. Python binary serialization.&lt;/p&gt;
&lt;p&gt;Airflow also has a webserver which shows dashboards and lets users edit metadata like connection strings to data sources. Since everything is stored in the database, the web server component of Airflow is an independent &lt;a href="http://gunicorn.org/"&gt;gunicorn&lt;/a&gt; process which reads and writes the database.&lt;/p&gt;
&lt;h2&gt;Execution&lt;/h2&gt;
&lt;p&gt;In Airflow, the unit of execution is a &lt;code&gt;Task&lt;/code&gt;. DAG’s are made up of tasks, one &lt;code&gt;.py&lt;/code&gt; file is a DAG. &lt;a href="http://pythonhosted.org/airflow/tutorial.html"&gt;See tutorial.&lt;/a&gt; Although you can tell Airflow to execute just one task, the common thing to do is to load a DAG, or all DAGs in a subdirectory. Airflow loads the &lt;code&gt;.py&lt;/code&gt; file and looks for instances of class &lt;code&gt;DAG&lt;/code&gt;. DAGs are identified by the textual &lt;code&gt;dag_id&lt;/code&gt; given to them in the &lt;code&gt;.py&lt;/code&gt; file. This is important, because this is used to identify the DAG (and it’s hourly/daily instances) throughout Airflow; changing the &lt;code&gt;dag_id&lt;/code&gt; will break dependencies in the state!&lt;/p&gt;
&lt;p&gt;The DAG contains the first date when these tasks should (have been) run (called &lt;code&gt;start_date&lt;/code&gt;), the recurrence interval if any (called &lt;code&gt;schedule_interval&lt;/code&gt;), and whether the subsequent runs should depend on each other (called &lt;code&gt;depends_on_past&lt;/code&gt;). Airflow will interleave slow running DAG instances, ie. it will start the next hour’s jobs even if the last hour hasn’t completed, as long as dependencies permit and overlap limits permit. An instance of a &lt;code&gt;DAG&lt;/code&gt;, eg. one that is running for 2016-01-01 06:00:00 is called a &lt;code&gt;DAGRun&lt;/code&gt;. A &lt;code&gt;DAGRun&lt;/code&gt; is identified by the id of the DAG postfixed by the &lt;code&gt;execution_date&lt;/code&gt; (not when it’s running, ie. not &lt;code&gt;now()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Tasks, like DAGs are also identified by a textual id. Internally, instances of tasks are instances of &lt;code&gt;TaskInstance&lt;/code&gt;, identified by the task’s &lt;code&gt;task_id&lt;/code&gt; plus the &lt;code&gt;execution_date&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The tasks in a DAG may define dependencies on other tasks using &lt;code&gt;set_upstream()&lt;/code&gt; and &lt;code&gt;set_downstream()&lt;/code&gt;. Airflow will raise an exception when it finds cycles in the DAG.&lt;/p&gt;
&lt;p&gt;A task is a parameterized operator. Airflow provides many types of operators, such as &lt;code&gt;BashOperator&lt;/code&gt; for executing a bash script, &lt;code&gt;HiveOperator&lt;/code&gt; for executing Hive queries, and so on. All these operators derive from &lt;code&gt;BaseOperator&lt;/code&gt;. In line with Airflow being “an SQLAlchemy app”, &lt;code&gt;BaseOperator&lt;/code&gt; is derived from SQLAlquemy's &lt;code&gt;Base&lt;/code&gt; class, so objects can be pushed to the database; this pattern happens throughout Airflow. Operators don’t actually contain the database specific API calls (eg. for Hive or Mysql); this logic is contained in hooks, eg. class &lt;code&gt;HiveCliHook&lt;/code&gt;. All hooks are derived from class &lt;code&gt;BaseHook&lt;/code&gt;, a common interface for connecting and executing queries. So, whereas Luigi has one &lt;code&gt;Target&lt;/code&gt; class (and subclasses), in Airflow this logic is distributed into operators and hooks.&lt;/p&gt;
&lt;p&gt;There are 3 main type of operators (all three use the same hook classes to accomplish their job):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Sensor:&lt;/strong&gt; Waits for events to happen. This could be a file appearing in HDFS, the existence of a Hive partition, or waiting for an arbitrary MySQL query to return a row.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Remote Execution:&lt;/strong&gt; Triggers an operation in a remote system. This could be an HQL statement in Hive, a Pig script, a map reduce job, a stored procedure in Oracle or a Bash script to run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data transfers:&lt;/strong&gt; Move data from one system to another. Push data from Hive to MySQL, from a local file to HDFS, from Postgres to Oracle, or anything of that nature.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most interesting are sensors. They allow tasks to depend on special “sensor tasks”, which are actually files or datasets. A sensor let’s you specify how often it should be checked (default 1 minute), and when it should time out (default 1 week). These are all derived from class &lt;code&gt;BaseSensorOperator&lt;/code&gt;. There is a special sensor called &lt;code&gt;ExternalTaskSensor&lt;/code&gt;, which lets a task depend on another task (specified by a &lt;code&gt;dag_id&lt;/code&gt; and a &lt;code&gt;task_id&lt;/code&gt; and &lt;code&gt;execution_date&lt;/code&gt;) in another DAG, since this is not supported “by default”. &lt;code&gt;ExternalTaskSensor&lt;/code&gt; actually just checks what the specified record looks like in the Airflow state database.&lt;/p&gt;
&lt;p&gt;All operators have a &lt;code&gt;trigger_rule&lt;/code&gt; argument which defines the rule by which the generated task get triggered. The default value for &lt;code&gt;trigger_rule&lt;/code&gt; is &lt;code&gt;all_success&lt;/code&gt; and can be defined as “trigger this task when all directly upstream tasks have succeeded. Others are: &lt;code&gt;all_failed&lt;/code&gt;, &lt;code&gt;all_done&lt;/code&gt;, &lt;code&gt;one_failed&lt;/code&gt;, &lt;code&gt;one_success&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Scheduling and executors&lt;/h2&gt;
&lt;p&gt;Recap: Airflow supports calendar scheduling (hour/daily tasks). Each such run is an instance of a DAG (internally, a &lt;code&gt;DAGRun&lt;/code&gt; object), with tasks and their dependencies. As mentioned previously, DAGs can depend on their previous runs (&lt;code&gt;depends_on_past&lt;/code&gt;), and additionally, specific task dependencies across DAGs is possible with the &lt;code&gt;ExternalTaskSensor&lt;/code&gt; operator. The maximum number of DAG runs to allow per DAG can be limited with &lt;code&gt;max_active_runs_per_dag&lt;/code&gt; in &lt;code&gt;airflow.cfg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When running Airflow, we have to specify what sort of executor to use in &lt;code&gt;airflow.cfg&lt;/code&gt;: &lt;code&gt;SequentialExecutor&lt;/code&gt;, &lt;code&gt;LocalExecutor&lt;/code&gt; or &lt;code&gt;CeleryExecutor&lt;/code&gt;; all three derive from &lt;code&gt;BaseExecutor&lt;/code&gt;. The sequential executor runs locally in a single process/thread, and waits for each task to finish before starting the next one; it should only be used for testing/debugging. The &lt;code&gt;LocalExecutor&lt;/code&gt; also runs tasks locally, but spawns a new process for each one using &lt;code&gt;subprocess.popen()&lt;/code&gt; to run a new &lt;code&gt;bash&lt;/code&gt;; the maximum number of processes can be configured with &lt;code&gt;parallelism&lt;/code&gt; in &lt;code&gt;airflow.cfg&lt;/code&gt;. Inside the &lt;code&gt;bash&lt;/code&gt;, it runs an &lt;code&gt;airflow&lt;/code&gt;, parameterized to just run the a given &lt;code&gt;dag_id&lt;/code&gt; &lt;code&gt;task_id&lt;/code&gt; &lt;code&gt;execution_date&lt;/code&gt; combination using the &lt;code&gt;airflow&lt;/code&gt; run command line parametrization. The python code belonging to the task is read back from the database (where it was stored by the scheduler using &lt;code&gt;pickle&lt;/code&gt;). The &lt;code&gt;CeleryExecutor&lt;/code&gt; works similarly, except the job is pushed inside a distributed &lt;a href="http://www.celeryproject.org/"&gt;celery&lt;/a&gt; queue.&lt;/p&gt;
&lt;p&gt;When running Airflow, internally a number of jobs are created. A job is a long running something that handles running  smaller units of work; all jobs derive from &lt;code&gt;BaseJob&lt;/code&gt;. There is &lt;code&gt;SchedulerJob&lt;/code&gt;, which manages a single DAG (creates DAG runs, task instances, manages priorities),  &lt;code&gt;BackfillJob&lt;/code&gt; for backfilling a specific DAG, and &lt;code&gt;LocalTaskJob&lt;/code&gt; when running a specific &lt;code&gt;dag_id&lt;/code&gt; &lt;code&gt;task_id&lt;/code&gt; &lt;code&gt;execution_date&lt;/code&gt; combination (as requested by the &lt;code&gt;LocalExecutor&lt;/code&gt; or the &lt;code&gt;CeleryExecutor&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;When running the airflow scheduler, the &lt;code&gt;SchedulerJob&lt;/code&gt; supports loading DAGs from a folder: in this case, new code added/changed is automatically detected and loaded. This is very convenient, because new code just has to be placed on the production server, and it’s automatically picked up by Airflow.&lt;/p&gt;
&lt;p&gt;So in Airflow there is no need to start worker processes: workers are spawned as subprocesses by the &lt;code&gt;LocalExecutor&lt;/code&gt; or remotely by celery. Also, more than one scheduler/executor/main process can run, sitting on the main database. When running tasks, Airflow creates a lock in the database to make sure tasks aren’t run twice by schedulers; other parallelism is enforced by unique database keys (eg. only one &lt;code&gt;dag_id&lt;/code&gt; &lt;code&gt;execution_date&lt;/code&gt; combination allowed to avoid schedulers creating multiple &lt;code&gt;DAGRun&lt;/code&gt; copies). &lt;em&gt;Note: I’m not sure what the point would be of running several schedulers, other than redundancy, and whether this truly works without hiccups; the TODO file includes this todo item: “Distributed scheduler”.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Airflow supports pools to limit parallelism of certain types of tasks (eg. limit number of bash jobs, limit number of Hive connections); this is similar to Luigi resources. Priorities are also supported: The default &lt;code&gt;priority_weight&lt;/code&gt; is 1, and can be bumped to any number. When sorting the queue to evaluate which task should be executed next, Airflow uses the &lt;code&gt;priority_weight&lt;/code&gt;, summed up with all of the &lt;code&gt;priority_weight&lt;/code&gt; values from tasks downstream from this task.&lt;/p&gt;
&lt;p&gt;Airflow supports heartbeats. Each job will update a heartbeat entry in the database. If a job hasn’t updated it’s heartbeat for a while, it’s assumed that it has failed and it’s state is set to &lt;code&gt;SHUTDOWN&lt;/code&gt; in the database. This also allows for any job to be killed externally, regardless of who is running it or on which machine it is running. &lt;em&gt;Note: I’m not sure how this works, because from my reading of the code, the actual termination of the process that didn’t send the heartbeat should be performed by the process itself; but if it stuck or blocked and didn’t send a heartbeat, then how will it notice it should shut itself down?&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Other interesting features&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SLAs:&lt;/strong&gt; Service Level Agreements, or time by which a task or DAG should have succeeded, can be set at a task level as a timedelta. If one or many instances have not succeeded by that time, an alert email is sent detailing the list of tasks that missed their SLA. The event is also recorded in the database and made available in the web UI under Browse -&amp;gt; Missed SLAs where events can be analyzed and documented.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;XCom:&lt;/strong&gt; XComs let tasks exchange messages, allowing more nuanced forms of control and shared state. The name is an abbreviation of “cross-communication”. XComs are principally defined by a key, value, and timestamp, but also track attributes like the task/DAG that created the XCom and when it should become visible. Any object that can be pickled can be used as an XCom value, so users should make sure to use objects of appropriate size. XComs can be “pushed” (sent) or “pulled” (received). When a task pushes an XCom, it makes it generally available to other tasks. Tasks can push XComs at any time by calling the &lt;code&gt;xcom_push()&lt;/code&gt; method. In addition, if a task returns a value (either from its Operator’s &lt;code&gt;execute()&lt;/code&gt; method, or from a &lt;code&gt;PythonOperator&lt;/code&gt;’s &lt;code&gt;python_callable()&lt;/code&gt; function), then an XCom containing that value is automatically pushed. Tasks call &lt;code&gt;xcom_pull()&lt;/code&gt; to retrieve XComs, optionally applying filters based on criteria like key, source &lt;code&gt;task_id&lt;/code&gt;s, and source &lt;code&gt;dag_id&lt;/code&gt;. By default, &lt;code&gt;xcom_pull()&lt;/code&gt; filters for the keys that are automatically given to XComs when they are pushed by being returned from execute functions (as opposed to XComs that are pushed manually).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variables:&lt;/strong&gt; Variables are a generic way to store and retrieve arbitrary content or settings as a simple key value store within Airflow. Variables can be listed, created, updated and deleted from the UI (Admin -&amp;gt; Variables) or from code. While your pipeline code definition and most of your constants and variables should be defined in code and stored in source control, it can be useful to have some variables or configuration items accessible and modifiable through the UI.&lt;/p&gt;
&lt;h2&gt;Contrib stuff&lt;/h2&gt;
&lt;p&gt;Like Luigi, Airflow has an impressive library of stock operator classes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash&lt;/li&gt;
&lt;li&gt;Mysql&lt;/li&gt;
&lt;li&gt;Postgresql&lt;/li&gt;
&lt;li&gt;MSSQL&lt;/li&gt;
&lt;li&gt;Hive&lt;/li&gt;
&lt;li&gt;Presto&lt;/li&gt;
&lt;li&gt;HDFS&lt;/li&gt;
&lt;li&gt;S3&lt;/li&gt;
&lt;li&gt;HTTP sensor&lt;/li&gt;
&lt;li&gt;and many more...
Redshift is currently not supported.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Source code and tests&lt;/h2&gt;
&lt;p&gt;The main codebase is ~21,000 LOC (python, js, html), plus  about ~1,200 lines of unit test code.
Other Python libraries used on the server side:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://jinja.pocoo.org/"&gt;Jinja&lt;/a&gt; for templating (why, if we’re using Python code to define jobs anyway?)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gunicorn.org/"&gt;Gunicorn&lt;/a&gt; and &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt; for HTTP&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.python.org/pypi/dill"&gt;Dill&lt;/a&gt; for pickling&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tox.readthedocs.org"&gt;Tox&lt;/a&gt; for testing
and many more...&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Airflow’s design decisions are very close to my heart: the fact that it’s an SQLAlchemy app make managing state, restarting the daemon, or running more in parallel very easy.  It has lots of contrib stuff baked in, so it’s easy to get started. The dashboard is very nice, and also shows historic runs nicely color-coded. If I were to build a new ETL system, I would definitely consider using Airflow (over Luigi, since Airflow has many more features out of the box).&lt;/p&gt;
&lt;p&gt;What I don’t like about Airflow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Apart from special sensor operators, doesn’t deal with files/datasets as inputs/outputs of tasks directly. This I find an odd design decision, as it leads to some complications:&lt;ul&gt;
&lt;li&gt;The state database stores the state of tasks, not the datasets; if the state database is lost, it’s hard to restore the historic state of the ETL, even if all the datasets are there. It’s better to separate datasets and tasks, and represent the historic state of ETL using the state of the datasets&lt;/li&gt;
&lt;li&gt;It’s harder to deal with tasks that appear to finish correctly, but don’t actually produce output, or good output. In the Airflow architecture this problem only shows up later, when a task downstream (hopefully) errors out. This can happen eg. if a bash script forgets to set -e.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I think it’d be better if workers could be started independently, and picked up tasks scheduled by a central scheduler; instead Airflow starts workers centrally.&lt;/li&gt;
&lt;li&gt;Still a work in progress, not many tests, probably will run into bugs in production. Also see the end of &lt;a href="https://medium.com/handy-tech/airflow-tips-tricks-and-pitfalls-9ba53fba14eb#.lzfjq4wx9"&gt;this blog post&lt;/a&gt;, they restart the Airflow process pretty often because of some bug.&lt;/li&gt;
&lt;li&gt;Personally, I'm still not convinced that the ETL-job-as-code is the right way to go.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Links, talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/airbnb/airflow"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://airflow.readthedocs.org"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/airbnb/airflow#links"&gt;Slides from Airflow users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="airflow"></category></entry><entry><title>Systems thinking and system traps</title><link href="/systems-thinking.html" rel="alternate"></link><published>2016-01-06T00:00:00+01:00</published><updated>2016-01-06T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2016-01-06:/systems-thinking.html</id><summary type="html">&lt;p&gt;Thinking in Systems, written by the late Donella Meadows, is a book about how to think about systems, how to control systems and how systems change and control themselves. A system can be anything from a heating furnace to a social system. The gem of the book is the part about system traps. System traps are ways a system can go wrong; examples are drift to low performance, seeking the wrong goals, shifting the burden, etc.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.amazon.com/gp/product/1603580557/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1944687762&amp;amp;pf_rd_s=lpo-top-stripe-1&amp;amp;pf_rd_t=201&amp;amp;pf_rd_i=0123859158&amp;amp;pf_rd_m=ATVPDKIKX0DER&amp;amp;pf_rd_r=181NKCEKSEPQ62PT0S07"&gt;Thinking in Systems&lt;/a&gt;, written by the late &lt;a href="https://en.wikipedia.org/wiki/Donella_Meadows"&gt;Donella Meadows&lt;/a&gt;, is a book about how to think about systems, how to control systems and how systems change and control themselves. A system can be anything from a heating furnace to a social system. The book is conceptual, there’s not a single equation in it, it's not about differential equations or control theory.&lt;/p&gt;
&lt;h2&gt;System traps&lt;/h2&gt;
&lt;p&gt;The gem of the book is the part about &lt;em&gt;system traps&lt;/em&gt;. System traps are ways a system can go wrong. It’s really interesting to read about system traps and then notice and observe them in action: in micro environments such as a company and in macro environments such as an industry or a  country. Here’s a list of the most interesting system traps from the book, with some examples.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Policy resistance:&lt;/strong&gt; The inherent resistance of the establishment to allow changes to affect the system. People would rather live with a flawed system that is familiar then to allow changes that might cause uncertainty and instability. Such resistance can cause inevitable collapse to be more dramatic, sometime even catastrophic.&lt;br&gt;
Example: &lt;a href="http://www.wsj.com/articles/obama-to-discuss-gun-control-options-with-attorney-general-1451646004"&gt;US citizens resisting Obama’s gun control changes.&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Drift to low performance:&lt;/strong&gt; The notion that prolonged failures causes acceptance of the new state of things, “new normal”.&lt;br&gt;
Example: a great example is soccer in Hungary. Hungary used to have a very strong soccer culture, but over time quality decayed to the point where today, a draw or only getting defeated by 1 goal is considered a good result. All this even though the hungarian government is investing large amounts into the sport. The root cause for this sustained drift to low performance seems to be that soccer is used as a way to channel money from the government to private individuals, ie. corruption.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Seeking wrong goals:&lt;/strong&gt; Sometime goals change. Many systems suffer from the fact that original goal don’t make any sense in the current context, or never did. Pursuit of wrong goals will cause the system pursue these goals, capturing wrong or insignificant metrics, leaving the illusion of progress, while heading toward system collapse.&lt;br&gt;
Example: &lt;a href="http://techcrunch.com/2011/07/30/vanity-metrics/"&gt;startups seeking to increase vanity metrics&lt;/a&gt; such as registered users and bookings instead of engagement and profits.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Shifting the burden:&lt;/strong&gt; Notion that risk is shifted to someone else, while success is reaped by the actor.&lt;br&gt;
Example: &lt;a href="https://hbr.org/2013/05/six-myths-about-venture-capitalists"&gt;venture capitalists and hedge fund managers work under a model&lt;/a&gt; where they get a nice base salary, a nice bonus if their fund performs well, but there is no downside for them. Turn around times are on the order of 10 years, so there’s little historic data on fund manager’s performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The tragedy of commons:&lt;/strong&gt; This is classic economic theory, described in terms of system thinking. The common, defined as community space – such as a town common, is a shared resource. This resource can be governed by community standards, privatization or effective regulation. Each approach has tradeoffs and benefits. It’s the conclusion of the [Donella Meadows] that only regulation is effective since the community standards are usually not enough.&lt;br&gt;
Example: &lt;a href="https://en.wikipedia.org/wiki/Cybersquatting"&gt;domain name squatting&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Success to the Successful:&lt;/strong&gt; The notion that success will give advantage to those that have already succeeded, thus limiting the “losers” ability to win in the future. “The rich get richer!”&lt;br&gt;
Example: that’s how it is everywhere, see &lt;a href="http://www.economist.com/blogs/economist-explains/2014/05/economist-explains"&gt;Thomas Piketty’s book Capital&lt;/a&gt;. A more specific example is entrepreneurs who’ve had a successful startup previous have an easier time raising money for their next startup. Surprisingly, &lt;a href="https://hbr.org/2014/02/research-serial-entrepreneurs-arent-any-more-likely-to-succeed/"&gt;data doesn’t show a correlation between past and future success&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Rule beating:&lt;/strong&gt; The notion that rules, or laws, are ignored, broken or skirted. The cause of rule breaking is usually related to the fact that these rules are perceived as unjust or not flexible enough wrt real life issues.&lt;br&gt;
Example: there’s a whole industry called &lt;a href="https://en.wikipedia.org/wiki/Search_engine_optimization"&gt;SEO&lt;/a&gt; to &lt;a href="https://www.quora.com/What-techniques-do-websites-use-to-game-the-Google-search-engine"&gt;game search engine rankings&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="systems"></category><category term="books"></category></entry><entry><title>Luigi review</title><link href="/luigi.html" rel="alternate"></link><published>2015-12-20T00:00:00+01:00</published><updated>2015-12-22T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2015-12-20:/luigi.html</id><summary type="html">&lt;p&gt;I review Luigi, an execution framework for writing data pipes in Python code. It supports task-task dependencies, it has a simple central scheduler with an HTTP API and an extensive library of helpers for building data pipes for Hadoop, AWS, Mysql etc. It was written by Spotify for internal use and open sourced in 2012. A number of companies use it, such as Foursquare, Stripe, Asana.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Luigi is an execution framework for writing data pipes in Python code. It supports task-task dependencies, it has a simple central scheduler with an HTTP API and an extensive library of helpers for building data pipes for Hadoop, AWS, Mysql etc. It was written by Spotify for internal use and open sourced in 2012. A number of companies use it, such as Foursquare, Stripe, Asana.&lt;/p&gt;
&lt;h2&gt;Execution&lt;/h2&gt;
&lt;p&gt;Suppose that part of your ETL process is to take some data A, apply transformation X on it, and save it as Y. In Luigi, you would write a &lt;code&gt;.py&lt;/code&gt; file which contains a class X, which derives from class &lt;code&gt;Task&lt;/code&gt;. X would have three methods: &lt;code&gt;requires(), run(), and output()&lt;/code&gt;.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;requires&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Then you execute luigi and pass this &lt;code&gt;.py&lt;/code&gt; file to it, like &lt;code&gt;luigi --module x X&lt;/code&gt; if the file name is &lt;code&gt;x.py&lt;/code&gt;. When given a &lt;code&gt;Task&lt;/code&gt;, luigi:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Calls the &lt;code&gt;output()&lt;/code&gt; method, which returns one or more objects deriving from class &lt;code&gt;Target&lt;/code&gt;. A &lt;code&gt;Target&lt;/code&gt; is something which has an &lt;code&gt;exists()&lt;/code&gt; method which returns either &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt;. Luigi calls &lt;code&gt;exists()&lt;/code&gt; on all the targets to see whether they exist. If all return &lt;code&gt;True&lt;/code&gt;, luigi will flag this task as &lt;code&gt;DONE&lt;/code&gt; and never call &lt;code&gt;run()&lt;/code&gt;.
   If at least one of the output targets returned &lt;code&gt;False&lt;/code&gt;, this job needs to be run.&lt;/li&gt;
&lt;li&gt;Luigi then calls the &lt;code&gt;requires()&lt;/code&gt; method to see what other tasks need to first run for this task to run successfully. &lt;code&gt;requires()&lt;/code&gt; returns one or more objects deriving from class &lt;code&gt;Task&lt;/code&gt;, and recursively performs this process for all those.
   Note: after returning, luigi checks whether the output targets of the required tasks really exists. This is encapsulated in the &lt;code&gt;complete()&lt;/code&gt; method, the default implementation just calls &lt;code&gt;exists()&lt;/code&gt; on all targets returned by &lt;code&gt;output()&lt;/code&gt;; the method can optionally be overridden in the derived &lt;code&gt;Target&lt;/code&gt; class. The purpose of &lt;code&gt;complete()&lt;/code&gt; is to make sure &lt;code&gt;run()&lt;/code&gt; was successful, because if a required target’s &lt;code&gt;run()&lt;/code&gt; didn’t raise a Python exception but didn’t actually produce the output needed, then &lt;code&gt;run()&lt;/code&gt; shouldn’t be called. In this case the required task is re-run.&lt;/li&gt;
&lt;li&gt;Luigi calls the &lt;code&gt;run()&lt;/code&gt; method and sets the task status to &lt;code&gt;DONE&lt;/code&gt; if no Python exceptions were raised.
   Note: &lt;code&gt;run()&lt;/code&gt; can also dynamically &lt;code&gt;yield&lt;/code&gt; dependencies tasks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Local and central scheduler&lt;/h2&gt;
&lt;p&gt;When luigi is launched and a task is given to it a &lt;code&gt;Worker&lt;/code&gt; object is created. Workers need to talk to a &lt;code&gt;Scheduler&lt;/code&gt;, which manages the dependency graph of tasks and tells workers what to do. So when the local worker object is created, it can either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a local scheduler in the process, or&lt;/li&gt;
&lt;li&gt;Connect to a remote scheduler using the HTTP API. This is the default.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Local scheduler:&lt;/em&gt; The local scheduler can be used by passing &lt;code&gt;--local-scheduler&lt;/code&gt; to the luigi runtime. When running with the local scheduler, the algorithm given above is run recursively, and then luigi exits. This is usually only used for testing and debugging purposes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Central scheduler:&lt;/em&gt; More interesting is the central scheduler. The central scheduler is a separate &lt;code&gt;luigid&lt;/code&gt; Python Tornado app that workers can talk to over HTTP. It performs two tasks: scheduling of tasks based on the dependency graph and serving a simple web dashboard on port 8082 (default). Note that the central scheduler:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Doesn’t see or execute the &lt;code&gt;Task&lt;/code&gt; object's code, hence it never sees or checks whether targets exist; this is always performed by workers.&lt;/li&gt;
&lt;li&gt;The task is identified by its signature:&lt;ul&gt;
&lt;li&gt;Python name of the class; in the example above it’s X.&lt;/li&gt;
&lt;li&gt;The values of the parameters passed to the task, eg. &lt;code&gt;day=2015-12-01&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Parameters are member variables in the &lt;code&gt;Task&lt;/code&gt; objects which derive from class &lt;code&gt;Parameter&lt;/code&gt;, eg.:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;day&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DateParameter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;By specifying &lt;code&gt;significant=False&lt;/code&gt; in the &lt;code&gt;Parameter&lt;/code&gt; constructor, we can tell Luigi not to treat it as part of the task signature.&lt;/p&gt;
&lt;p&gt;The worker builds the local dependency graph and then uploads it to the central scheduler. Then it asks the central scheduler what it should do. The central scheduler potentially receives dependency graphs from several workers, and merges them, assuming tasks with the same name (and parameter values) uploaded from different workers are the same (generate the same &lt;code&gt;output()&lt;/code&gt; targets, contain the same &lt;code&gt;run()&lt;/code&gt; logic, etc).&lt;/p&gt;
&lt;p&gt;Given the dependency graph, the central scheduler then tells workers to start running tasks. A worker can only run tasks that it uploaded to the central scheduler, because those are the tasks that that Python process loaded. So workers are not generic workers, they can only work on the tasks that they were started with!&lt;/p&gt;
&lt;p&gt;Given a dependency graph, the scheduler will tell workers to run tasks that have no dependencies. By default, the order is non-deterministic. However, tasks can specify a priority, tasks with higher priority run first. The default priority is 0. Example:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;priority&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;something&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Because priorities are in code, the worker must evaluate them and pass it on to the central scheduler.&lt;/p&gt;
&lt;h2&gt;Local parallelism&lt;/h2&gt;
&lt;p&gt;More than 1 worker thread can be created by passing &lt;code&gt;--workers N&lt;/code&gt; to luigi. This is registered to the central scheduler, and if possible N tasks are run in parallel by one worker.
So there are multiple levels of parallelism in Luigi:
1. Multiple workers
2. Multiple threads in workers
3. Each task can have further parallelism, eg. a Hadoop MapReduce job.&lt;/p&gt;
&lt;h2&gt;Managing a library of tasks&lt;/h2&gt;
&lt;p&gt;What if we’re managing a library of 100s or 1000s of ETL jobs? While I haven’t used Luigi for this, it seems that the basic building blocks are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python &lt;code&gt;import&lt;/code&gt; statements: our jobs are distributed into different &lt;code&gt;.py&lt;/code&gt; files, so we need to &lt;code&gt;import&lt;/code&gt; them to use them in &lt;code&gt;requires()&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WrapperTask&lt;/code&gt; objects: these are special sink tasks which don’t have an output, they just require other tasks to be run.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This part puts a lot of work on the user of Luigi:
1. If we create a new task and forget to add it to the sink task, it won’t be executed (unless it’s a dependency for something else).
2. If we refactor a job (eg. rename the task class, change parameters), we have to search and replace all references in subsequent &lt;code&gt;requires()&lt;/code&gt; methods. Since Python isn’t a statically typed language, this has to be done by hand.
3. If running workers on separate machines, it’s our job to synchronize the library of &lt;code&gt;.py&lt;/code&gt; files (eg. using &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;cron&lt;/code&gt; jobs to sync very often). Different versions of tasks with different logic or local, uncommitted changes propagating to the central scheduler will lead to hard to find bugs and data corruption.&lt;/p&gt;
&lt;h2&gt;Date parameters&lt;/h2&gt;
&lt;p&gt;In an ETL system, most tasks will have a date(time) parameter which tells the code which day/hour to run the scripts for. For example, a Daily Active User (DAU) script computes the number of unique DAUs for a given day. Because this is such a common use-case, Luigi has a number of helper classes for dealing with date parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateParameter"&gt;DateParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.MonthParameter"&gt;MonthParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.YearParameter"&gt;YearParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateHourParameter"&gt;DateHourParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateMinuteParameter"&gt;DateMinuteParameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/api/luigi.parameter.html#luigi.parameter.DateIntervalParameter"&gt;DateIntervalParameter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Often tasks have to be re-run for a number of days. One way to do this is to call luigi repeatedly from the command line. Or we can use the built in &lt;code&gt;RangeDailyBase&lt;/code&gt; (also &lt;code&gt;RangeHourlyBase&lt;/code&gt;) helpers:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# instead of calling this repeatedly:&lt;/span&gt;
    &lt;span class="c1"&gt;# luigi task Task --date 2015-01-XX&lt;/span&gt;
&lt;span class="c1"&gt;# do this:&lt;/span&gt;
$ luigi --module task RangeDailyBase --of Task --start &lt;span class="m"&gt;2015&lt;/span&gt;-01-01 --stop &lt;span class="m"&gt;2015&lt;/span&gt;-01-31
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;The name of the date parameter of the task can be specified with &lt;code&gt;--param_name==&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When we pass in a large number of dates (as an interval), the &lt;code&gt;RangeXBase&lt;/code&gt; classes will instantiate a task object for each date and call &lt;code&gt;complete()&lt;/code&gt; to check whether that task needs to be run. This can be very slow, eg. if each one creates a database connection and then closes it down.
There are two optimization classes &lt;code&gt;RangeDaily&lt;/code&gt; and &lt;code&gt;RangeHourly&lt;/code&gt; that solve this problem. These are used just like the two &lt;code&gt;Base&lt;/code&gt; versions from the command line. But instead of instantiating many tasks which potentially don’t have to be run, they assume and call the task’s &lt;code&gt;bulk_complete()&lt;/code&gt; classmethod to get a list of dates which have to be run. So the user has to implement a &lt;code&gt;bulk_complete()&lt;/code&gt; to use &lt;code&gt;RangeDaily&lt;/code&gt; and &lt;code&gt;RangeHourly&lt;/code&gt;.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ luigi --module task RangeDaily --of Task --start &lt;span class="m"&gt;2015&lt;/span&gt;-01-01 --stop &lt;span class="m"&gt;2015&lt;/span&gt;-01-31
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Note: it seems Luigi doesn’t support bulk &lt;em&gt;running&lt;/em&gt; of parameter intervals.&lt;/p&gt;
&lt;h2&gt;Scheduling&lt;/h2&gt;
&lt;p&gt;Most ETL systems have jobs which need to run every hour or every day. Luigi doesn’t have a concept of calendar scheduling, this is up to the user. The recommended method by the authors is to create sink tasks and run them from &lt;code&gt;cron&lt;/code&gt; when the external input files (eg. raw log files) are likely to be available.&lt;/p&gt;
&lt;p&gt;Rescheduling failed tasks is influenced by the following parameters in the central scheduler’s &lt;code&gt;luigi.cfg&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;retry-delay&lt;/code&gt;: when to re-schedule, default 900 seconds&lt;/li&gt;
&lt;li&gt;&lt;code&gt;remove-delay&lt;/code&gt;: how long the central scheduler keeps tasks around that have no stakeholder; a stakeholder is a worker who uploaded that task&lt;/li&gt;
&lt;li&gt;&lt;code&gt;disable-hard-timeout&lt;/code&gt;: if a task fails again after this much time, it is disabled for good&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the worker’s &lt;code&gt;luigi.cfg&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;worker-keep-alive&lt;/code&gt;: you probably need to set this to true, so workers will stay alive when they run out of jobs to run, as long as they have some pending job waiting to be run. Otherwise workers will disconnect from the central scheduler and exit if there’s nothing to do, even if there are tasks which will be scheduled a few minutes from now.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;retry-external-tasks&lt;/code&gt;: If true, incomplete external tasks (i.e. tasks where the &lt;code&gt;run()&lt;/code&gt; method is &lt;code&gt;NotImplemented&lt;/code&gt;) will be retested for completion while Luigi is running. This means that if external dependencies are satisfied after a workflow has started, any tasks dependent on that resource will be eligible for running.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The central scheduler has a feature called task history. This logs task completion to a database, and exposes it on the dashboard.&lt;/p&gt;
&lt;p&gt;For tasks where the output is a database table, Luigi needs to keep track of successful inserts. It uses a special marker table for this (set with &lt;code&gt;marker-table&lt;/code&gt; in &lt;code&gt;luigi.cfg&lt;/code&gt;, default name is &lt;code&gt;table_updates&lt;/code&gt;). When a task finishes whose target is a database table, an entry is created in the marker table with the task’s &lt;code&gt;task_id&lt;/code&gt; (its name and parameter values). When the target’s &lt;code&gt;exists()&lt;/code&gt; method is called, this marker table is queried to check whether the task has been run (the &lt;code&gt;task_id&lt;/code&gt; is passed by the task to the &lt;code&gt;Target&lt;/code&gt; in its constructor).&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;p&gt;Resources can be used to introduce limits on task parallelism. For example, suppose we never want to run more than 10 mysql tasks, or we never want to run more than 3 instances of the hourly job &lt;code&gt;count_users&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Resources are declared in the &lt;code&gt;luigi.cfg&lt;/code&gt; file of the scheduler:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;count_users&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Resource use is given in the resources property of the task object in the Python code, like:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;mysql&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;# using 2 mysql connections in this task&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;h2&gt;Contrib stuff&lt;/h2&gt;
&lt;p&gt;Luigi has an impressive library of stock &lt;code&gt;Target&lt;/code&gt; and &lt;code&gt;Task&lt;/code&gt; classes, each with lots of functionality baked in as helper methods. This is the big reason why I think Luigi is popular and why I would consider using it.&lt;/p&gt;
&lt;p&gt;Luigi has &lt;code&gt;Task&lt;/code&gt; and &lt;code&gt;Target&lt;/code&gt; classes which support:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google Bigquery&lt;/li&gt;
&lt;li&gt;Hadoop jobs&lt;/li&gt;
&lt;li&gt;Hive queries&lt;/li&gt;
&lt;li&gt;Pig queries&lt;/li&gt;
&lt;li&gt;Scalding jobs&lt;/li&gt;
&lt;li&gt;Spark jobs&lt;/li&gt;
&lt;li&gt;Postgresql, Redshift, Mysql tables&lt;/li&gt;
&lt;li&gt;and more… &lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Source code and tests&lt;/h2&gt;
&lt;p&gt;I spent a fair amount of time digging through the Luigi Python source code. It’s pretty clean Python code with a lot of tests. Code size is about 18KLOC plus 16KLOC tests. It’s pretty easy to understand and extend.&lt;/p&gt;
&lt;h2&gt;Sample cases&lt;/h2&gt;
&lt;p&gt;Trying it out on a free &lt;a href="http://c9.io"&gt;cloud9&lt;/a&gt; Docker instance:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install tornado &lt;span class="c1"&gt;# luigi uses the tornado web server&lt;/span&gt;
$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$PATH&lt;/span&gt;:/home/ubuntu/workspace/luigi/bin
$ &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;PYTHONPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/home/ubuntu/workspace/luigi:.
$ luigid
&lt;span class="m"&gt;2015&lt;/span&gt;-12-19 &lt;span class="m"&gt;14&lt;/span&gt;:18:08,492 luigi-interface&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;11022&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; INFO: Loaded &lt;span class="o"&gt;[]&lt;/span&gt;
&lt;span class="m"&gt;2015&lt;/span&gt;-12-19 &lt;span class="m"&gt;14&lt;/span&gt;:18:08,494 luigi.server&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;11022&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; INFO: No prior state file exists at /var/lib/luigi-server/state.pickle. Starting with clean slate
&lt;span class="m"&gt;2015&lt;/span&gt;-12-19 &lt;span class="m"&gt;14&lt;/span&gt;:18:08,497 luigi.server&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;11022&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; INFO: Scheduler starting up
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;In another terminal, this is the default Luigi sample to try:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; luigi/examples
$ luigi --module top_artists AggregateArtists --date-interval &lt;span class="m"&gt;2012&lt;/span&gt;-06
&lt;span class="c1"&gt;# does the job, creates files locally!&lt;/span&gt;
$ luigi --module top_artists AggregateArtists --date-interval &lt;span class="m"&gt;2012&lt;/span&gt;-06
&lt;span class="c1"&gt;# notices files are there, doesn’t do anything&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Let’s play around with Luigi. Let’s create this x.py:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;luigi&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;X&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;WrapperTask&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;task_namespace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;examples&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Running X&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;requires&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;Bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;task_namespace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;examples&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;IntParameter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Bar &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Bar touched &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;luigi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LocalTarget&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/tmp/bar/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;And run it like:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ luigi --module x examples.X
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;This will create an &lt;code&gt;X&lt;/code&gt; task and 10 &lt;code&gt;Bar&lt;/code&gt; tasks. The 10 &lt;code&gt;Bar&lt;/code&gt; tasks will touch &lt;code&gt;/tmp/bar/…&lt;/code&gt; and that’s it.
Let’s delete the tmp files, and create a similarly named y.py, with identical &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Bar&lt;/code&gt; tasks, except &lt;code&gt;X&lt;/code&gt; renamed to &lt;code&gt;Y&lt;/code&gt;. Let’s launch two workers, one with x and one with y. Notice that the central scheduler will merge the dependency graphs and treat the &lt;code&gt;Bar&lt;/code&gt; tasks coming from the different workers/codes as the same, because their &lt;code&gt;task_id&lt;/code&gt; (class name plus parameters) are identical. It’s a bit weird, but this is how Luigi works. Another thing you’ll notice is that at the end of the execution, one of &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; will be unfinished (not green on the dashboard). This is because the workers are run without &lt;code&gt;--worker-keep-alive&lt;/code&gt;. So the first worker who finishes its tasks and is waiting for the other worker to finish the last &lt;code&gt;Bar&lt;/code&gt; will exit (it’s got nothing to do). If that worker was eg. the x worker, then task &lt;code&gt;X&lt;/code&gt; is not going to be run by anyone! if we turn on &lt;code&gt;--worker-keep-alive&lt;/code&gt; in the command-line, this oddity goes away.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;When designing an ETL framework, I would make (and have made) different design decisions compared to Luigi. But if I were tasked with creating a new ETL framework from scratch (eg. at a new company), I would definitely consider using Luigi. There is simply too much useful stuff there to ignore (and re-implement).
&lt;strong&gt;However, I would expect to:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find unexpected and painful behaviour in Luigi.&lt;/li&gt;
&lt;li&gt;Write significant scaffolding code to make it useful:&lt;ol&gt;
&lt;li&gt;Syncing the task library to different workers&lt;/li&gt;
&lt;li&gt;Scheduling series of tasks&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;li&gt;Alerting&lt;/li&gt;
&lt;li&gt;Dashboard for the ETL datasets and jobs (see below)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Downsides of Luigi:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sometimes unexpected behaviour: for example, a wrapper task can reach &lt;code&gt;DONE&lt;/code&gt; status without ever running the &lt;code&gt;run()&lt;/code&gt; method depending on non-deterministic execution order.&lt;/li&gt;
&lt;li&gt;The biggest downside to Luigi is that ETL jobs are specified as programmatic Python Task objects and not given is some sort of DSL. This means no external tool can reasonably/easily parse a library of tasks and extract dependency information, which would be useful for eg. generating documentation of the ETL system. Also, analysts have to learn Python.&lt;/li&gt;
&lt;li&gt;The web dashboard of the central scheduler is basically useless.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Links, talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/luigi"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luigi.readthedocs.org/en/stable/index.html"&gt;Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/spotify/luigi#who-uses-luigi"&gt;Slides from Luigi users&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="data"></category><category term="etl"></category><category term="workflow"></category><category term="luigi"></category></entry><entry><title>Cargo Cult Data</title><link href="/cargo-cult-data.html" rel="alternate"></link><published>2015-01-26T00:00:00+01:00</published><updated>2015-12-22T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2015-01-26:/cargo-cult-data.html</id><summary type="html">&lt;p&gt;Cargo cult data is when you're collecting and looking at data when making decisions, but you're only following the forms and outside appearances of scientific investigation and missing the essentials, so it doesn't work.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Cargo cult science&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Richard_Feynman"&gt;R. P. Feynman&lt;/a&gt; was a Nobel-prize winning physicist who coined the term &lt;a href="https://en.wikipedia.org/wiki/Cargo_cult_science"&gt;cargo cult science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In Feynman's words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the South Seas there is a cargo cult of people. During the [second world] war they saw airplanes land with lots of good materials, and they want the same thing to happen now [after the Americans left]. So they've arranged to imitate things like runways, to put fires along the sides of the runways, to make a wooden hut for a man to sit in, with two wooden pieces on his head like headphones and bars of bamboo sticking out like antennas—he's the controller—and they wait for the airplanes to land. They're doing everything right. The form is perfect. It looks exactly the way it looked before. But it doesn't work. No airplanes land. So I call these things cargo cult science, because they follow all the apparent precepts and forms of scientific investigation, but they're missing something essential, because the planes don't land.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Feynman cautioned that to avoid becoming cargo cult scientists, researchers must avoid fooling themselves, be willing to question and doubt their own theories and their own results, and investigate possible flaws in a theory or an experiment. He recommended that researchers adopt an unusually high level of honesty which is rarely encountered in everyday life, and gave examples from advertising, politics, and behavioral psychology to illustrate the everyday dishonesty which should be unacceptable in science.&lt;/p&gt;
&lt;h2&gt;Cargo cult data&lt;/h2&gt;
&lt;p&gt;The same idea applies to data. Cargo cult data is when you're collecting and looking at data when making decisions, but you're only following the forms of scientific investigation and missing the essentials, so it doesn't work. &lt;em&gt;So in the end you're like the natives of the South Seas, and the planes don't land for you either.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Signs that you're doing cargo cult data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you don't have standardized logging across your products&lt;/li&gt;
&lt;li&gt;you routinely break your logging and have holes in your dataset&lt;/li&gt;
&lt;li&gt;you don't have standardized KPIs across your products and company&lt;/li&gt;
&lt;li&gt;you're not A/B testing all your releases&lt;/li&gt;
&lt;li&gt;you don't have explicit hypothesis for your experiments&lt;/li&gt;
&lt;li&gt;you don't know what statistical power is&lt;/li&gt;
&lt;li&gt;you confuse statistical significance and magnitude of change&lt;/li&gt;
&lt;li&gt;you're using online forms to evaluate A/B tests&lt;/li&gt;
&lt;li&gt;you stop A/B tests as soon as they're statistically significant (=peeking)&lt;/li&gt;
&lt;li&gt;you're not tracking your experiments and their outcomes historically&lt;/li&gt;
&lt;li&gt;you don't know what standard deviation (=confuse signal and noise)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are no easy answers how to avoid cargo cult data, just as there are no easy answers how to avoid cargo cult science. If you are thinking about this as a company, your best bet is to hire smart mathematicians or physicist for your data team and listen to what they say. Personally, it's a matter of understanding statistics and being disciplined in your work. Fortunately there are &lt;a href="https://www.coursera.org/specializations/jhu-data-science"&gt;great courses on Coursera&lt;/a&gt;, &lt;a href="http://www.amazon.com/s/ref=dp_byline_sr_book_1?ie=UTF8&amp;amp;text=Allen+B.+Downey&amp;amp;search-alias=books&amp;amp;field-author=Allen+B.+Downey&amp;amp;sort=relevancerank"&gt;great books on Amazon&lt;/a&gt; and a &lt;a href="https://en.wikipedia.org/wiki/A/B_testing"&gt;wealth of information available online&lt;/a&gt;.&lt;/p&gt;</content><category term="data"></category></entry></feed>