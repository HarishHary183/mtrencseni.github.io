<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Bytepawn - Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2019-03-12T00:00:00+01:00</updated><entry><title>Hacker News Embeddings with PyTorch</title><link href="/hacker-news-embeddings-with-pytorch.html" rel="alternate"></link><published>2019-03-12T00:00:00+01:00</published><updated>2019-03-12T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2019-03-12:/hacker-news-embeddings-with-pytorch.html</id><summary type="html">&lt;p&gt;A PyTorch model is trained on public Hacker News data, embedding posts and comments into a high-dimensional vector space, using the mean squared error (MSE) of dot products as the loss function. The resulting model is reasonably good at finding similar posts.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/vectors.png" alt="Vector space" style="width: 300px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is based on &lt;a href="https://douwe.com/about"&gt;Douwe Osinga’s&lt;/a&gt; excellent &lt;a href="https://www.amazon.com/Deep-Learning-Cookbook-Practical-Recipes/dp/149199584X"&gt;Deep Learning Cookbook&lt;/a&gt;, specifically &lt;a href="https://github.com/DOsinga/deep_learning_cookbook/blob/master/04.2%20Build%20a%20recommender%20system%20based%20on%20outgoing%20Wikipedia%20links.ipynb"&gt;Chapter 4&lt;/a&gt;, embeddings. Embedding is a simple thing: given an entity like a Hacker News post or a Hacker News user, we associate an n-dimensional vector with it. We then do a simple thing: if two entities are similar in some way, we assert that the dot product (cosine similarity) should be +1, ie. the vectors should be “aligned”. If two entities are not similar, we assert that the dot product should be -1, ie. they should point in different directions. We then feed the data to a model, and in the training process get the optimizer to find assignments of entities to vectors such that those assertions are satisfied as much as possible. The most famous example of embeddings is Google's &lt;a href="https://en.wikipedia.org/wiki/Word2vec"&gt;word2vec&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/dlc.jpg" alt="Deep Learning Cookbook" style="width: 400px;"/&gt;&lt;/p&gt;
&lt;p&gt;In the book, embedding is performed on movies. For each movie, the wikipedia page is retrieved, and outgoing links to other wiki pages are collected. Two movies are similar if they both link to the same wiki page, else they are not similar. &lt;a href="https://keras.io/"&gt;Keras&lt;/a&gt; is used to train the model and the results are reasonably good.&lt;/p&gt;
&lt;p&gt;I wanted to implement the same thing in &lt;a href="https://pytorch.org"&gt;PyTorch&lt;/a&gt;, but on a different data set, to keep it interesting. As a regular &lt;a href="https://news.ycombinator.com"&gt;Hacker News&lt;/a&gt; reader, I chose Hacker News. Likes of user are not public, but comments are, so I use that for similarity.&lt;/p&gt;
&lt;p&gt;The plan is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Retrieve the top 1,000 HN posts from 2018 by number of comments&lt;/li&gt;
&lt;li&gt;For each post, retrieve the unique set of users who commented&lt;/li&gt;
&lt;li&gt;Use these (post, user) pairs for similarity embedding&lt;/li&gt;
&lt;li&gt;Train with &lt;a href="https://en.wikipedia.org/wiki/Mean_squared_error"&gt;mean squared error&lt;/a&gt; (MSE)&lt;/li&gt;
&lt;li&gt;Use the resulting model to get post similarity&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code for this recipe is up on Github (link to Github).&lt;/p&gt;
&lt;h2&gt;Getting the top 1000 HN posts&lt;/h2&gt;
&lt;p&gt;The simplest way to get this is from &lt;a href="https://cloud.google.com/bigquery"&gt;Google BigQuery&lt;/a&gt;, which has a &lt;a href="https://console.cloud.google.com/marketplace/details/y-combinator/hacker-news"&gt;public Hacker News dataset&lt;/a&gt;. We can write a SQL query and download the results as a CSV file from the Google Cloud console:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SELECT
    id,
    descendants,
    title
FROM
    `bigquery-public-data.hacker_news.full`
WHERE
        timestamp &amp;gt;= &amp;quot;2018-01-01&amp;quot;
    AND timestamp &amp;lt;  “2019-01-01”
    AND type = &amp;quot;story&amp;quot;
    AND score &amp;gt; 1
ORDER BY
    2 DESC
LIMIT
    1000
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The result of this is &lt;code&gt;top_1000_posts.csv&lt;/code&gt; (link to Github).&lt;/p&gt;
&lt;h2&gt;Retrieve commenters for top posts&lt;/h2&gt;
&lt;p&gt;Getting the comments is not practical from BigQuery because the table stores the tree hierarchy (&lt;code&gt;parent_id&lt;/code&gt; of the parent comment, but not the &lt;code&gt;post_id&lt;/code&gt;), so we’d have to query repeatedly to get all the comments of the post, which is inconvenient. Fortunately there’s an easier way. &lt;a href="https://algolia.com"&gt;Algolia&lt;/a&gt; has a Hacker News API where we can download one big JSON per post, containing all the comments. The API endpoint for this is:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;https://hn.algolia.com/api/v1/items/&amp;lt;post_id&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So we just go through all the posts from the previous step and download each one from Algolia.
Getting the set of commenters out of the JSON would be the easiest with &lt;code&gt;json.load()&lt;/code&gt;, but this sometimes fails on bad JSON. Instead we use an rxe regexp:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rxe.one('"author":"').one_or_more(rxe.set_except(['"'])).one('"')&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The code for this is here on Github (LINK). The script caches files, so repeatedly running it doesn’t repeatedly re-download data from Algolia.&lt;/p&gt;
&lt;p&gt;The script outputs the &lt;code&gt;(post, user)&lt;/code&gt; pairs into &lt;code&gt;post_comments_1000.csv&lt;/code&gt; (Github link).&lt;/p&gt;
&lt;h2&gt;Building the model&lt;/h2&gt;
&lt;p&gt;PyTorch has a built-in module for Embeddings, which makes building the model simple. It’s essentially a big array, which stores for each entity the assigned high-dimensional vector. In our case, both posts and users are embedded so if there are num_posts posts and num_users users, so num_vectors = num_posts + num_users. So the array has num_vectors row, each row corresponds to that entity’s embedding vector.&lt;/p&gt;
&lt;p&gt;PyTorch will then optimize the entries in this array, so that the dot products of the combinations of the vectors are +1 and -1 as specified during training, or as close as possible.&lt;/p&gt;
&lt;p&gt;We create a Model which contains the embedding. We implement the forward() function, which just returns the dot product for a minibatch of posts and users, as per the current embedding vectors:&lt;/p&gt;
&lt;p&gt;class Model(torch.nn.Module):
    def &lt;strong&gt;init&lt;/strong&gt;(self, num_vectors, embedding_dim):
        super(Model, self).&lt;strong&gt;init&lt;/strong&gt;()
        self.embedding = torch.nn.Embedding(num_vectors, embedding_dim, max_norm=1.0)
    def forward(self, input):
        t1 = self.embedding(torch.LongTensor([v[0] for v in input]))
        t2 = self.embedding(torch.LongTensor([v[1] for v in input]))
        dot_products = torch.bmm(
            t1.contiguous().view(len(input), 1, self.embedding.embedding_dim),
            t2.contiguous().view(len(input), self.embedding.embedding_dim, 1)
        )
        return dot_products.contiguous().view(len(input))&lt;/p&gt;
&lt;p&gt;Next, we need to write a function to build the minibatches we will use for training. For training, we will pass in existing combinations and “assert” that the dot product should be +1, and some missing combinations with -1:&lt;/p&gt;
&lt;p&gt;def build_minibatch(num_positives, num_negatives):
    minibatch = []
    for _ in range(num_positives):
        which = int(len(idx_list) * random())
        minibatch.append(idx_list[which] + [1])
    for _ in range(num_negatives):
        while True:
            post = int(len(posts) * random())
            user = min_user_idx + int(len(users) * random())
            if post not in idx_user_posts[user]:
                break
        minibatch.append([post, user] + [-1])
    shuffle(minibatch)
    return minibatch&lt;/p&gt;
&lt;p&gt;Now we can perform the training. We will embed into 50 dimensions, we will use 500 positive and 500 negative combinations per minibatch. We use the Adam optimizer and minimize the MSE between our asserted dot products and the actual dot products:&lt;/p&gt;
&lt;p&gt;embedding_dim = 50
model = Model(num_vectors, embedding_dim)
optimizer = torch.optim.Adam(model.parameters())
loss_function = torch.nn.MSELoss(reduction='mean')
num_epochs = 50
num_positives = 500
num_negatives = 500
num_steps_per_epoch = int(len(post_comments) / num_positives)
for i in range(num_epochs):
    for j in range(num_steps_per_epoch):
        optimizer.zero_grad()
        minibatch = build_minibatch(num_positives, num_negatives)
        y = model.forward(minibatch)
        target = torch.FloatTensor([v[2] for v in minibatch])
        loss = loss_function(y, target)
        if i == 0 and j == 0:
            print('r: loss = %.3f' % float(loss))
        loss.backward(retain_graph=True)
        optimizer.step()
    print('%s: loss = %.3f' % (i, float(loss)))&lt;/p&gt;
&lt;h1&gt;print out some samples to see how good the fit is&lt;/h1&gt;
&lt;p&gt;minibatch = build_minibatch(5, 5)
y = model.forward(minibatch)
target = torch.FloatTensor([v[2] for v in minibatch])
print('Sample vectors:');
for i in range(5+5):
    print('%.3f vs %.3f' % (float(y[i]), float(target[i])))&lt;/p&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;p&gt;r: loss = 1.016
0: loss = 1.009
...
49: loss = 0.633
Sample vectors:
0.319 vs -1.000
0.395 vs 1.000
0.226 vs -1.000
-0.232 vs -1.000
0.537 vs 1.000
0.179 vs -1.000
-0.020 vs 1.000
0.392 vs 1.000
0.141 vs 1.000
-0.096 vs -1.000&lt;/p&gt;
&lt;p&gt;We can see that training is able to reduce the MSE by about 40% from the initial random vectors by finding better alignments. That doesn’t sound too good, but it’s good enough for recommendations to work. Let’s write a function to find the closest vectors to a query vector:&lt;/p&gt;
&lt;p&gt;def similar_posts_by_title(title):
    post_id = title_to_id[title]
    pv = get_post_vector(post_id)
    dists = []
    for other_post in posts:
        if other_post == post_id: continue
        ov = get_post_vector(other_post)
        dist = torch.dot(pv, ov)
        dists.append([float(dist), 'https://news.ycombinator.com/item?id=' + other_post, id_to_title[other_post]])
    similars = sorted(dists)[-3:]
    similars.reverse()
    return similars&lt;/p&gt;
&lt;p&gt;We can use this to find similar posts, and it works:&lt;/p&gt;
&lt;p&gt;&lt;make table here&gt;
Discussion
Clearly we could use the text of the posts/comments to gauge similarity, and would get much better results.
Training doesn’t converge, if the positive/negative ratio is too different from 1/1. Obviously, if we include too many positive pairs where we “assert” +1 dot product, the optimizer would just pull all the vectors together to get +1 all the time and reduce MSE. If we include too many negative pairs, it will pull all posts to one vector and all users to the opposing vector, this configuration will satisfy the training criteria and result in a low MSE. (In the book, 1/10 ratio is used, I think it’s accidental that it works in that case.)
When emitting the (post, user) pairs, we cut the users, and only keep users who have between 3 and 50 comments. The lower 3 is just to cut out users who don’t connect posts, so won’t be valuable to the embedding similarity training; so this cut makes the training set leaner and meaner. The 50 is to throw out users who comment on a lot posts, and hence pollute the similarity signal during training. Interestingly, without the upper limit of 50, the model doesn’t converge to a useful configuration! This took a lot of playing around to figure out.
The sample above uses similarity between vectors to find posts similar to a query post, and also displays the “similarity” dot product. With the same logic, we can also get recommendations for query users, or similar users to query users. However, in this case the dot product is always significantly lower than in the movie-movie case. The users are more scattered in the high-dimensional space, the movies are more tightly packed in a subspace.
Issues/bugs that slowed me down:
As mentioned, without the upper 50 cut on user’s comments, it didn’t converge.
Both posts and users are embedded, so we must remember at which row in the embedding matrix the user vectors start (min_user_idx in the code). Initially I forgot to account for this, both started indexing at 0. Everything ran, but the similarity results were garbage. A nicer solution here would be to use 2 Embedding objects (essentially 2 arrays), so we don’t have to remember the offset.
I forgot to call optimizer.zero_grad() in the zero grad. Everything ran, but the similarity results were garbage. Without the zero_grad() call, the gradients are just cumulated, and the optimizer jumps around aimlessly.&lt;/p&gt;</content><category term="pytorch embedding"></category></entry><entry><title>PyTorch Basics: Solving the Ax=b matrix equation with gradient descent</title><link href="/pytorch-basics-solving-the-axb-matrix-equation-with-gradient-descent.html" rel="alternate"></link><published>2019-02-08T00:00:00+01:00</published><updated>2019-02-08T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2019-02-08:/pytorch-basics-solving-the-axb-matrix-equation-with-gradient-descent.html</id><summary type="html">&lt;p&gt;I will show how to solve the standard &lt;strong&gt;A x = b&lt;/strong&gt; matrix equation with PyTorch. This is a good toy problem to show some guts of the framework without involving neural networks.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/computational-graph.PNG" alt="PyTorch computational graph" style="width: 300px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;PyTorch is my favorite deep learning framework, because it's a hacker's deep learning framework. It’s easier to work with than Tensorflow, which was developed for Google’s internal use-cases and ways of working, which just doesn’t apply to use-cases that are several orders of magnitude smaller (less data, less features, less prediction volume, less people working on it). This is not a PyTorch vs Tensorflow comparison post, for that see &lt;a href="https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b"&gt;this post&lt;/a&gt;. There are tons of &lt;a href="https://pytorch.org/tutorials/"&gt;great tutorials&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=_H3aw6wkCv0"&gt;talks&lt;/a&gt; that help people quickly get a deep neural network model up and running with PyTorch. &lt;a href="https://pytorch.org/docs/stable/torchvision/datasets.html"&gt;PyTorch comes with standard datasets (like MNIST)&lt;/a&gt; and &lt;a href="https://pytorch.org/docs/stable/torchvision/models.html"&gt;famous models (like Alexnet) out of the box&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Under the hood, PyTorch is computing derivatives of functions, and backpropagating the gradients in a computational graph; this is called autograd. This can also be applied to solve problems that don’t explicitly involve a deep neural network.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/computational-graph.PNG" alt="PyTorch computational graph" style="width: 300px;"/&gt;&lt;/p&gt;
&lt;p&gt;To illustrate this, we will show how to solve the standard &lt;strong&gt;A x = b&lt;/strong&gt; matrix equation with PyTorch. This is a good toy problem to show some guts of the framework without involving neural networks.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/axb.PNG" alt="Ax=b" style="width: 400px;"/&gt;&lt;/p&gt;
&lt;p&gt;As a reminder, in the &lt;strong&gt;A x = b&lt;/strong&gt; matrix equation, &lt;strong&gt;A&lt;/strong&gt; is a fixed matrix, &lt;strong&gt;b&lt;/strong&gt; is a fixed vector, and we’re looking for vector &lt;strong&gt;x&lt;/strong&gt; such that &lt;strong&gt;A x&lt;/strong&gt; is just the vector &lt;strong&gt;b&lt;/strong&gt;. If &lt;strong&gt;A&lt;/strong&gt; is a 1x1 matrix, then this is just a scalar equation, and &lt;strong&gt;x = b / A&lt;/strong&gt;. Let’s write this as &lt;strong&gt;x = A&lt;sup&gt;-1&lt;/sup&gt; b&lt;/strong&gt;, and then this applies to the n x n matrix case as well: the exact solution is to compute the inverse of &lt;strong&gt;A&lt;/strong&gt;, and multiply it by &lt;strong&gt;b&lt;/strong&gt;. (Note: the  technical conditions for a solution is &lt;strong&gt;det A ≠ 0&lt;/strong&gt;, I'll ignore this since I'll be using random matrices). Let’s say the solution is &lt;strong&gt;x = x&lt;sub&gt;s&lt;/sub&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: using gradient descent to estimate the solution is not necessary, as the solution can be computed quickly with &lt;a href="https://en.wikipedia.org/wiki/Invertible_matrix#Methods_of_matrix_inversion"&gt;matrix inversion&lt;/a&gt;. We're doing this to understand PyTorch on a toy problem.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How can we use &lt;a href="https://pytorch.org/docs/stable/autograd.html"&gt;PyTorch and autograd&lt;/a&gt; to solve it? We can use it to approximate the solution: start with some random &lt;strong&gt;x&lt;sub&gt;0&lt;/sub&gt;&lt;/strong&gt;, compute the vector &lt;strong&gt;A x&lt;sub&gt;0&lt;/sub&gt; - b&lt;/strong&gt;, take the norm &lt;strong&gt;L =  ‖ A x&lt;sub&gt;0&lt;/sub&gt; - b   ‖&lt;/strong&gt;, and use gradient descent to find a next, better &lt;strong&gt;x&lt;sub&gt;1&lt;/sub&gt;&lt;/strong&gt; vector so that it’s closer to the real solution &lt;strong&gt;x&lt;sub&gt;s&lt;/sub&gt;&lt;/strong&gt;. They key idea is that for &lt;strong&gt;x=x&lt;sub&gt;s&lt;/sub&gt;&lt;/strong&gt;, the norm &lt;strong&gt;L =     ‖ A x&lt;sub&gt;s&lt;/sub&gt; - b   ‖ = 0&lt;/strong&gt;, it vanishes. So we want to minimize &lt;strong&gt;L&lt;/strong&gt;. This &lt;strong&gt;L&lt;/strong&gt; is called the loss function in such optimization problems.&lt;/p&gt;
&lt;p&gt;Let’s start with the standard L2 norm:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/l2-definition.PNG" alt="L2 norm definition" style="width: 150px;"/&gt;&lt;/p&gt;
&lt;p&gt;This will result in a parabolic loss function, where we will converge to the minimum.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/parabola2d.PNG" alt="L2 norm parabola" style="width: 300px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;This is what the PyTorch code for setting up &lt;strong&gt;A, x&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; looks like. We initialize &lt;strong&gt;A&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt; to random:&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/d0f65883f2be329cac1ec390869d02e0.js"&gt;&lt;/script&gt;

&lt;p&gt;We set &lt;code&gt;requires_grad&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; for &lt;strong&gt;A&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;. These are constants in this scenario, their gradient is zero. &lt;strong&gt;x&lt;/strong&gt; is the variable which we will compute gradients for, so we set &lt;code&gt;requires_grad = True&lt;/code&gt;.&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/a232e345680f17f0c00ce816036308f1.js"&gt;&lt;/script&gt;

&lt;p&gt;We then tell PyTorch to do a backward pass and compute the gradients:&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/4583fdf66ea02a4d4756103c734074c5.js"&gt;&lt;/script&gt;

&lt;p&gt;At this point, PyTorch will have computed the gradient for &lt;strong&gt;x&lt;/strong&gt;, stored in &lt;code&gt;x.grad.data&lt;/code&gt;. What this means is “adjust &lt;strong&gt;x&lt;/strong&gt; in this direction, by this much, to decrease the loss function, given what &lt;strong&gt;x&lt;/strong&gt; is right now”. Now we just need to introduce a step size to control our speed of descent, and actually adjust &lt;strong&gt;x&lt;/strong&gt;:&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/c1929a58684b2962c4008b24564b48a0.js"&gt;&lt;/script&gt;

&lt;p&gt;Almost done. We just need to set &lt;code&gt;step_size&lt;/code&gt;, put this in a for loop, and figure out when to stop it:&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/3e1b3d47f55fdb5a54518312d298940a.js"&gt;&lt;/script&gt;

&lt;p&gt;Let’s stop it when the loss is smaller than &lt;code&gt;stop_loss&lt;/code&gt;, and put an upper bound on the number of iterations. Putting it all together:&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/d6be2e6b35f100be1a92e6726f5210c2.js"&gt;&lt;/script&gt;

&lt;p&gt;It’s a good exercise to play around with this. Set a specific &lt;strong&gt;A&lt;/strong&gt; and &lt;strong&gt;b&lt;/strong&gt;, print things out, try other dimensions, use numpy to get the inverse and compare the solutions, etc.&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;How is this related to neural networks?&lt;/strong&gt;&lt;br/&gt;
In a fully connected (FC) layer, each input is multiplied by a weight to get the next's layer values. Putting the weights together, they form a matrix (tensor), which is multiplied by the input activations, just like &lt;strong&gt;A x&lt;/strong&gt;. In real neural networks, non-linearity is introduced at each node, eg. a &lt;a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"&gt;ReLu&lt;/a&gt; function; we don't have that, this is a linear problem.  Note that in real machine learning scenarios, the loss function (potential surface) is in a very high dimensional space and has several minima; the goal is not to find the global optimum (which is untractable), just a good enough local one (that gets the job done).&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/potential-surface.PNG" alt="Potential surface" style="width: 300px;"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do we know what &lt;code&gt;step_size&lt;/code&gt; should be? How do we know when to stop?&lt;/strong&gt;&lt;br/&gt;
In this case, because the loss function we’re optimizing is quadratic, we can get away with a fixed step size, as the gradient will get smaller as we approach the optimum (the solution). In more complicated deep neural network scenarios (where the step size is called the learning rate), there are strategies on how to gradually decay the step size. In general, if the step size is too small, we waste a lot of time far away from the solution. If it’s too big, we jump around the optimum and it may never converge. See also &lt;a href="https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1"&gt;Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/learning-rate.PNG" alt="Learning rate" style="width: 300px;"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What should the stopping condition be?&lt;/strong&gt;&lt;br/&gt;
In this toy example, the &lt;code&gt;step_size&lt;/code&gt; is set smaller than &lt;code&gt;stop_loss&lt;/code&gt;, so it can converge on the optimum below the accepted loss. If you would set the &lt;code&gt;stop_loss&lt;/code&gt; to be much smaller than the step size, you will see that it never stops, it will jump around the optimum (left side of above picture).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What exactly is the quantity &lt;code&gt;x.grad.data&lt;/code&gt;?&lt;/strong&gt;&lt;br/&gt;
It's the gradient of the loss function we called backward() on, with respect to the variable, in this case &lt;strong&gt;x&lt;/strong&gt;. So in the dim=1 case it’s just &lt;strong&gt;dL/dx&lt;/strong&gt;. Note that since &lt;strong&gt;x&lt;/strong&gt; is the only independent variable here, the partial derivative is the total derivative. If we had multiple independent variables, we'd have to add the partial derivates to get the &lt;a href="https://en.wikipedia.org/wiki/Total_derivative"&gt;total derivative&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if we use the L1 norm as the loss function?&lt;/strong&gt;&lt;br/&gt;
To use the L1 norm, set &lt;code&gt;p=1&lt;/code&gt; in the code. The L1 norm in &lt;code&gt;dim=1&lt;/code&gt; is the &lt;strong&gt;abs()&lt;/strong&gt; function, so it’s derivative is piecewise constant. In this case the slope is &lt;strong&gt;+-   ‖A  ‖&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/l1-norm.PNG" alt="L1 norm" style="width: 300px;"/&gt;&lt;/p&gt;
&lt;p&gt;This is “less nice” than the L2 norm for this simple case, because the gradient doesn’t vanish as the solution approaches the optimum. The solution is more likely to overshoot the optimum and oscillate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What happens if we run this on the GPU?&lt;/strong&gt;&lt;br/&gt;
To turn on the GPU, put this line at the top:&lt;/p&gt;
&lt;script src="https://gist.github.com/mtrencseni/ae8c2b405025c21a63c6562d8dc7dcff.js"&gt;&lt;/script&gt;

&lt;p&gt;On my computer, this is significantly slower on the GPU than the CPU, because copying such a small problem to the GPU on each iteration creates a time overhead which is not worth it. This toy example is too small to demonstrate how GPUs speed up deep learning.&lt;/p&gt;</content><category term="pytorch"></category></entry><entry><title>Automating a Call Center with Machine Learning</title><link href="/automating-a-call-center-with-machine-learning.html" rel="alternate"></link><published>2019-01-27T00:00:00+01:00</published><updated>2019-01-27T00:00:00+01:00</updated><author><name>Marton Trencseni</name></author><id>tag:None,2019-01-27:/automating-a-call-center-with-machine-learning.html</id><summary type="html">&lt;p&gt;Over a period of 6 months, we rolled out a Machine Learning model to predict a customer’s delivery (latitude, longitude). During the recent holiday peak, this ML model handled most of Fetchr’s order scheduling.&lt;br/&gt;&lt;br/&gt;&lt;img src="/images/ml-share3.png" alt="Share of ML scheduled versus Call center scheduled deliveries" style="width: 400px;"/&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Impact&lt;/h2&gt;
&lt;p&gt;Over a period of 6 months, we rolled out a Machine Learning model to predict a customer’s delivery (latitude, longitude). During the recent holiday peak, this ML model handled most of Fetchr’s order scheduling.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ml-share3.png" alt="Share of ML scheduled versus Call center scheduled deliveries" style="width: 800px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In Europe and the US, addresses is not something we think about a lot. My address in Hungary is, for example:&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;1114 Budapest, Szabolcska Mihaly u. 7&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here “u.” stands for “utca”, which means “street”. 1114 is my zip code in Hungary. Sometimes I’m lazy and I shorten it, like:&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;1114 BP, Szabolcska 7&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the US, it’s customary to write it out in a different order and “street” is dropped:&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;em&gt;7 Szabolcska Mihaly, Budapest, 1114&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If I open Google Maps (in an incognito window), I can enter either of the three, and it will point me to the precise (latitude, longitude) of my apartment, which happens to be (47.476117, 19.044950).&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/szabolcska7.png" alt="Szabolcska Mihaly u. 7" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;I can give either address string to a delivery company in Hungary, and they will find my apartment. Why does this work? In the US and Europe, the following all hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;zip codes exist and everybody uses them&lt;/li&gt;
&lt;li&gt;address formats are reasonably standardized&lt;/li&gt;
&lt;li&gt;most people know what their address is (“my zip code is 1114”)&lt;/li&gt;
&lt;li&gt;most people know how to write out their addresses&lt;/li&gt;
&lt;li&gt;companies like Google have a known database of addresses (and maps)&lt;/li&gt;
&lt;li&gt;companies like Google have an incentive to make services like Google Maps work&lt;/li&gt;
&lt;li&gt;web shops can enforce address formats, eg. can force the user to select from known zip codes, street names in those zip codes, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In many countries in the Middle East, some of the above do not hold. For example, in the United Arab Emirates (UAE), there are no zip codes. There are street names, but which street a building falls on is often ambiguous. Also, streets have many names, official and slang, english and arabic. Often, people don’t know their streets: for example, I live in a hotel in Dubai, and I don’t know which street the building is on (more than half of the population in Dubai are expats). Sometimes buildings have a street number, sometimes not; sometimes people know the number, sometimes not. Finally, there are large areas, even in Dubai, where Google Maps doesn't know street names or numbers:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/no-street-names.png" alt="No street names" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Many times people also don't give their street name as an address, instead they give an area name (which is itself ambiguous) and building name (“Princess Tower”) or a nearby point of interest (POI) like “near Burger King in Al Barsha, next to SZR” ("SZR" stands for "Sheikh Zayed Road", it's a 2x8 lane super-highway in Dubai). This is the situation in cities like Dubai or Riyadh; in remote areas, in the desert, resolving locations is even harder.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Interestingly, a few years ago the UAE government created a system to identify buildings called Makani codes, which is a 10 digit number. Every building in the UAE has a Makani code, and every building must have a plaque showing the Makani code. Unfortunately, very few people know their building's Makani code; it’s not widely used (eg. I don’t know my building's Makani code).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/makani.jpg" alt="Makani numbers" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;Here are some UAE and KSA addresses Fetchr delivered to in the past (changed capitalization to improve readability:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Near to Safeer Mall, Khuzam&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Greece Cluster, Building Greece 05 Shop 04, International City, Dubai&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Near by Emirates NBD, Nad Al Sheba, Dubai&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Batha Near Al Rajhi Building near Al Electron Building&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;I work at Royal Green Golf &amp;amp; Country Club&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;My home is near by colors street for car decoration in Jeddah&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Villa, King Khalid street, Down Town&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Google Maps or Open Street Maps doesn’t help here!&lt;/p&gt;
&lt;p&gt;The trick to a successful delivery in this region is the phone number! Unlike in the US or Europe, where an address is enough, here the &lt;strong&gt;phone number is king&lt;/strong&gt;; no package is accepted without the customer’s phone number. For incomplete or ambigious addresses, delivery companies rely on calling the customer to figure out where to go: call the customer, and try to figure out where to send the package based on the conversation with the customer, and then the package is dispatched to that (latitude, longitude). This is called &lt;strong&gt;scheduling&lt;/strong&gt;, the goal here is to figure out the spacetime coordinates of the delivery: (latitude, longitude, day, time), but we’ll ignore the (day, time) here.&lt;/p&gt;
&lt;h2&gt;Modeling&lt;/h2&gt;
&lt;p&gt;The problem we took on: given the freetext (phone, address), can we predict (latitude, longitude), so we can avoid a call to the customer? We set up a dummy service for this and got our software engineers to pass in the (phone, country, city, address); if we can make a good prediction, we return the predicted (latitude, longitude), else we return &lt;code&gt;NO_PREDICTION&lt;/code&gt;, in which case everything happens as before, the customer gets a call. (This is actually an oversimplification, for example the customer can also self-schedule using our app or mweb.)&lt;/p&gt;
&lt;p&gt;The service in production is running a number of models. A model is a way to predict the (latitude, longitude). When the service receives a (phone ... address) request, it goes through the models in a fixed order. If a model returns the (latitude, longitude), the service returns it. If the model returns &lt;code&gt;NO_PREDICTION&lt;/code&gt;, it moves on to the next. The models which returns the best quality coordinates is the first in line, and so on.&lt;/p&gt;
&lt;p&gt;So what models do we actually use? We currently have a total of 5 models running in production. I will describe 2 at a high level below.&lt;/p&gt;
&lt;h2&gt;Repeats&lt;/h2&gt;
&lt;p&gt;When working on building dashboards to understand our delivery operations, we created a metric which shows the % of our customers who are repeat customers. Customers can be identified by their phone numbers, which are also passed in as free text, but normalizing this is easy. It turns out we have a lot of customers that we’re already delivered to! This is an obvious opportunity: if we’ve delivered to a customer before, and recorded the actual (latitude, longitude) of the delivery (the driver app automatically does this when the package is delivered), then we can look this up. This should work most of the time, because people don’t move that often. This is the basic idea of this model (details omitted on purpose).&lt;/p&gt;
&lt;p&gt;The repeat model is simple, but it works amazingly well. The delivery performance (out of 100 dispatches, how many deliveries are successful) of this model outperforms our call center, and is on par with customer self-scheduling (which is the best channel). Part of the reason is that repeat customers are a biased group.&lt;/p&gt;
&lt;h2&gt;Address matching&lt;/h2&gt;
&lt;p&gt;What about non-repeat customers? Can we know where to go just based on the address?&lt;/p&gt;
&lt;p&gt;Initially we tried a lot of things, too many to detail here. Broadly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instead of predicting the (latitude, longitude), use a more coarse grained geographic division of zones (eg. divide Dubai into a few 100 polygons), and try to predict the correct zone; here we tried various approaches:&lt;ul&gt;
&lt;li&gt;building a separate model for each zone&lt;/li&gt;
&lt;li&gt;building one city-level model with multiple activations, one per zone&lt;/li&gt;
&lt;li&gt;decision tree and other models on feature vectors constructed from bag-of-words models, TF-IDF, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;use raw OpenStreetMaps (OSM) data, extracting “sites”, and matching to that&lt;/li&gt;
&lt;li&gt;mixing-and-matching the above two&lt;/li&gt;
&lt;li&gt;various string tokenization and matching approaches&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After a lot of experimentation, I wasn’t satisfied with the overall performance of the models, and I didn't have enough confidence to put them into production. However, after weeks of working with the data, I realized that I can try something pretty simple “by hand”. I usually look at Dubai data, and I noticed a lot of addresses include the area name, which is pretty unambigious, for example “Jumeirah Village Circle” or “Jumeirah Village Triangle”.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/jvt.png" alt="JVT and JVC" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;p&gt;I knew that these areas were served by the same driver, because I’ve been out with him several times to understand what happens on the ground. So if the service returns the middle of the area as a (latitude, longitude) for those addresses, it’ll get dispatched by the correct driver, a good enough first step. So I spent a day looking at Google Maps and OSM and simply wrote out a few hundred rules by hand, did some quick sanity checks to make sure past addresses which would match these were in the right location, and then wrote a simple model which essentially does substring checking. I then put it into production for a few orders / day. A few days later I evaluated the delivery performance, and saw that while it’s not excellent, it’s not that bad. (I later removed this manual model from production, the ML version is much better).&lt;/p&gt;
&lt;p&gt;So the question was, how do I make this better, and generalize it? I noticed this pattern while looking at the data, clearly there’s more patterns like this in the data, let’s get the machine to learn it. This is what we did: there’s a backend component, which looks at all our historic deliveries, and finds good rules, the production service then just uses these rules (details omitted on purpose). It's an interesting approach: a rule based engine in production, but the rules are coming out of an ML model; this makes it really easy to tune (see below) and add/remove exceptions.&lt;/p&gt;
&lt;h2&gt;Knobs to turn&lt;/h2&gt;
&lt;p&gt;A really nice property of our models is that they have knobs to turn. On the repeat model, we can accept better or worse address similarity when comparing to past addresses. On the address matching type models, we can accept more or less tightly packed historic coordinates when deciding which rule to run in production. This allows us to turn knobs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;run models in “tight” mode, where we schedule less orders (more prediction queries return &lt;code&gt;NO_PREDICTION&lt;/code&gt; and go to the call center), but the returned coordinates are very accurate and hence we get good delivery performance.&lt;/li&gt;
&lt;li&gt;run models in “wide” mode, where we schedule more orders (less orders return &lt;code&gt;NO_PREDICTION&lt;/code&gt; and go to the call center), but the returned coordinates are on average less accurate and hence we get lower delivery performance---but we pass less orders to the call center.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can use these knobs to make choices. For example, if other scheduling channels are not available, it makes sense to run the model as wide as possible; and there's a break-even point, where the model performs as well on average as the call center.&lt;/p&gt;
&lt;h2&gt;Winner take all&lt;/h2&gt;
&lt;p&gt;The delivery market has a “winner take all” dynamic: more order volume means higher density, means more laoded drivers, means more efficient drivers, means lower cost. This also applies to the ML models. The more deliveries a company has made, the more repeats it will have (eventually, it will cover the entire population of a country/city). The more deliveries a company has made, the better address rules it can extract from its data. More past deliveries lead to higher efficiency today.&lt;/p&gt;
&lt;h2&gt;Statistical improvements&lt;/h2&gt;
&lt;p&gt;There are a lot of ways to improve these models. The simplest one is based on counting. Using the address matching model as a use-case, we can simply count how many dispatches are coming from each rule (like the toy model example &lt;code&gt;“jumeirah village triangle” -&amp;gt; (latitude, longitude)&lt;/code&gt;), compute the delivery performance (=deliveries/dispatches) for each rule, and prune the badly performing ones. There’s an exploration-exploitation trade-off here, so we use an epsilon-greedy strategy. For more on this, see &lt;a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"&gt;multi-armed bandits&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Metrics&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Good Machine Learning goes hand in hand with good Data Engineering and Analytics.&lt;/strong&gt; This project came out of building 100s of charts and metrics to understand and visualize Fetchr’s operations and business. For this project, the most relevant were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Repeat %&lt;/strong&gt;: what % of our daily dispatches are going to customer we’ve delivered to before; the higher, the easier it is to do a good job on predicting customer location and behaviour based on past data. Since Fetchr is very successful and operates at scale, we have a fair share of repeats.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scheduling accuracy&lt;/strong&gt;: scheduling accuracy is the % of deliveries where the scheduled coordinate and the delivery coordinate is within X meters. The challenge is, the delivery coordinate is unreliable: sometimes the drivers update the order status hours after the delivery event (eg. while having coffee), so the delivery coordinate is unreliable. The scheduled coordinate itself could also be incorrect. But when the two are close together, it’s very likely that they point to the correct location. Scheduling accuracy can also be benchmarked when back-testing models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delivery performance&lt;/strong&gt;: Delivery performance is a daily metric, it’s the % of dispatches that are successfully delivered. Delivery performance is not something we can back-test when building models, it has to be measured in production, experimentally, eg. on a small 1% release. (Delivery performance is the One Metric That Matters for delivery companies, we live and die by it.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scheduling channel splits, model splits&lt;/strong&gt;: also a daily metric, it shows what % of dispatches came from which scheduling channel (call center, ML, self-scheduling, etc.), and specifically for the ML channel, what % came from which model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conversion:&lt;/strong&gt; of all orders passed to the ML model for coordinate prediction, what % do we return a coordinate (instead of &lt;code&gt;NO_PREDICTION&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="/images/ml-conversion3.png" alt="ML Conversion%" style="width: 650px;"/&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The delivery coordinate prediction service has been a great success at Fetchr. The version currently in production is relatively simple, easy to understand and tunable, and adding exceptions is easy. There are lots of improvement opportunities in the current models themselves, ordering of models based on features, and of course making more complex models. Our goal is to go further up and toward the right in (conversion, accuracy) space!&lt;/p&gt;</content><category term="fetchr"></category><category term="machine-learning"></category><category term="call-center"></category></entry></feed>