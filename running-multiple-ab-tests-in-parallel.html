<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="google-site-verification" content="14dvQcWyDWHBo8aM0kld8XzEa6KypAzCQDz1_KPus9E" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn - Marton Trencseni Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="It is possible to run multiple A/B tests at once and measure accurate lifts on the same metric by randomizing user assignments into A/B test experiments." />
<meta name="keywords" content="ab-testing">
<meta property="og:site_name" content="Bytepawn - Marton Trencseni"/>
<meta property="og:title" content="Running multiple A/B tests in parallel"/>
<meta property="og:description" content="It is possible to run multiple A/B tests at once and measure accurate lifts on the same metric by randomizing user assignments into A/B test experiments."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/running-multiple-ab-tests-in-parallel.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-04-06 00:00:00+02:00"/>
<meta property="article:modified_time" content="2020-04-06 00:00:00+02:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Data"/>
<meta property="article:tag" content="ab-testing"/>
<meta property="og:image" content="/images/abpa4.png"/>

  <title>Bytepawn - Marton Trencseni &ndash; Running multiple A/B tests in parallel</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <!--<h2><a href="">Bytepawn - Marton Trencseni</a></h2>-->
      <h2><a href="http://bytepawn.com">Bytepawn</a></h2>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="running-multiple-ab-tests-in-parallel">Running multiple A/B tests in parallel</h1>
    <p>Marton Trencseni - Mon 06 April 2020 - <a href="/category/data.html">Data</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>Suppose we have N=10,000 users and want to run 2 A/B test experiments, $E_1$ and $E_2$, both of which are trying to move the same metric. In this post I will assume the metric we are trying to move is timespent per user (or something like it, a number assigned to each user). The same logic also applies to conversions, but timespents are a better illustration of the concepts.</p>
<p>It is a common misconception that when running two experiments, we have to split our users between the two experiments, so each experiment will have 50,000 users in it, and each bucket will have 2,500 users in it (A in $E_1$, B in $E_1$, A in $E_2$, B in $E_2$). The cause of this misconception is the belief that if a user is in both experiments, then we cannot tell which experiment led to the user spending more time.</p>
<p>At face value, this is an error in statistical reasoning. We don’t really care why an <em>individual user</em> spent more or less time with the product, what we care about is the average timespent between A and B. As long as that measurement is accurate, individual users’ being influenced by multiple experiments is irrelevant. Accurate here means that we would measure the same thing (statistically) if we were running only one A/B test.</p>
<p>Having said that, there are cases when running multiple tests on the same user leads to statistical errors: this happens if the experiments interact. In other words, if we assume that run by itself $E_1$ lifts by X, and $E_2$ lifts by Y, and if we run both than the lift is X+Y, then we're fine. But if the effects interact with each other, in which case the combined lift is something else (eg. X+Y/2, because the presence of X suppresses Y), then we cannot run them in parallel. This happens if eg. the experiments are making UI changes to the same dialog.</p>
<p>If the experiments are independent, there is in fact no need to limit the sample sizes, both experiments can run on all 100,000 users, in parallel.</p>
<p><a href="https://github.com/mtrencseni/playground/blob/master/Parallel%20AB%20tests.ipynb">The code shown below is up on Github.</a></p>
<h2>Modeling the experiments</h2>
<p>To understand why parallel experiments work, let’s remember how an A/B test is modeled in our simulations: each user is represented by a independent random variable (RV). Independent, because we assume users don’t affect each other (so, we’re not in a social network setting here), and it’s a random variable because individual user outcomes are random. In this post, like before, I will use an <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a> to model timespents. The exponential distribution has one parameter $\mu$, which works out to be the mean. I will assume that by default, users have $\mu=1$.</p>
<p>In our timespent simulations, when we say that an A/B test is actually working, we model this by increasing the $\mu$ parameter for the user’s random variable. In the end, we will sample the random variable, so the actual outcome can be any timespent $t&gt;0$, but on average, users with lifted parameters will have higher timespents. This is the key: in an A/B test, we don’t care about individual user’s outcomes, since they are statistically random anyway, we care about measuring accurate average lifts between groups of users.</p>
<h2>Visualizing one A/B test</h2>
<p>There is an easy visual way to understand why parallel A/B tests work. Before we look at the parallel cases, as a starting point, let’s look at the simple case of just one experiment. We can use code like in the previous posts for this:</p>
<div class="highlight"><pre><span></span><span class="n">N</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">funnels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">1.00</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="p">]</span>
<span class="n">timespent_params</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">timespents</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">funnel_assignment</span> <span class="o">=</span> <span class="n">simulate_abtest</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">timespent_params</span><span class="p">,</span> <span class="n">force_equal</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">timespent_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">timespents</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">expon</span><span class="p">(</span><span class="n">param</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<p>What this code is doing: there are $N=10,000$ users, we will split them evenly between A and B funnels in the experiment (per <code>funnels</code>, 2nd column). Each user is modeled by an exponential random variable's parameter (<code>timespent_params</code>, which has default parameter 1). The function <code>simulate_abtest()</code> assigns each user into A or B, it returns this assignment into <code>funnel_assignment</code>. Further, it adjusts the <code>timespent_params</code>, by increasing the RV’s parameter for users in the B bucket by 1, leaving As alone (per <code>funnels</code>, 1st column). The final <code>for</code> loop samples the exponential distributions and stores the actual timespent values per user in <code>timespents</code>.</p>
<p>We can visualize the outcome of this experiment by drawing both the parameters and the actual timespents of each user. Since there are $N=10,000$ users, we can do so on a 100x100 image. The left side shows the parameters ($\mu=1$ or $\mu=2$), the right side shows the actual, sampled timespents.</p>
<p><img src="/images/abpa1.png" alt="" style="width: 600px;"/></p>
<p>Let’s change our visualization a little bit: let’s make it so we draw the A bucket users on top, and the B bucket users on the bottom:</p>
<p><img src="/images/abpa2.png" alt="" style="width: 600px;"/></p>
<p>This nicely shows us what’s going on. On the left side, we can see that the random variables for A and B have different values. On the right side, we can see that after sampling, the difference is still discernible with the naked eye. Note that the two left and two right sides are showing the same values, only arranged differently.</p>
<h2>Two A/B tests in parallel</h2>
<p>Now let’s run 2 A/B tests in parallel. In both cases, A leaves the RV’s parameter alone. But for $E_1$, we lift it to $\mu=2$, for $E_2$ we lift it to $\mu=3$. Users are in both A/B tests, and they are assigned into A and B buckets randomly, independently in the two experiments:</p>
<div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="mi">1000</span>
<span class="n">num_tests</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">base_lift</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">timespent_params</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">timespents</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">funnel_assignments</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tests</span><span class="p">):</span>
    <span class="n">funnels</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="mi">0</span><span class="p">,</span>                  <span class="mf">0.5</span><span class="p">],</span>
        <span class="p">[(</span><span class="n">test</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">base_lift</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">]</span>
    <span class="n">funnel_assignment</span> <span class="o">=</span> <span class="n">simulate_abtest</span><span class="p">(</span><span class="n">funnels</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">timespent_params</span><span class="p">)</span>
    <span class="n">funnel_assignments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">funnel_assignment</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">timespent_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">timespents</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">expon</span><span class="p">(</span><span class="n">param</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<p>Note that we call <code>simulate_abtest()</code> in a loop, for each experiment. Let’s visualize the outcome here: we expect that the parameter image will have 4 colors, corresponding to whether a user ended up in AA ($\mu=1$), AB ($\mu=2$) BA ($\mu=3$) or BB ($\mu=4$):</p>
<p><img src="/images/abpa3.png" alt="" style="width: 600px;"/></p>
<p>The left side image in fact has 4 colors. After random sampling, the right side looks just as random as in the single A/B test case. Now let’s do the same trick as before, and draw the image so that As are on top, and Bs are on the bottom. We can pick whether we do this for $E_1$ or $E_2$, we will see the same thing, here I'm doing it for $E_2$'s A and B:</p>
<p><img src="/images/abpa4.png" alt="" style="width: 600px;"/></p>
<p>Note again that these are the same exact values as above, only reordered. We can see that on the left side, there are 2 possible parameters on each side (the A and B variations from the <em>other</em> experiment). And on the right side we can see that even though the other experiment is also running, we can clearly tell apart the average value between top (A) and bottom (B).</p>
<p>After visualization, we can also numerically see this:</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">test</span><span class="p">,</span> <span class="n">funnel_assignment</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">funnel_assignments</span><span class="p">):</span>
    <span class="n">ps</span><span class="p">,</span> <span class="n">ts</span> <span class="o">=</span> <span class="p">[[],</span> <span class="p">[]],</span> <span class="p">[[],</span> <span class="p">[]]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">which_funnel</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">funnel_assignment</span><span class="p">):</span>
        <span class="n">ts</span><span class="p">[</span><span class="n">which_funnel</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timespents</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">ps</span><span class="p">[</span><span class="n">which_funnel</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timespent_params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ps_means</span><span class="p">,</span> <span class="n">ts_means</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">ts</span><span class="p">]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">CompareMeans</span><span class="o">.</span><span class="n">from_data</span><span class="p">(</span><span class="n">ts</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">ztest_ind</span><span class="p">(</span><span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;larger&#39;</span><span class="p">,</span> <span class="n">usevar</span><span class="o">=</span><span class="s1">&#39;unequal&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;test &#39;</span><span class="o">%</span><span class="mo">02</span><span class="n">d</span><span class="s1">&#39;, experiment lift=</span><span class="si">%.4f</span><span class="s1">, blended parameter lift=</span><span class="si">%.4f</span><span class="s1">, measured lift=</span><span class="si">%.4f</span><span class="s1">, p-value=</span><span class="si">%.4f</span><span class="s1">&#39;</span>
          <span class="o">%</span> <span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="p">(</span><span class="n">test</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">base_lift</span><span class="p">,</span> <span class="n">ps_means</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">ps_means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ts_means</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">ts_means</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">))</span>
</pre></div>


<p>Prints something like:</p>
<div class="highlight"><pre><span></span>test 0, experiment lift=1.0000, expected lift=0.9700, measured lift=0.9634, p-value=0.0000
test 1, experiment lift=2.0000, expected lift=1.9850, measured lift=1.9924, p-value=0.0000
</pre></div>


<p>In the first test, we lifted by 1. Because of the other test also running and changing the random variable parameters, on average our random variable parameters were shifted by 0.97 (instead of 1). After sampling, the actual lift was 0.96. And at this sample size, this lift had a very low p-value (since it’s a doubling, it’s easy to measure). And in the next row, we can see the second A/B test, which is also easily measureable.</p>
<h2>Multiple A/B tests in parallel</h2>
<p>Maybe this only worked because there were only 2, and we lifted the RV’s parameter so aggressively (doubling, tripling). Let’s see what happens if we run 11 in parallel, with a $\mu$ lift of 0, 0.1, 0.2 ... 1.0 (so the first one doesn’t work, the last one doubles). The numeric outcomes:</p>
<div class="highlight"><pre><span></span>test 00, experiment lift=0.0000, blended parameter lift=0.0605, measured lift=0.0656, p-value=0.0101
test 01, experiment lift=0.1000, blended parameter lift=0.1168, measured lift=0.1027, p-value=0.0001
test 02, experiment lift=0.2000, blended parameter lift=0.2122, measured lift=0.1929, p-value=0.0000
test 03, experiment lift=0.3000, blended parameter lift=0.3125, measured lift=0.3113, p-value=0.0000
test 04, experiment lift=0.4000, blended parameter lift=0.4011, measured lift=0.4153, p-value=0.0000
test 05, experiment lift=0.5000, blended parameter lift=0.5037, measured lift=0.5526, p-value=0.0000
test 06, experiment lift=0.6000, blended parameter lift=0.6013, measured lift=0.5849, p-value=0.0000
test 07, experiment lift=0.7000, blended parameter lift=0.6867, measured lift=0.6740, p-value=0.0000
test 08, experiment lift=0.8000, blended parameter lift=0.8124, measured lift=0.8005, p-value=0.0000
test 09, experiment lift=0.9000, blended parameter lift=0.8973, measured lift=0.8763, p-value=0.0000
test 10, experiment lift=1.0000, blended parameter lift=1.0157, measured lift=1.0209, p-value=0.0000
</pre></div>


<p>It’s still pretty good. Hoever, there are some deviations, f Let’s visualize the middle one, where $\mu$ is lifted by 0.5:</p>
<p><img src="/images/abpa6.png" alt="" style="width: 600px;"/></p>
<p>We can see that on the left, because there are so many A/B tests in play, the RV parameters also look random, but we can still see the difference, also in the sampled timespents on the right side.</p>
<h2>Monte Carlo simulations to estimate variance of parallel A/B tests</h2>
<p>In the above case, for the 7th test, the true experiment lift was 0.7000, but due to the presence of other A/B tests, the blended parameter lift between the two buckets (left side on the images) was 0.6867. Let’s quantify how much of a variance we can expect, as a function on $N$. Let’s run a scenario where we’re running 7 A/B tests at the same time, with lifts of <code>[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.10]</code> on different $N$ population of<code>[10*1000, 20*1000, 30*1000, 40*1000, 50*1000, 100*1000, 200*1000, 1000*1000]</code> sizes 100 times each, and compute means and variances for average parameter lift and actual measured lift.</p>
<p>First, the average parameter lifts, with errors to show the variance:</p>
<p><img src="/images/abpa7.png" alt="" style="width: 600px;"/></p>
<p>We can see that even at low N, the average parameter lifts are pretty close to the intended experimental lift. The variance is so small, we can barely see it. Let’s zoom in on the orange line:</p>
<p><img src="/images/abpa8.png" alt="" style="width: 600px;"/></p>
<p>The variance goes down with $N$ as expected, but even at $N=10,000$ it’s very low (notice the y-axis). Now the same for actual measured lifts (after the random variables are sampled):</p>
<p><img src="/images/abpa9.png" alt="" style="width: 600px;"/></p>
<p>Here we have bigger error bars, it’s hard to see what’s going on. Let’s look at the orange bar again:</p>
<p><img src="/images/abpa10.png" alt="" style="width: 600px;"/></p>
<p>The magnitude of the standard deviation (the error bar), plotted by itself:</p>
<p><img src="/images/abpa11.png" alt="" style="width: 600px;"/></p>
<p>Finally, we can plot the average measured p value (with a Z-test), for each parallel A/B test, for each $N$:</p>
<p><img src="/images/abpa13.png" alt="" style="width: 600px;"/></p>
<p>This is essentially a clue to the sample size $N$ we need to be able to detect the signal at a given $\alpha$ critical value of p.</p>
<h2>Per experiment random user id</h2>
<p>As these Monte Carlo simulations show, it is possible to run multiple A/B tests at once, across the whole population, on the same outcome metric, and still measure the experimental lift accurately.</p>
<p>It is only necessary to randomize the users between the funnels A and B (and C...) for each experiment independently of the other experiments. A simple solution for this is to have a randomly generated seed for each experiment (like <code>90bb5357</code> for experiment $E_1$, <code>a5f50c2b</code> for experiment $E_2$, and so on), combine these with a per user id (like user_id, or cookie_id) to get a per experiment random object, that is fixed for the user (so when the user comes back, we compute the same random object), and then modulo that to the number of funnels we have to decide whether to put the user into A or B in each experiment:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">funnel_user</span><span class="p">(</span><span class="n">base_traffic_split</span><span class="p">,</span> <span class="n">test_seed</span><span class="p">,</span> <span class="n">user_id</span><span class="p">):</span>
    <span class="n">test_id</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">test_seed</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">user_id</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;ascii&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>
    <span class="n">bits</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">test_id</span><span class="p">,</span> <span class="mi">16</span><span class="p">))[</span><span class="mi">3</span><span class="p">:]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">bit</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5</span><span class="o">**</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bit</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bits</span><span class="p">)])</span>
    <span class="k">if</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">base_traffic_split</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;A&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39;B&#39;</span>
</pre></div>


<h2>Conclusion</h2>
<p>By randomizing user assignments into A/B test experiments, it is possible to run multiple A/B tests at once and measure accurate lifts on the same metric.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/ab-testing.html">ab-testing</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Running multiple A/B tests in parallel",
  "headline": "Running multiple A/B tests in parallel",
  "datePublished": "2020-04-06 00:00:00+02:00",
  "dateModified": "2020-04-06 00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/running-multiple-ab-tests-in-parallel.html",
  "description": "It is possible to run multiple A/B tests at once and measure accurate lifts on the same metric by randomizing user assignments into A/B test experiments."
}
</script></body>
</html>