<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="google-site-verification" content="14dvQcWyDWHBo8aM0kld8XzEa6KypAzCQDz1_KPus9E" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-1812620-2');
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn - Marton Trencseni Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
  <meta name="author" content="Marton Trencseni" />
  <meta name="description" content="" />
<meta property="og:site_name" content="Bytepawn - Marton Trencseni"/>
<meta property="og:type" content="blog"/>
<meta property="og:title" content="Bytepawn - Marton Trencseni"/>
<meta property="og:description" content=""/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content=""/>
  <title>Bytepawn - Marton Trencseni &ndash; Tag: gym</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <!--<h2><a href="">Bytepawn - Marton Trencseni</a></h2>-->
      <h2><a href="http://bytepawn.com">Bytepawn</a></h2>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h2><a href="/using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch.html#using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch">Using simulated self-play to solve all OpenAI Gym classic control problems with Pytorch</a></h2>
    <p>
      Marton Trencseni - Thu 14 November 2019
      &#8226; Tagged with
      <a href="/tag/python.html">python</a>,      <a href="/tag/pytorch.html">pytorch</a>,      <a href="/tag/reinforcement.html">reinforcement</a>,      <a href="/tag/learning.html">learning</a>,      <a href="/tag/openai.html">openai</a>,      <a href="/tag/gym.html">gym</a>    </p>
  </header>
  <div>
      <p>I use simulated self-play by ranking episodes by summed reward. Game outcomes are divided in two by cutting at the median, winners are assigned +1 rewards, losers are assigned -1 rewards, like in games like Go and Chess. Unlike naive policy gradient descent used in previous posts, this version solves all OpenAI classic control problems, albeit slowly.<br/><br/> <img src="/images/mountaincar.png" alt="OpenAI mountaincar" style="width: 400px;"/></p>
      <a class="btn" href="/using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch.html#using-simulated-self-play-to-solve-all-openai-gym-classic-control-problems-with-pytorch">Continue reading</a>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="/solving-openai-gym-classic-control-problems-with-pytorch.html#solving-openai-gym-classic-control-problems-with-pytorch">Applying policy gradient to OpenAI Gym classic control problems with Pytorch</a></h2>
    <p>
      Marton Trencseni - Tue 12 November 2019
      &#8226; Tagged with
      <a href="/tag/python.html">python</a>,      <a href="/tag/pytorch.html">pytorch</a>,      <a href="/tag/reinforcement.html">reinforcement</a>,      <a href="/tag/learning.html">learning</a>,      <a href="/tag/openai.html">openai</a>,      <a href="/tag/gym.html">gym</a>    </p>
  </header>
  <div>
      <p>I try to generalize the policy gradient algorithm as introduced earlier to solve all the OpenAI classic control problems. It works for CartPole and Acrobot, but not for Pendulum and MountainCar environments. <br/><br/> <img src="/images/classic_control.png" alt="OpenAI classic control environments" style="width: 400px;"/></p>
      <a class="btn" href="/solving-openai-gym-classic-control-problems-with-pytorch.html#solving-openai-gym-classic-control-problems-with-pytorch">Continue reading</a>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="/solving-the-cartpole-reinforcement-learning-problem-with-pytorch.html#solving-the-cartpole-reinforcement-learning-problem-with-pytorch">Solving the CartPole Reinforcement Learning problem with Pytorch</a></h2>
    <p>
      Marton Trencseni - Tue 22 October 2019
      &#8226; Tagged with
      <a href="/tag/python.html">python</a>,      <a href="/tag/pytorch.html">pytorch</a>,      <a href="/tag/reinforcement.html">reinforcement</a>,      <a href="/tag/learning.html">learning</a>,      <a href="/tag/openai.html">openai</a>,      <a href="/tag/gym.html">gym</a>,      <a href="/tag/cartpole.html">cartpole</a>    </p>
  </header>
  <div>
      <p>The CartPole problem is the Hello World of Reinforcement Learning, originally described in 1985 by Sutton et al. The environment is a pole balanced on a cart. CartPole is one of the environments in OpenAI Gym, so we don't have to code up the physics. Here I walk through a simple solution using Pytorch. <br/><br/> <img src="/images/cartpole.gif" alt="Cartpole animation" style="width: 400px;"/></p>
      <a class="btn" href="/solving-the-cartpole-reinforcement-learning-problem-with-pytorch.html#solving-the-cartpole-reinforcement-learning-problem-with-pytorch">Continue reading</a>
  </div>
</article>


    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1812620-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1812620-2');
</script><script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Bytepawn - Marton Trencseni ",
  "url" : "",
  "image": "",
  "description": ""
}
</script></body>
</html>