<!DOCTYPE html>
<html lang="en">
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="I try to generalize the policy gradient algorithm as introduced earlier to solve all the OpenAI classic control problems. It works for CartPole and Acrobot, but not for Pendulum and MountainCar environments." />
<meta name="keywords" content="python, pytorch, reinforcement, learning, openai, gym">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="Solving OpenAI Gym classic control problems with Pytorch"/>
<meta property="og:description" content="I try to generalize the policy gradient algorithm as introduced earlier to solve all the OpenAI classic control problems. It works for CartPole and Acrobot, but not for Pendulum and MountainCar environments."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/solving-openai-gym-classic-control-problems-with-pytorch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-11-12 00:00:00+01:00"/>
<meta property="article:modified_time" content="2019-11-12 00:00:00+01:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="pytorch"/>
<meta property="article:tag" content="reinforcement"/>
<meta property="article:tag" content="learning"/>
<meta property="article:tag" content="openai"/>
<meta property="article:tag" content="gym"/>
<meta property="og:image" content="/images/classic_control.png"/>

  <title>Bytepawn &ndash; Solving OpenAI Gym classic control problems with Pytorch</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="solving-openai-gym-classic-control-problems-with-pytorch">Solving OpenAI Gym classic control problems with Pytorch</h1>
    <p>Posted on Tue 12 November 2019 in <a href="/category/machine-learning.html">Machine Learning</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>In <a href="http://bytepawn.com/solving-the-cartpole-reinforcement-learning-problem-with-pytorch.html">the previous blog post</a> we used a simple Reinforcement Learning method called policy gradient to solve the CartPole-v1 environment from OpenAI. This post is about seeing how far I can take this basic approach. Can it solve the other, harder classic control problems in OpenAI? </p>
<p>The OpenAI classic control problem set consists of:</p>
<ul>
<li><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py">CartPole-v1</a>: Balance a pole on a cart.</li>
<li><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py">Acrobot-v1</a>: Swing up and balance a two-link robot.</li>
<li><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py">Pendulum-v0</a>: Swing up and balance a pendulum.</li>
<li><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py">MountainCar-v0</a>: Drive up a big hill.</li>
<li><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py">MountainCarContinuous-v0</a>: Drive up a big hill.</li>
</ul>
<h2>Environments</h2>
<p>All these problems are similar in that the state space (which is the input space for the policy neural network) is a few real numbers. The action space (which is the output space for the policy) is sometimes discrete (left/right) and sometimes a real (magnitude):</p>
<p><img src="/images/classic_control.png" alt="OpenAI classic control environments" style="width: 600px;"/></p>
<table>
    <tr>
        <td><b>env</b></td>
        <td><b>CartPole-v1</b></td>
        <td><b>Acrobot-v1</b></td>
        <td><b>Pendulum-v0</b></td>
        <td><b>MountainCar-v0</b></td>
        <td><b>MountainCarContinuous-v0</b></td>
    </tr>
    <tr>
        <td><b>description</b></td>
        <td>balance a pole on a cart</td>
        <td>swing up a two-link robot</td>
        <td>swing up a pendulum</td>
        <td>drive up a big hill</td>
        <td>drive up a big hill</td>
    </tr>
    <tr>
        <td><b>code</b></td>
        <td><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py">cartpole.py</a></td>
        <td><a nref="https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py">acrobot.py</a></td>
        <td><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py">pendulum.py</a></td>
        <td><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py">mountain_car.py</a></td>
        <td><a href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py">continuous_mountain_car.py</a></td>
    </tr>
    <tr>
        <td><b>state</b></td>
        <td>4 reals: cart position, velocity, pole angle, velocity</td>
        <td>6 reals: sine and cosine of the two rotational joint angles and the joint angular velocities</td>
        <td>3 reals: sine and cosine of the angle and angular velocity</td>
        <td>2 reals: position, velocity</td>
        <td>2 reals: position, velocity</td>
    </tr>
    <tr>
        <td><b>action</b></td>
        <td>discrete: left/right</td>
        <td>discrete: left/nothing/right, the torque on the second joint</td>
        <td>1 real between -2 and 2, the torque</td>
        <td>discrete: left/nothing/right, the force on the car</td>
        <td>1 real between -1 and 1, the force on the car</td>
    </tr>
    <tr>
        <td><b>episode length</b></td>
        <td>500</td>
        <td>500</td>
        <td>200</td>
        <td>200</td>
        <td>999</td>
    </tr>
    <tr>
        <td><b>reward</b></td>
        <td>+1 for each timestep the agent stays alive</td>
        <td>-1 for each timestep the agent takes to swing up</td>
        <td>negative reward as a function of the angle</td>
        <td>-1 for each timestep the agent doesn’t reach the top of the hill</td>
        <td>negative for applied action, +100 once solved</td>
    </tr>
    <tr>
        <td><b>reward threshold for solved</b></td>
        <td>475</td>
        <td>-100</td>
        <td>None (I used -150)</td>
        <td>-110</td>
        <td>90</td>
    </tr>
</table>

<h2>Coding it up</h2>
<p>To attack all the problems with one script, I took the script from the previous post and made it more general. First, a function which returns the input and output space dimensions:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">in_out_length</span><span class="p">(</span><span class="n">env</span><span class="p">):</span>
    <span class="n">input_length</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">box</span><span class="o">.</span><span class="n">Box</span><span class="p">):</span>
        <span class="n">output_length</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">discrete</span><span class="o">.</span><span class="n">Discrete</span><span class="p">):</span>
        <span class="n">output_length</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="k">return</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span>
</pre></div>


<p>For example, for CartPole, it returns <code>(4, 2)</code>. The next question is, how to attack environments such as Pendulum, where the action is real and not discrete? The Policy gradient formalism uses probabilities, so I chose to quantize the action space and pretend it’s discrete. This makes the quantization levels a parameter. I also wanted to play around with different reward functions when constructing the loss function, so I collected these into an <code>EnvDescriptor</code>:</p>
<div class="highlight"><pre><span></span><span class="n">fields</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;name&#39;</span><span class="p">,</span>
    <span class="s1">&#39;model_class&#39;</span><span class="p">,</span>
    <span class="c1"># the rest are optional, in case it&#39;s needed for the env</span>
    <span class="s1">&#39;reward_func&#39;</span><span class="p">,</span>
    <span class="s1">&#39;output_quantization_levels&#39;</span><span class="p">,</span>
    <span class="s1">&#39;output_range&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reward_threshold&#39;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">EnvDescription</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;EnvDescription&#39;</span><span class="p">,</span> <span class="n">fields</span><span class="p">,</span> <span class="n">defaults</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">fields</span><span class="p">))</span>    
</pre></div>


<p>The descriptors for the problems are:</p>
<div class="highlight"><pre><span></span><span class="n">env_descriptions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">EnvDescription</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">,</span>
                   <span class="n">model_class</span><span class="o">=</span><span class="n">PolicyShallowNN</span><span class="p">,</span>
                  <span class="p">),</span>
    <span class="n">EnvDescription</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Acrobot-v1&#39;</span><span class="p">,</span>
                   <span class="n">model_class</span><span class="o">=</span><span class="n">PolicyShallowNN</span><span class="p">,</span>
                  <span class="p">),</span>
    <span class="n">EnvDescription</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Pendulum-v0&#39;</span><span class="p">,</span>
                   <span class="n">model_class</span><span class="o">=</span><span class="n">PolicyDeepNN</span><span class="p">,</span>
                   <span class="n">reward_func</span><span class="o">=</span><span class="n">pendelum_reward_func</span><span class="p">,</span>
                   <span class="n">output_quantization_levels</span><span class="o">=</span><span class="mi">101</span><span class="p">,</span>
                   <span class="n">output_range</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                   <span class="n">reward_threshold</span><span class="o">=-</span><span class="mi">150</span><span class="p">,</span>
                  <span class="p">),</span>
    <span class="n">EnvDescription</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;MountainCar-v0&#39;</span><span class="p">,</span>
                   <span class="n">model_class</span><span class="o">=</span><span class="n">PolicyDeepNN</span><span class="p">,</span>
                   <span class="n">reward_func</span><span class="o">=</span><span class="n">mountaincar_reward_func</span>
                  <span class="p">),</span>
    <span class="n">EnvDescription</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;MountainCarContinuous-v0&#39;</span><span class="p">,</span>
                   <span class="n">model_class</span><span class="o">=</span><span class="n">PolicyDeepNN</span><span class="p">,</span>
                   <span class="n">reward_func</span><span class="o">=</span><span class="n">mountaincar_reward_func</span><span class="p">,</span>
                   <span class="n">output_quantization_levels</span><span class="o">=</span><span class="mi">101</span><span class="p">,</span>
                   <span class="n">output_range</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                  <span class="p">),</span>
<span class="p">]</span>
</pre></div>


<p>I defined two policy neural networks: one which is a simple affine linear map, and one with one hidden layer network and <code>ReLu</code> for nonlinearity. Both end with <code>softmax</code> to get a probability distribution over the discrete (or discretized) action space, as before:</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PolicyShallowNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PolicyShallowNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">PolicyDeepNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PolicyDeepNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">output_length</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<p>With these generalizations, we use plain-vannila policy descent:</p>
<ul>
<li>for each episode<ul>
<li>finish the episode</li>
<li>if the descriptor contains a custom reward function, use that, otherwise use the env’s default reward function to compute rewards, which are then rolled up with a gamma factor and multiplied by -1 to get the loss function (value)</li>
<li>using <code>gamma = 0.99</code></li>
</ul>
</li>
<li>every 10 episodes, use the Adam optimizer to improve the weights on the policy, by backprop’ing from the added up losses (of 10 episodes)</li>
<li>check whether the avg reward of the last 10 episodes if greater than the threshold (always use the env’s rewards here), if yes, we’re done</li>
</ul>
<p>Computing the loss function:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reward_func</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">reward_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">reward_func</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">scaled_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># decay rewards with gamma</span>
    <span class="n">tail</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">reward</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="c1"># backward</span>
        <span class="n">tail</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">tail</span>
        <span class="n">scaled_rewards</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tail</span><span class="p">)</span> <span class="c1"># insert at beginning</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">scaled_rewards</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">r</span>
    <span class="n">loss</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>


<p>The main training loop:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_solve</span><span class="p">(</span><span class="n">env_desc</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="mi">1000</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Doing </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_desc</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span> <span class="o">=</span> <span class="n">in_out_length</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">output_quantization_levels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">output_length</span> <span class="o">*=</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">output_quantization_levels</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">model_class</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">output_length</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">episode_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="k">print</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;&#39;</span> <span class="k">if</span> <span class="nb">bool</span><span class="p">(</span><span class="n">episode</span> <span class="o">%</span> <span class="mi">100</span><span class="p">)</span> <span class="k">else</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">))</span>
        <span class="n">probs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">states</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">_max_episode_steps</span><span class="p">):</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">select_action_from_policy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">output_quantization_levels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">output_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">action</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">env_desc</span><span class="o">.</span><span class="n">output_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">output_range</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">env_desc</span><span class="o">.</span><span class="n">output_quantization_levels</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">action</span><span class="p">])</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span>
            <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="n">episode_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
        <span class="n">episode_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_reward</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_solved</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">episode_rewards</span><span class="p">,</span> <span class="n">reward_threshold</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_desc</span><span class="p">)):</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Solved in </span><span class="si">%s</span><span class="s1"> episodes!&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">episode</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">env</span><span class="p">,</span> <span class="n">model</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">env_desc</span><span class="o">.</span><span class="n">reward_func</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Failed!&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">env</span><span class="p">,</span> <span class="n">model</span>
</pre></div>


<h2>Results</h2>
<p>This approach is able to solve CartPole-v1, Acrobot-v1, but fails on the Pendulum and MountainCar problems. The optimizer gets stuck in a bad minima and never finds a good policy solution. An interesting bad behavior happens for positive loss functions: the loss function is structured like <code>loss := -1 * sum(reward x log(probability of action taken))</code>, where the <code>log(probability of action taken)</code> is negative, so the overall expression is positive, assuming the reward is positive. In this case, making the loss 0 would be a global minimum. This can happen if the optimizer sets the probabilities of an arbitrary sub-optimal policy to one, hence making the <code>log(probability)</code> zero, making the entire loss function go to zero, even though the solution is actually "random".</p>
<p>Intuitively, it seems this simple implementation of gradient descent only works for reactive environments like CartPole and Acrobot, where the policy network doesn’t have to find a “plan” (ie. swing left and then right to get up), it just has to react to the current state, irrespective of the history. To solve the harder environments, more advanced training approaches are required.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/python.html">python</a>
      <a href="/tag/pytorch.html">pytorch</a>
      <a href="/tag/reinforcement.html">reinforcement</a>
      <a href="/tag/learning.html">learning</a>
      <a href="/tag/openai.html">openai</a>
      <a href="/tag/gym.html">gym</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Solving OpenAI Gym classic control problems with Pytorch",
  "headline": "Solving OpenAI Gym classic control problems with Pytorch",
  "datePublished": "2019-11-12 00:00:00+01:00",
  "dateModified": "2019-11-12 00:00:00+01:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/solving-openai-gym-classic-control-problems-with-pytorch.html",
  "description": "I try to generalize the policy gradient algorithm as introduced earlier to solve all the OpenAI classic control problems. It works for CartPole and Acrobot, but not for Pendulum and MountainCar environments."
}
</script></body>
</html>