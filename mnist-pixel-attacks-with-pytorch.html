<!DOCTYPE html>
<html lang="en">
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
    styles: {
    ".MathJax .mo, .MathJax .mi": {color: "black ! important"}}
    },
    tex2jax: {
      inlineMath: [['$','$'], ['\\\\(','\\\\)']],
      displayMath: [['$$','$$'], ["\\[","\\]"]],
      processEscapes: true,
    }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="/theme/css/style.min.css">
  <!--<link rel="stylesheet" type="text/css" href="/theme/css/pygments.min.css">-->
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments/github.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/font-awesome.min.css">
  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bytepawn Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="Marton Trencseni" />
<meta name="description" content="It’s easy to build a CNN that does well on MNIST digit classification. How easy is it to break it, to distort the images and cause the model to misclassify?" />
<meta name="keywords" content="python, pytorch, cnn, torchvision, mnist, skl">
<meta property="og:site_name" content="Bytepawn"/>
<meta property="og:title" content="MNIST pixel attacks with Pytorch"/>
<meta property="og:description" content="It’s easy to build a CNN that does well on MNIST digit classification. How easy is it to break it, to distort the images and cause the model to misclassify?"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="/mnist-pixel-attacks-with-pytorch.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-06-01 00:00:00+02:00"/>
<meta property="article:modified_time" content="2019-06-01 00:00:00+02:00"/>
<meta property="article:author" content="/author/marton-trencseni.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="pytorch"/>
<meta property="article:tag" content="cnn"/>
<meta property="article:tag" content="torchvision"/>
<meta property="article:tag" content="mnist"/>
<meta property="article:tag" content="skl"/>
<meta property="og:image" content="/images/mnist-attack-accuracy.png"/>

  <title>Bytepawn &ndash; MNIST pixel attacks with Pytorch</title>
</head>
<body>
  <aside>
    <div>
      <a href="/">
        <img src="/theme/img/profile.png" alt="" title="">
      </a>
      <h1><a href=""></a></h1>
      <p></p>
      <nav>
        <ul class="list">
          <li><a href="/">home</a></li>
          <li><a href="https://github.com/mtrencseni">github</a></li>
          <li><a href="https://www.linkedin.com/in/mtrencseni">linkedin</a></li>
          <li><a href="http://arxiv.org/find/all/1/au:+trencseni/0/1/0/all/0/1">arxiv</a></li>
        </ul>
      </nav>
      <ul class="social">
      </ul>
    </div>
  </aside>
  <main>

<article>
  <header>
    <h1 id="mnist-pixel-attacks-with-pytorch">MNIST pixel attacks with Pytorch</h1>
    <p>Posted on Sat 01 June 2019 in <a href="/category/machine-learning.html">Machine Learning</a></p>
  </header>
  <div>
    <h2>Introduction</h2>
<p>It’s easy to build a CNN that does well on MNIST digit classification. How easy is it to break it, to distort the images and cause the model to misclassify? The post was inspired by 2 papers, both are worth reading:</p>
<ul>
<li><a href="https://arxiv.org/abs/1710.08864">One Pixel Attack for Fooling Deep Neural Networks</a></li>
<li><a href="https://keenlab.tencent.com/en/2019/03/29/Tencent-Keen-Security-Lab-Experimental-Security-Research-of-Tesla-Autopilot/">Tencent’s Keen Labs get a Tesla to leave the lane by placing a few white dots on the road</a></li>
</ul>
<p>For the experiments, I’m using the ~99% accurate CNN that I’ve trained in the <a href="http://bytepawn.com/solving-mnist-with-pytorch-and-skl.html#solving-mnist-with-pytorch-and-skl">previous MNIST post</a>. 
The <a href="https://github.com/mtrencseni/pytorch-playground/blob/master/06-pixel-attacks/MNIST-pixel-attacks.ipynb">ipython notebook is up on Github.</a></p>
<h2>Random noise</h2>
<p>The first and simplest thing I tried is adding random noise. I wanted to see how random noise leads to classification errors, ie. reduced accuracy. Pytorch’s <a href="https://pytorch.org/docs/stable/torchvision/transforms.html">transformer framework</a> made these experiments easy: I just had to add one <code>Lambda()</code> to the transformers chain:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">distort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_pixels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pixels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="p">()</span><span class="o">*</span><span class="mi">28</span><span class="p">)][</span><span class="nb">int</span><span class="p">(</span><span class="n">random</span><span class="p">()</span><span class="o">*</span><span class="mi">28</span><span class="p">)]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">plot_accuracies</span><span class="p">(</span><span class="n">distorted_pixels</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">):</span>
    <span class="n">clear_output</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;distorted pixels&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">distorted_pixels</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">distorted_pixels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">my_test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
            <span class="s1">&#39;../data&#39;</span><span class="p">,</span>
            <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
               <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
               <span class="n">transforms</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">distort</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_pixels</span><span class="o">=</span><span class="n">i</span><span class="p">)),</span>
               <span class="c1"># the above line was added compared to the original model</span>
               <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,)),</span>
            <span class="p">])</span>
        <span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">actuals</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">test_label_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">my_test_loader</span><span class="p">)</span>
    <span class="n">distorted_pixels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">actuals</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))</span>
    <span class="n">plot_accuracies</span><span class="p">(</span><span class="n">distorted_pixels</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">)</span>
</pre></div>


<p>Note that the <code>transforms.Lambda()</code> was added after the <code>transforms.ToTensor()</code>, so at this point the image is a 28x28 tensor of floats between 0 and 1, and before the <code>transforms.Normalize()</code>. For reference, this is what a distorted image looks like (fifth test image in MNIST, a digit 4, original and with 100 pixels distorted):</p>
<p><img src="/images/mnist-four.png" alt="MNIST 4 distorted" style="width: 600px;"/></p>
<p>The result shows the decline in accuracy, as a function of how many pixels are randomly distorted on the test image:</p>
<p><img src="/images/mnist-attack-accuracy.png" alt="MNIST attack accuracy" style="width: 600px;"/></p>
<p>The accuracy degradation is pretty linear. I was surprised by this, I was expecting the model to perform well up to a certain distortion, and then break down (more inclined inverse S-curve). Note that I set the pixels randomly, so at high counts it’s possible that a pixel is “set twice”, so the actual number of distorted pixels is lower.</p>
<h2>One-pixel attacks</h2>
<p>Based on the above, breaking the model by setting one-pixel shouldn’t be easy. But in the random noise example, we were setting pixels randomly; is it possible to find a special vulnerable pixel and break the classification? When looking for a one-pixel vulnerability, there are 3 parameters, x and y, and the pixel value. Here I will only play with x and y, I will set the pixel value to 1.0.</p>
<p>Unlike in the article referenced in the intro, here I’m doing a brute-force attack. Two outer loops go through the pixel location, and each time it runs the test images through the model, and counts how many images are misclassified. Note that without any distortion the model achieves 98.9% accuracy, so out of 10,000 test images there are 110 that are misclassified even without distortion. These 110 are always subtracted, they are not counted.</p>
<p>The code:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_pixel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">misclassified_images</span><span class="p">(</span><span class="n">actuals</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
    <span class="n">vuls</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">actuals</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">actuals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">vuls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">set</span><span class="p">(</span><span class="n">vuls</span><span class="p">)</span>

<span class="n">my_test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
        <span class="s1">&#39;../data&#39;</span><span class="p">,</span>
        <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
           <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
           <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,)),</span>
        <span class="p">])</span>
    <span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">actuals</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">test_label_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">my_test_loader</span><span class="p">)</span>
<span class="n">base_misclassifieds</span> <span class="o">=</span> <span class="n">misclassified_images</span><span class="p">(</span><span class="n">actuals</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="n">vulnerable_images</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">28</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">28</span><span class="p">):</span>
        <span class="n">my_test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span>
                <span class="s1">&#39;../data&#39;</span><span class="p">,</span>
                <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                   <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                   <span class="n">transforms</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">set_pixel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)),</span>
                   <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,)),</span>
                <span class="p">])</span>
            <span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">actuals</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">test_label_predictions</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">my_test_loader</span><span class="p">)</span>
        <span class="n">misc</span> <span class="o">=</span> <span class="n">misclassified_images</span><span class="p">(</span><span class="n">actuals</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="n">actual_vulnerables</span> <span class="o">=</span> <span class="n">misc</span><span class="o">.</span><span class="n">difference</span><span class="p">(</span><span class="n">base_misclassifieds</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;(</span><span class="si">%s</span><span class="s1">, </span><span class="si">%s</span><span class="s1">) vulnerables: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">actual_vulnerables</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">actual_vulnerables</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s1">&#39;--&gt; </span><span class="si">%s</span><span class="s1"> is </span><span class="si">%s</span><span class="s1">, classified as </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">actuals</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[</span><span class="n">a</span><span class="p">]))</span>
        <span class="n">vulnerable_images</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">actual_vulnerables</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">base_misclassifieds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">vulnerable_images</span><span class="p">))</span>
</pre></div>


<p>In the end, it finds only 69 (out of 10,000) that are vulnerable. At most locations, it finds vulnerabilities, but it’s always the same images. For example, image #3906 is very vulnerable, changing the (0,8) pixel gets this 1 classified as a 3:</p>
<p><img src="/images/mnist-one.png" alt="MNIST 1 attacked" style="width: 600px;"/></p>
<p>This seems to go against the first cited article’s finding, where ~75% of CIFAR-10 images are found to have a one-pixel vulnerability. Possible explanations:</p>
<ul>
<li>classifying digits is simpler than image categories, so MNIST is less vulnerable</li>
<li>playing around with the pixel values would yield more vulnerabilities</li>
</ul>
<h2>Out-of-bounds one-pixel attacks</h2>
<p>Finally, I played around with setting out-of-bounds values, ie. what if I numerically set the pixel value to be greather than 1.0. Let’s repeat the distortion experiment, but set values to 5.0 instead of 1.0. At 20 distorted pixels the accuracy already drops to ~50%:</p>
<p><img src="/images/mnist-oob-attack-accuracy.png" alt="MNIST attack accuracy" style="width: 600px;"/></p>
<p>I attibute this to the <code>relu()</code> activation, which cuts off at the low end, but not on the high-end, so these out-of-bounds values are able to travel through the deep layers and throw off the model:</p>
<p><img src="/images/relu.png" alt="relu()" style="width: 600px;"/></p>
<p>Intuitively, neural nets find blobs in high-dimensional space and assign it to a label, and by doing this we move the image away from the blob along one dimension.</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="/tag/python.html">python</a>
      <a href="/tag/pytorch.html">pytorch</a>
      <a href="/tag/cnn.html">cnn</a>
      <a href="/tag/torchvision.html">torchvision</a>
      <a href="/tag/mnist.html">mnist</a>
      <a href="/tag/skl.html">skl</a>
    </p>
  </div>
</article>

    <footer>
      <p>&copy; Marton Trencseni </p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "MNIST pixel attacks with Pytorch",
  "headline": "MNIST pixel attacks with Pytorch",
  "datePublished": "2019-06-01 00:00:00+02:00",
  "dateModified": "2019-06-01 00:00:00+02:00",
  "author": {
    "@type": "Person",
    "name": "Marton Trencseni",
    "url": "/author/marton-trencseni.html"
  },
  "image": "{{ SITEURL }}/{{ THEME_STATIC_DIR }}/img/profile.png",
  "url": "/mnist-pixel-attacks-with-pytorch.html",
  "description": "It’s easy to build a CNN that does well on MNIST digit classification. How easy is it to break it, to distort the images and cause the model to misclassify?"
}
</script></body>
</html>